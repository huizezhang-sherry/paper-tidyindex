---
title: "A Tidy Framework and Infrastructure to Systematically Assemble Spatio-temporal Indexes from Multivariate Data"
format:
  tandf-pdf:
    keep-tex: true  
pdf-engine: pdflatex
author:
  - name: H. Sherry Zhang
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0002-7122-1463
    email: huize.zhang@monash.edu
  - name: Dianne Cook
    affiliation: 
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0002-3813-7155
  - name: Ursula Laa
    affiliation: 
      - name: University of Natural Resources and Life Sciences
        department: Institute of Statistics
        address: Gregor-Mendel-Straße 33, 1180 Wien, Austria
        city: Vienna
        country: Austria
    orcid: 0000-0002-0249-6439
  - name: Nicolas Langrené
    affiliation: 
      - name: BNU-HKBU United International College
        department: Department of Mathematical Sciences
        address: 2000 Jintong Road, Tangjiawan, Zhuhai
        city: Zhuhai, Guangdong
        country: China
    orcid: 0000-0001-7601-4618
  - name: Patricia Menéndez
    affiliation:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0003-0701-6315
abstract: |
  - indexes, useful, quantify severity, early monitoring, 
  - A huge number of indexes have been proposed by domain experts, however, a large majority of them are not being adopted, reused, and compared in research or in practice. 
  - One of the reasons for this is the plenty of indexes are quite complex and there is no obvious easy-to-use implementation to apply them to user's data. 
  - The paper describes a general pipeline framework to construct indexes from spatio-temporal data, 
  - This allows all the indexes to be constructed through a uniform data pipeline and different indexes to vary on the details of each step in the data pipeline and their orders. 
  - The pipeline proposed aim to smooth the workflow of index construction through breaking down the complicated steps proposed by various indexes into small building blocks shared by most of the indexes.
  - The framework will be demonstrated with drought indexes as examples, but appliable in general to environmental indexes constructed from multivariate spatio-temporal data
keywords:
  - indexes
  - data pipeline
  - software design
bibliography: bibliography.bib  
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
options(width = 70)
library(tidyverse)
library(latex2exp)
library(patchwork)
library(tidyindex)
library(lmomco)
library(lubridate)
library(SPEI)
library(tsibble)
library(GGally)
library(patchwork)
```

# Introduction

**Why index is useful, why people care about indexes**

*incorporate the following in why using index: multiple pieces of information (variables) that need to be taken into account*

Many concepts relevant to decision making cannot be directly measured, however, they are crucial for resource allocation, early prevention, and other operational purpose. 
  
  - USDM and European Drought Observatory publishes drought indicator to [...], 
  - NOAA and BOM producing forecast on El Nino alert system to [...]
  - OECD and UN producing indexes measuring socio-economic aspects of country for policy making.

There decision making often needs to consider multiple aspects and indexes is a statistical tool to pack these information into a single number, making it easier for policy makers and public communication. 

While the [decision to make] is often multi-faceted, simple indexes tend to be favoured when it comes to communicating to stakeholders and public.

**Define what is an index, what is not**

While indexes are commonly used in operation and research, their compositions and structures can vary substantially. An index can be composed from a singe variable, such as the Standardized Precipitation Index (SPI) [@spi], or multiple variables, also known as composite indexes or composite indicators, like the Human Development Index [@hdi]. Some indexes can be expressed as a single equation, like Normalized Difference Vegetation Index (NDVI) [@ndvi], calculated as $(\text{NIR} + \text{Red})/(\text{NIR} + \text{Red})$. In other cases, indexes themselves can be used as variables to construct indexes. For example, SPI is used to in many complex indexes, such as the U.S. Drought monitor and European Combined Drought Index (CDI) [@cdi]. While the index framework proposed in this paper applies to all the aforementioned indexes, our focus will be on studying complex indexes that combine multiple variables using statistical methods. 

The initial plan aims to investigate how variable combinations can affect the indexes and whether we can use statistical methods to recommend better combination for decision making. However, when analysing specific index examples, we find the composition of indexes is much more complicated than originally expected. In addition to combining multiple variables, indexes can have a spatial and/or temporal component to capture the neighbouring effect in space and the prolonged effect in time. [Constructing indexes from combining variables with spatial and temporal component is further complicated by different treatments to process data from different sources such as satellite imageries, in-situ stations, and large physical models.]

**What is the challenges with current index construction**

While numerous indexes are proposed by researchers, most well-known indexes only use one or two variables. More complex indexes tend to have difficulties to reach consensus given the uncertainty of their behaviors in different scenarios. Typical ways to evaluate new indexes is to compare it with the standard/ well-known indexes, such as SPI or SPEI for drought indexes, or a response variable relevant for the monitoring purpose, i.e.  crop yield for monitoring agricultural drought. 


  - However, more doubt can still raises: Whether the variable used to construct the index is a good set of variables. 
  - Does the index series capture the variables measures. Whether other ways of combining the variables results in difference in the index. 
  - Being able to answer these questions help to prove the robustness, extendibility and dimensionality of the index, as proposed by xxx as criteria for (selecting?) indexes.

**what can be done if people adopt this pipeline/ why it is beneficial?**

  - In order to evaluate indexes, indexes need to be broken down into pieces that it constitutes and then we can analyse how each component will affect the index. 
  - The OECD handbook [@oecd_handbook_2008] provides technical guidelines to construct composite indexes for comparing country performance for policy-making. 
  This paper identifies a more general pipeline framework where statistical indexes (of all kinds) can be built from. 
  Based on the pipeline, an R package is built to validate the indexes, [more details on the functionality here] from understanding multivariate relationships to uncertainty analysis. 

<!-- The pipeline approach is general while adaptable to most index construction.  -->
<!-- It allows indexes to be created, studied, and compared in a structured tidy form and enables statistical analysis of indexes to be performed easily:  -->


**who would benefit from this paper**

  - This work provides researchers actively developing new indexes with tools to evaluate the proposed indexes. 
  - Helping index analysts to identify weakness in existing indexes for methodology improvement. 


The rest of the paper is structured as follows: @sec-pipeline reviews the concept of data pipeline in R. The pipeline framework for index construction is presented in @sec-index-pipeline. @sec-dev explains how to include a new building block in each pipeline module. Examples are given in @sec-examples to demonstrate the index construction with the pipeline built. 

# Data pipeline {#sec-pipeline}

*Think about if there is another word for data pipeline*

**Why you should care about pipeline**

Data pipeline is not a new concept to computing. It refers to a set of data processing elements connected in series, where the output of one element is the input of the next one.
@wickham_plumbing_2009 argues that whether made explicit or not, the pipeline has to be presented in every graphics program.
The paper also argues that breaking down graphic rendering into steps is beneficial for understanding the implementation and comparing between different graphic systems. 
The discussion on pipeline construction is well documented in early interactive graphics software: @buja_elements_1988, @sutherland_orca_2000, and @xie_reactive_2014 and their pipeline steps include non-linear transformation, variable standardization, randomization and dimension reduction.

**What is pipeline, its underlying software design philosophy, and how these are reflected in R**

One of the most commonly known pipeline examples is perhaps the Unix pipeline where programs can be concatenated with `|` to flow the output from the last program into the next program, i.e. 
    
````
command 1 | command 2 | command 3 | ...
````

To solve a complex problem, the Unix system builds simple programs that do one thing well and work well together.
This design is also reflected in the tidyverse ecosystem in R.
To solve a complicated data problem using tidyverse, analysts typically build the solution using a collection of tools from the tidyverse toolbox. 
The data object can flow smoothly from one command to the next, safeguarded by the tidy data format [@wickham_tidy_2014], which prescribes three rules on how to lay out tabular data.
The tidyverse tools also embrace a strong human-centered design where function names are intuitive and easy to reference through autocomplete.
With the tidyverse design principle in mind, the tidymodel suite enables analysts to build machine learning models through the data pipeline. 
It includes typical tasks required in machine learning like data resampling, feature engineering, model fitting, model tuning, and model evaluation.
An advantage of tidymodel pipeline over separate software for individual models is that analysts no longer need to write model-specific syntax to work with each model, but pipeline-specific syntax that is applicable to all the models implemented in tidymodel.
This allows users to easily experiment with a collection of machine learning models. 

**Constructing indexes would also benefit from pipeline and embracing the aforementioned design philosophy.**

In index construction, data pipeline is often presented in a workflow diagram in the research paper to illustrate how the raw data is transformed into the final indexes. 
This agrees with Wickham's argument on the presence of the data pipeline, however, more often than not, the pipeline is not made explicit in the software.
Often the time, all the steps are lumped into a single wrapper function, rather than being split into smaller, modulated functions. 
This increases the cost of maintaining and understanding the code base, gives analysts little freedom to customise the indexes for specific needs, and hinders reusing existing code for building new indexes.
A pipeline approach unites a range of indexes under a single data pipeline and analysts can compose indexes from pipeline steps like building Legos from individual bricks.
In this workflow, analysts are not limited by indexes that have been already proposed and can easily combine pipeline steps to compose novel indexes.  Analysis of the indexes (i.e. calculation of uncertainty) is also feasible by adding external code into the pipeline.

# A pipeline for building statistical indexes {#sec-index-pipeline}

## How does the pipeline constructin of an index look like? {#sec-toy-example}

Consider a commonly used drought index: Standardized Precipitation-Evapotranspiration Index (SPEI)  [@spei]. Its construction involves: 

  1) transform the average temperature (`TMED`) into potential evapotranspiration (`pet`) 
  2) combine precipitation (`prcp`) and potential evapotranspiration (`pet`) into a single variable `diff`
  3) aggregate the difference series with a sliding window (`.scale`) 
  4) fit a distribution to the aggregated series, and 
  5) derive the index value from the normal density values.

Conventionally approach may combine all these steps into a single function, with some level of modularity. However, these modules may only work for the selected index offered by the package.

Under the pipeline approach, analysts first need to identify which module each step belongs to. 

Below shows the pseudocode for constructing SPEI with the pipeline:

````
DATA %>% 
  var_trans(.method = thornthwaite, Tave = TMED, ..., .new_name = "pet") %>%
  dim_red(diff = prcp - pet) %>%
  aggregate(.var = diff, .scale = 12, .new_name = "agg") %>%
  dist_fit(.method = "lmoms", .var = agg, .dist = DIST) %>%
  augment(.var = agg)
````

<!-- *FWI:*  -->

<!--   - *verbs as name for each step: `aggregate`, `normalise`* -->
<!--   - *use NSE for variable name: `prcp`, `c(tavg, prcp)`* -->
<!--   - *created variables are prefixed with dot: `.agg_prcp`* -->
<!--   - *avoid string and use function to avoid "gamma" vs. "gam": `gamma()`, `lmom()`* -->

The pipeline construct allows for multiple `.scales` and `.dist` to be evaluated in `aggregate()` and `dist_fit()` to compare index under different parameterisations. The result can then be passed into the `ggplot2` to crease visualisation. @fig-toy-example compares the SPEI calculated with two distributions (log-logistic and pearson III). 

Apart from evaluating multiples parameters, the pipeline approach allows the the steps written for one index can be directly extrapolate to another index building within the pipeline. The flexibility of the pipeline also integrate well with other existing packages, for examples, fitting distributions using L-moment is commonly used when constructing drought indexes. The package `lmomco` provides general L-moment fits to a wide range of distributions and users can easily access to all the distributions within the pipeline. 

<!-- additional benefit: -->

<!--   - Multiple scales and multiple distributions can be fitted using the `c(...)` syntax to compare index values constructed from different parameterisations;  -->
<!--   - Intermediate results can be checked after each step; and -->
<!--   - Additional steps and analysis can be wired into the workflow for index diagnostics and customised user need. -->

```{r out.width = "100%"}
knitr::include_graphics(here::here("figures/toy-example-spei.png"))
```


```{r fig-toy-example, eval = FALSE, echo = FALSE}
#| fig-align: center
#| fig-width: 10
#| fig-height: 3
#| fig-cap: "Standardised Precipitation Evapotranspiration Index (SPEI) calculated with two distribution fits in Step 3 described above: generalised logistic and pearson III distribution, using the `wichita` data from the package `SPEI`."
library(tidyindex)
library(lmomco)
library(ggplot2)
library(tsibble)
library(SPEI)
library(tidyverse)
library(lubridate)
data(wichita)

dt <- wichita %>%
  mutate(ym = make_yearmonth(YEAR, MONTH), id = 1) %>% 
  init(id = id, time = ym) %>%
  var_trans(.pet_method = "thornthwaite", .vars = TMED, .lat = 37.6475) %>%
  dim_red(diff = PRCP - pet) %>%
  aggregate(.var = diff, .scale = 12) %>%
  dist_fit(.dist = list(loglogistic(), pearsonIII()), .method = "lmoms", .var = .agg) %>%
  augment(.var = .agg)

dt$data %>%
  mutate(.dist = ifelse(.dist == "parglo", "Log-logistic", "PearsonIII")) %>% 
  ggplot(aes(x = ym, y = .index, color = .dist)) +
  geom_line() + 
   theme_bw() +
  theme(
    panel.grid = element_blank()
  ) +
  scale_x_yearmonth(name = "Year", date_break = "2 years", date_label = "%Y") +
  scale_color_brewer(palette = "Dark2", name = "distribution") + 
  ylab("SPEI")

```

## Pipeline steps for constructing indexes

<!-- *constructing time series index should also be encapsulated in the framework* -->

*any index can be broken down into multiple steps and then we can do things with it: swap, change parameter, etc*


*variables == indicators*

An overview of the pipeline is given in @fig-pipeline-steps to illustrate the construction from raw data to the final indexes.
The pipeline includes eight modules for operations in the spatial, temporal, and multivariate aspects of the data as well as modules for comparing and communicating indexes.
Analysts are free to select the modules they need and arrange them in the order they see fit to construct indexes.
While the starting point of the pipeline is raw data, there are steps prior to this that are crucial to the success of an index. 
For example, the defined index needs to be useful for measuring the concept of interest and variables need to be collected from reliable sources with proper quality control. 

  * align the spatial resolution and temporal frequency of data collected from different satellite products
  * obtain and clean variables from countries, potentially aggregate from regional data into country level
  * merge ground-based and satellite-based data 

Before elaborating each of the eight pipeline modules as subsections, the data notation will be first introduced. Let $\mathbf{x}(\mathbf{s};\mathbf{t})$ denote the raw data with spatial, temporal, and multivariate aspects: the spatial dimension $\mathbf{s} =  (s_1, s_2, \cdots, s_n)^\prime$ is defined in the 2D space: $\mathbf{s} \in \mathcal{D}_s \subseteq \mathbb{R}^2$, the temporal dimension $\mathbf{t} = (t_1, t_2, \cdots, t_J)^\prime$ is defined in the 1D space: $\mathbf{t} \in \mathcal{D}_t \subseteq \mathbb{R}$. When more than one variable is involved, the multivariate data can also be written as: $\mathbf{x}(\mathbf{s}; \mathbf{t}) = (x_1(\mathbf{s}; \mathbf{t}), x_2(\mathbf{s}; \mathbf{t}), \cdots, x_P(\mathbf{s}; \mathbf{t}))^\prime$.

```{r fig-pipeline-steps}
#| fig-align: center
#| fig-cap: "Diagram of pipeline steps for index construction. will need to be updated with better design and the distribution fitting step."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-steps.png"))
```

### Temporal processing

The construction of an index sometimes needs to consider information from neighbouring time periods.
The temporal processing is a general operator on the time dimension of the data in the form of  

\begin{equation}
f_{\mathcal{\psi}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where $\psi \in \Psi \subseteq \mathbb{R}^{d_{\psi}}$ is the parameters associated with the temporal operation and $d_{\psi}$ is the number of parameter of $\psi$. 
A typical example of temporal processing is aggregation, which is used in the drought index SPI to measure the lack of precipitation for meteorological drought.
In SPI, monthly precipitation is aggregated by a time scale parameter $k$: $x(s_i;t_{j^\prime}) = \sum_{j = j^\prime-k+1}^{j^\prime}x(s_i; t_j),$ where $j^\prime$ is the new time index after the aggregation. In this notation, each spatial location is separately aggregated and precipitation is summed from $k$ month back, $j^\prime - k + 1$, to the current period, $j^\prime$, to create the aggregated series, indexed by $j^\prime$. 

*more explicit on k will influence 1) long term vs. short term, 2) uncertainty*

The choice of time scales parameter $k$ can result in variation in the calculated index values: a small $k$ of 3 or 6 months produces the index more sensitive to individual months, while a large $k$ of 24 or 36, an equivalent to a 2- or 3-year aggregation, gives dryness information relative to the long term condition. As will be shown in section [SECTION EXAMPLE], this variation may even lead to conflicting conclusions on the dry/wet condition of the area, highlighting the importance to account for index uncertainty when interpreting index values for decision-making.

*Effective drought index*

### Spatial processing

Spatial processing may be needed when indexes are not calculated independently on each collected location or when variables collected from multiple sources need to be fused before further processing. The process can be written as a general operation in the form of 

\begin{equation}
x(\mathbf{s}^\prime;\mathbf{t}) = g_{\mathcal{\theta}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where $\theta \in \Theta \subseteq \mathbb{R}^{d_{\theta}}$ is the associated parameters in the process and $d_{\theta}$ is the number of parameter of $\theta$. 
An example of spatial processing is to align variables collected in different resolutions.
When variables are collected at different resolutions, analysts may choose to down-sample those in a finer resolution, $i$, to match those in a coarser resolution, $i^\prime$. This is a spatial aggregation and if aggregate using the mean, it can be written as 

\begin{equation}
g(x) = \frac{\sum_{i \in i^\prime}x}{n_{i^\prime}},
\end{equation}

where $i \in i^\prime$ includes all the cells from the finer resolution in the coarser grid and $n_{i^\prime}$ is the number of observations falls into the  coarser grid. Other examples of spatial processing include 1) borrowing information from neighbouring spatial locations to interpolate unobserved locations and 2) fusing variables from ground measures with satellite imageries.

### Variable transformation 

The purpose of variable transformation is to create variables that fits assumptions for further computing. These assumptions include a stable variance, normal distribution, or a certain scale required by some algorithms down the pipeline. Variable transformation is a general notion of a functional transformation on the variable: 

\begin{equation}
h_{\tau}(x(\mathbf{s};\mathbf{t})),
\end{equation} 

where $\tau \in T \subseteq \mathbb{R}^{d_{\tau}}$ is the parameter in the transformation if any, and $d_{\tau}$ is the number of parameter of $\tau$. Transformation is needed for data that are highly skewed and some common transformations include log, quadratic, and square root transformation.

<!-- A common assumption is normality in drought index.  -->

### Scaling

While scaling can be seen as a specific type of variable transformation, it is separated into its own step to make the step explicit in the pipeline.
The key difference between the two steps is that variable transformation typically changes the shape of the data while scaling only changes the data scale and can usually be written in the form of 

\begin{equation}
[x(s_i;t_j) - \alpha]/\gamma.
\end{equation}

For example, a z-score standardisation can be written in the above form with $\alpha = \bar{x}(s; t)$ and $\gamma = \sigma(s; t)$, a min-max standardisation uses $\alpha = \min[x(s_i, t_j)]$ and $\gamma = \max[x(s_i, t_j)] - \min[x(s_i, t_j)]$.
@fig-scale-var-trans-compare shows a collection of variable pre-processing operations and uses color to differentiate whether the operation is a variable transformation or a scaling step. While both variable transformation and scaling are pre-processing steps, the scaling operations in green show the same distribution as the original data. 

<!-- Take one or multiple variables to create a new variable,  -->
<!-- Restrict it to single variable, square root, log etc could be linearly, also non-linear -->

<!-- change the shape of the variable -->

<!-- <!-- A conversion of non-normal variables into (approximately) normal. wrong! -->

<!-- GAM, can you do additive model pairwise/ three-way -->

<!-- While being a common technique in data pre-processing, scaling has an impact on the outliers of the data and analysts need to be cautious on checking its effect. -->


```{r fig-scale-var-trans-compare}
#| fig-cap: "Comparison of operations in scaling (green) and variable transformation (orange) steps in free scale. Variables after the scaling operations have the same distribution as the origin, while the distribution changes after variable transformation. "
#| fig-height: 8
#| fig-width: 12
set.seed(123)
x <- rgamma(n = 1000, shape = 1, scale = 2)


trans_df <- tibble(origin = x, 
       `z-score` = (x - mean(x)/ sd(x)), 
       qudratic = x^2,
       log = log(x),
       `square root` = sqrt(x),
       `cubic root` = x^(1/3),
       minmax = (x - min(x))/ (max(x) - min(x)),
       boxcox = (x^0.5 - 1)/0.5,
       centering = x - mean(x)
       ) %>% 
  mutate(id = row_number(),
         ) %>% 
  pivot_longer(-id, names_to = "var", values_to = "value") %>% 
  mutate(category = ifelse(
    var %in% c("origin", "z-score", "minmax", "centering"),
    "Scaling", "Var. Trans.")
    ) 

latex_df <- tibble(
  var = c("origin", "z-score", "qudratic", "log", "square root", "cubic root",
          "minmax", "boxcox", "centering"),
  latex = c(
           r"($x$)",
           r"($\frac{x - \bar{x}}{sd(x)}$)",
           r"($x^2$)",
           r"($log(x)$)",
           r"($\sqrt{x}$)",
           r"($x^{1/3}$)",
           r"($\frac{x - min(x)}{max(x) - min(x)}$)",
           r"($\frac{x^{0.5} - 1}{0.5}$)",
           r"($x - \bar{x}$)"
         )
) %>% 
  left_join(trans_df %>% group_by(var) %>% summarise(max = max(value)))

trans_df %>% 
  ggplot() +
  geom_density(aes(x = value, color = category, y = after_stat(scaled))) + 
  geom_label(data = latex_df, 
             aes(x = 0.8 * max, y = 0.8, label = TeX(latex, output = "character")), 
             parse = TRUE) + 
  facet_wrap(vars(var), scales = "free") + 
  scale_color_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(legend.position = "bottom")
```


\newpage

### Dimension reduction

When indexes are constructed from multivariate information, dimension reduction methods combine that information into a univariate series. In the pipeline, dimension reduction includes methods that take multivariate inputs and output the data in a lower dimension (often univariate): 

\begin{equation}
x_{p^*}(\mathbf{s}; \mathbf{t}) \rightarrow x_p(\mathbf{s}; \mathbf{t}),
\end{equation}

where $p^* = 1, 2, \cdots, P^*$ and $p = 1, 2, \cdots, P$ reduce the variable dimension from $P$ to $P^*$. 
The most commonly used dimension reduction technique is Principal Component Analysis (PCA), also called Empirical Orthogonal Function (EOF) in earth science. It can be seen as a special case of weighting, where variables are summed up in a linear combination: $$x_{p^*}(\mathbf{s}; \mathbf{t}) = \sum_{p = 1}^{P}\lambda_{p}x_p(\mathbf{s};\mathbf{t}),$$ with restrictions imposed on the weight coefficient: $\sum_{p=1}^P\lambda_p^2 = 1$. In other cases of weighting, the coefficients can be as simple as giving equal weight to each variables. 

Some dimension reduction can also be formulated from domain-specific knowledge. This can be theories that describe the physics of the phenomenon being indexed or practical formulations used to extract useful features from the raw variables. For example, in the index SPEI, a difference series is calculated between precipitation and potential evapotranspiration (PET) and the validity of this formulation is backed up by climate water balance model [Thornthwaite, 1948], which describes [...]. *Add another example of remote sensing variables i.e. NDVI = (NIR - Red) / (NIR + Red)?*


<!-- Spectral satellite imageries often collected at different bands and it is common to combine values from multiple bands to compose new variables relevant to the index. The most common example of this is the Normalized Difference Vegetation Index (NDVI), which uses near-infrared (NIR) ($x_1(s;t)$) and red channel ($x_2(s;t)$): -->

<!-- \begin{equation} -->
<!-- \text{NDVI}(s;t) = \frac{x_1(s;t) - x_2(s;t)}{x_1(s;t) + x_2(s;t)}, -->
<!-- \end{equation} -->

<!-- which is used to evaluate the vegetation from near-infrared (NIR) and red channel. -->

While suggested weights and formulas can indicate norms adored by practitioners, analysts should be given the flexibility to experiment with different combinations when constructing indexes. 
This could help understand index behavior from its sensitivity to the variables and suggest alternative weights that better suit the specific tasks.


### Distribution fit

*model fit? *

<!-- With a probability model imposed,  -->
Distribution fit can be seen as the model fitting in its simplest term. It can be represented by 

\begin{equation}
F_{\eta}(x(\mathbf{s}; \mathbf{t})), 
\end{equation}

where $\eta \in H \subseteq \mathbb{R}^{d_{\eta}}$ is the distribution parameter and $d_{\eta}$ is the number of parameter of $\eta$. A distribution fit typically aims at finding the distribution that best fits the data. 
Analysts may start from a pool of candidate distributions with a chosen fitting method and goodness of fit measure. 
While it is useful to find the ultimate best distribution to fits the data, from a probabilistic perspective, the fitting procedure itself has an uncertainty associated with the data fed and the parameter chosen. A reasonable alternative is to understand how much the index values can vary given different distributions, fitting methods, and goodness of fit tests, and whether these variations are negligible in a given application.

### Normalising

This step maps the univariate series into a different scale, typically for ease of comparison across regions. 
For example, a normal scale, [0, 1], or [0, 100] may be favored for reporting certain indexes. 
In drought indexes, i.e. SPI or SPEI, the quantiles from the fitted distribution are converted into the normal scale via the normal reverse CDF function: $\Phi^{-1}(.)$. 
Normalising is usually used at the end of the pipeline and its main difference from the scaling step is that here the change of scale also changes the distribution of the variable. 
While being commonly used, this step can get criticism from analysts for forcing the data into the decided scale, which can be either unnecessary or inaccurately exaggerate or downplay the outliers. 
Also, the use of a normal scale needs to be interpreted with caution. 
@fig-normalising illustrates the normal density not being directly proportional to its probability of occurrence.
This is concerning, especially at the extreme values, since a small difference in the tail density can have magnitudes of difference in its probability of occurrence.

```{r fig-normalising}
#| fig-cap: "Scatterplot of normal quantiles against their density values. THree tail density values are highlighted with its probability of occurence labelled. Probability is calculated assuming monthly data: with a density of -2, the probability of occurrence is 1/pnorm(-2)/12 = 4 years. The non-linear relationship between the two quantities suggests normalised indexes need to be interpreted with caution since a slight change in the tail distribution can result in magnitudes of difference in its probability of occurrence."
#| fig-width: 9
library(tidyverse)
all <- tibble(x = seq(-3, 3, 0.01), y = pnorm(x)) 
dt <- tibble(x = c(-3, -2.5, -2), y = pnorm(x)) 
label <- tibble(x = c(-3, -2.5, -2), y = pnorm(x), 
                label = c("once in 62 years", "once in 13 years", "once in 4 years"))

dt %>% 
  ggplot() + 
  geom_point(aes(x= x, y = y)) + 
  geom_line(data = all, aes(x = x, y = y)) + 
  ggrepel::geom_label_repel(
    data = label, aes(x = x, y = y, label =label),
    nudge_y = 0.2, nudge_x = 0.1,
    arrow = arrow(length = unit(0.08, "inches")), min.segment.length = 0) +
  scale_x_continuous(breaks = seq(-3, 3, 1)) + 
  xlab("Normalised Score") + 
  ylab("Probability") + 
  theme_bw() + 
  theme(panel.grid.minor = element_blank())
```

### Benchmarking

Benchmarking sets a constant value to allow the constructed index to be compared across time.
Here we denote it with $u[x(s_i, t_j)]$ where $u$ is a scalar of interest in the index constructed. A benchmark value could be a constant or a function of the data, i.e. mean.

### Simplification

In public communication, the index values are usually accompanied by a categorical grade. The categorised grades are an ordered set of descriptive words or colors to communicate the severity or guide the comprehension of the indexes. 
The mapping from continuous index values to the discrete grades is called simplification in the pipeline and it can be written as a piece-wise function: 

\begin{equation}
\begin{cases}
C_0 & c_1 \leq (s_i; t_j) < c_0 \\
C_1 & c_2 \leq x(s_i; t_j) < c_1 \\
C_2 & c_3 \leq x(s_i; t_j) < c_2 \\
\cdots \\
C_z & c_z \leq x(s_i; t_j)
\end{cases}
\end{equation}

where $C_0, C_1,\cdots ,C_z$ are the categories and $c_0, c_1, \cdots, c_z$ are the thresholds for each category. In SPI, droughts are sorted into four categories: mild drought: $[-0.99, 0]$; moderate drought: $[-1.49, -1]$; severe drought: $[-1.99, -1.5]$, and extreme drought: $[-\infty, -2]$. In this case, $C_0, C_1, C_2, C_3$ are the drought categories: mild, moderate, severe, and extreme drought ($z = 3$) and $c_0 =0, c_1 = -1, c_2 = -1.5, c_3 = -2$ are the cutoff value for each class.

# Incorporating alternative methods into the pipeline components {#sec-dev}


<!-- # 1. unpack or the argumebt s  -->
<!-- # 2. construct the syntax to evalute: ~rescale_minmax(life_exp, min = xxx, max = xxxx) -->
<!-- # 3. evalute  -->
<!-- # 4. update the data object  -->




<!-- # [Extending the pipeline] a different name here: alternate a module with a different (method) -->

# Examples {#sec-examples}

## A drought index example

A common task for drought researchers is to compute indexes at different parameter combinations. This can be used to identify the spatial and temporal extent of drought events, recommend the best parameter choice, or compare the effectiveness of indexes for monitoring drought. The example below computes two indexes: SPI and SPEI, at various time scales and fitted distributions, for stations in the state of Queensland in Australia. The purpose of the example is to demonstrate the interfaces the tidyindex package built to allow easy computing at different parameter combinations. 

The state of Queensland in Australia is frequently affected by natural disaster events such as flood and drought, which can have significant impacts on its agricultural industry.  This study uses daily data from Global Historical Climatology Network Daily (GHCND), accessed via the package `rnoaa` to examine drought/flood condition in Queensland. Daily data is average into monthly and stations are excluded if monthly data contains missings, which is required for calculating both SPI and SPEI. This gives `r length(unique(queensland$id))` stations with complete records from 1990 January to 2022 April.

The function `compute_indexes()` can be used to collectively compute multiple indexes. The tidyindex offers wrapper functions, with the prefix idx_, that simplify the calculation of commonly used indexes by combining a set of pipeline steps into a single function. For example, the function `idx_spei()` includes the five steps previously described in @sec-toy-example (variable transformation, dimension reduction, temporal aggregation, distribution fit, and normalise). Each `idx_xxx()` function specifies the relevant parameters relevant to the index: the thornthwaite method is used to calculate PET in SPEI, with the average temperature (`tavg`) and latitude (`lat`) used as inputs. The SPEI is computed at four time scales (6, 12, 24, and 36 months) and fitted with two distributions (Log-logistic and General Extreme Value (GEV)). The SPI is also computed at the same four time scales and uses the default gamma distribution to fit the aggregated series. 

```{r cache=TRUE, echo = TRUE}
.scale <- c(6, 12, 24, 36)
(idx <- queensland %>%
  init(id = id, time = ym) %>%
  compute_indexes(
    spei = idx_spei(
      .pet_method = "thornthwaite", .tavg = tavg, .lat = lat,
      .scale = .scale, .dist = c(gev(), loglogistic())),
    spi = idx_spi(.scale = .scale)
  ))
```

The output from `compute_indexes()` contains index values and associated parameter in a long tibble. It includes the original variables (`id`, `ym`, `prcp`, `tmax`, `tmin`, `tavg`, `long`, `lat`, and `name`), index parameters (`.idx`, `.scale`, `.method`, and `.dist`), intermediate variables (`pet`, `.agg`, and `.fitted`), and the final index (`.index`). This data can be visualised across space or time, or simultaneously, to explore the wet/dry condition in Queensland. @fig-compute-spatial visualises the spatial distribution of SPI at two periods (2010 October - 2011 March and 2019 October - 2020 March) with significant natural disaster events: 2010/11 Queensland flood and 2019 Australia drought, which contributes to the notorious 2019/20 bushfire. @fig-compute-temporal displays the sensitivity of the SPEI series for one particular station, Texas post office, at different time scales and fitted distributions. These two plots demonstrate some possibilities to explore the indexes after they are computed from `compute_indexes()`. 


```{r}
library(sf)
qld_lga <- absmapsdata::lga2018 %>%
  filter(state_name_2016 == "Queensland") %>%
  rmapshaper::ms_simplify(keep = 0.3) %>%
  mutate(lga_name_2018 = str_replace(lga_name_2018, " \\([S|R|C|]\\)", ""),
         lga_name_2018 = str_replace(lga_name_2018, " \\(Qld\\)", ""))
```

```{r fig-compute-spatial}
#| fig-cap: "Spatial distribution of Standardized Precipitation Index (SPI-12) in Queensland, Australia during two major flood and drought events: 2010/11 and 2019/20. The map shows a continous wet period during the 2010/11 flood period and a mitigated drought situation, after its worst in 2019 December and 2020 Janurary, likely due to the increased rainfall in February from the meteorological record."
#| fig-width: 12
#| fig-height: 5
queensland_map <- ozmaps::abs_ste %>% filter(NAME == "Queensland") %>% 
  rmapshaper::ms_simplify(keep = 0.02)
queensland_map %>%
  ggplot() +
  geom_sf(fill = "transparent", color = "grey50", linewidth = 0.4) +
  geom_point(
    data = idx %>%
      filter(.idx == "spi", .scale == 12) %>%
      filter(((year(ym) %in% c(2010, 2019) & month(ym) >= 10) )|
               ((year(ym) %in% c(2011, 2020) & month(ym) <= 3) )), 
    aes(x = long, y = lat, color = .index)) +
  colorspace::scale_color_continuous_divergingx(palette = "Geyser", rev = TRUE, name = "Index") + 
  theme_void() +
  facet_wrap(vars(ym), ncol = 6)
```

```{r fig-compute-temporal}
#| fig-cap: 'Time series plot of Standardized Precipitation-Evapotranspiration Index (SPEI) at the Texas post office station (highlighted by a diamond shape in panel a). The SPEI is calculated at four time scales (6, 12, 24, and 36 months) and fitted with two distributions (Log Logistic and GEV). The dashed line at -2 represents the class "extreme drought" by the SPEI. A larger time scale gives a smoother index series, while also takes longer to recover from an extreme situation as seen in the  2019/20 drought period. The SPEI values from two distribution fits mostly agree, while GEV can results in more extreme values, i.e. in 1998 and 2020.'
#| fig-width: 12
#| fig-height: 6
texas <- idx %>%
    filter(name == "TEXAS POST OFFICE", .idx == "spei") %>% distinct(long, lat)
p1 <- queensland_map %>%
  ggplot() +
  geom_sf(fill = "transparent") +
  geom_point(data = queensland %>%
               distinct(id, long, lat, name),
             aes(x = long, y = lat)) +
  geom_point(data = texas, aes(x = long, y = lat),
             color = "orange", shape = 18, fill = "orange", size = 4) +  
  theme_void()


p2 <- idx %>%
  filter(name == "TEXAS POST OFFICE", .idx == "spei") %>%
  mutate(.dist = ifelse(.dist == "pargev", "GEV", "Log Logistic")) %>%
  rename(scale = .scale) %>% 
  ggplot(aes(x = ym, y = .index, color = .dist, group = .dist)) +
  geom_line() +
  theme_benchmark() +
  facet_wrap(vars(scale), labeller = label_both, ncol = 1) +
  scale_x_yearmonth(breaks = "2 year", date_labels = "%Y") +
  scale_color_brewer(palette = "Dark2", name = "Distribution") + 
  xlab("Year") + 
  ylab("Index")

(p1 | p2)  + 
  patchwork::plot_annotation(tag_levels = "a") + 
  patchwork::plot_layout(guides = "collect")  &
  theme(legend.position = "bottom") 
  
```

## An example on sustainable development indicators

<!-- Indexes are commonly used in social science is to rank entities (i.e. countries) on a topic through a range of indicators. This includes university ranking from QS, Times Higher Education; the Sustainable Development Goals (SDG) Index [@SDSN_Bertelsmann_2021] by the Sustainable Development Solutions Network (SDSN), the Human Development Index (HDI) [@UNDP_2021] by United Nations Development Programme (UNDP), etc. These indexes, or rankings, typically have a set of predefined dimensions, or pillars, and indicators are first reduced to these pillars and then reduced to an index.  -->

In the following example, the Human Development Index (HDI) will first be constructed using the pipeline syntax. The section will then demonstrate how expressions or parameters in the pipeline can be swapped to alternatives to study the index property.

Human Development Index (HDI) measures the development of countries through more than just economic growth, but also people's life expectancy and opportunity to receive education. These three identified dimensions are measured using four indicators: life expectancy at birth (health), expected years of schooling (education), mean years of schooling (education), and Gross National Income (GNI) *per capita* (standard of living). The technical notes [@UNDP_2021_tech_notes] have documented the procedures to calculate HDI and they are summarised below: 

  1. take log on GNI *per capita*,
  2. rescale the four indicators into [0, 1] using mini-max,
  3. aggregate the two education indicators using arithmetic mean, and
  4. aggregate the three dimensions into the index using geometric mean.
  
The values used in mini-max rescaling are summarised in @tbl-rescale-params and the justification of these numbers can be found in the technical notes mentioned above.

```{r}
#| tbl-cap-location: bottom
#| tbl-cap: Maximum and minimum values used to rescale the four HDI indicators into [0, 1] range. The maximum of GNI per capita is taken as the common log of 75,000, approximatly 4.875.
#| label: tbl-rescale-params
scaling_params <- tibble::tribble(
   ~Dimension, ~Indicator, ~Variable,  ~Minimum, ~Maximum,
  "Health",              "Life expectancy at birth (years)",   "life_exp",    "20",          "85",
  "Education",           "Expected years of schooling (years)",  "exp_sch",    "0",          "18",
  "Education",           "Mean years of schooling (years)",   "avg_sch",      "0",          "15",
  "Standard of living",  "GNI per capita (2017 PPP$)",     "gni_pc",       "100",      "75000"
  ) %>%
  mutate(across(contains("mum"), as.numeric),
         across(contains("mum"), ~ifelse(Variable == "gni_pc", log10(.x), .x)))
knitr::kable(scaling_params, digits = 3)
```

Among the four steps listed in the calculation, the first two are variable transformation. The next two can be considered as the dimension reduction step in the data pipeline given its intention to combine multivariate data into univariate. With the index pipeline proposed in the paper, HDI can be calculated as: 

```{r}
dt <- readxl::read_xlsx(path = here::here("data/hdi.xlsx") , skip = 4)
raw <- dt %>%
  janitor::clean_names() %>%
  dplyr::select(-paste0("x", seq(4, 14, 2))) %>%
  dplyr::rename(id = x1, countries = x2) %>%
  dplyr::mutate(across(c(id, human_development_index_hdi:hdi_rank), as.numeric)) %>%
  dplyr::filter(!is.na(id)) %>%
  dplyr::mutate(across(4:7, ~round(.x, digits = 3))) %>% 
  dplyr::select(-c(human_development_index_hdi,
                   gni_per_capita_rank_minus_hdi_rank, 
                   hdi_rank)) 
colnames(raw) <- c("id", "country", "life_exp", "exp_sch", "avg_sch", "gni_pc")
```

```{r echo = TRUE, message=TRUE}
dt <- raw %>% init(id = country, indicators = life_exp:gni_pc)
res <- dt %>%
  var_trans(gni_pc = log(gni_pc, base = 10)) %>% 
  var_trans(.method = rescale_minmax, .vars = life_exp:gni_pc,
            min = c(20, 0, 0, 2), max = c(85, 18, 15, log10(75000))) %>% 
  dim_red(sch = (exp_sch + avg_sch) / 2) %>%
  dim_red(index = (life_exp * sch * gni_pc)^(1/3))
```

<!-- The output is an `indri` object with a summary of operations followed by the data output. The data output contains the ID, country name, rescaled variables (`life_exp`, `exp_sch`, `avg_sch`, and `gni_pc`), the aggregated education dimension (`sch`), and the final index calculated (`index`). -->

Index analysts can be interested in studying the property of the index and the countries through the "what-if" questions. In the context of HDI, this can be what if an alternative dimension reduction expression is used to reduce the three dimensions into the HDI? Apart from the geometrical mean, linear combination is a common method to reduce the data dimension. With different weighting schemes, the combination can be compared to study the variable importance of each dimension or to reveal the underlying structures of countries. The following code tests five expressions (arithmetic mean, principal component analysis weight, and three weighting schemes emphasizing life expectancy, education, and GNI per capita respectively) using the `swap_exprs()` function. The function first locates the operation to be tested (`.var = index`), then accepts a list of expressions for testing `.expr`, before requesting the raw data:

```{r echo = TRUE, message=TRUE}
(res2 <- res %>%
  swap_exprs(
    .var = index,
    .expr = list(
      index1 = (life_exp + sch + gni_pc)/3,
      # PCA recommended weight
      index2 = 0.569 * life_exp + 0.576 * sch + 0.586 * gni_pc, 
      index3 = 0.8 * life_exp + 0.1 * sch + 0.1 * gni_pc,
      index4 = 0.1 * life_exp + 0.8 * sch + 0.1 * gni_pc,
      index5 = 0.1 * life_exp + 0.1 * sch + 0.8 * gni_pc),
    .raw_data = dt))
```

The resulting data includes five more columns corresponding to the five alternative indexes. The country ranking can be further computed on the data and plotted in a scatterplot matrix as in @fig-hdi-expr. The plot shows almost similar ranks suggested by the indexes calculated using geometric mean (`rank0`), the arithmetic mean (`rank1`), and the PCA recommended weights (`rank2`). The agreement of the three methods supports the use of geometric mean as the dimension reduction method in the original index. The scatterplot matrix also reveals the difference in ranking when the three dimensions are given different weights in `rank3`, `rank4`, and `rank5`. The panel `rank4` vs `rank5` shows a cluster of highlighted points with relatively high ranks according to `rank5` but low in `rank4`.  These countries are printed in @tbl-hdi-expr prints along with their three dimensions, calculated indexes, and rankings. While having a high life expectancy and GNI *per capita*, these countries (Andorra, Qatar, San Marino, Kuwait, and Brunei Darussalam) score relatively low in education, which is weighted heavily in `index4` (`0.1 * life_exp + 0.1 * sch + 0.8 * gni_pc`).


```{r fig-hdi-expr}
#| fig-height: 8
#| fig-width: 12
#| fig-cap: "Comparing country ranks from six different index constructions: geometric mean (`rank0`), arithmetic mean (`rank1`), principal component analysis recommended weights (`rank2`), and heavier weight on life expectancy, education, and GNI per capita (`rank3` - `rank5`). Geometric mean, arithmetic mean, and PCA produce similar rankings where countries are aligned in a diagnoal line, while country rankings varies when compare among `rank3` to `rank5`. Panel `rank5` vs. `rank4` highlights a cluster of countries that rank high by `rank5` but low by `rank4`. The plot demonstrates the necessity to test on various alternative expressions to verify the robustness of the method and to capture distinctive characteristics of different countries. "
with_rank <- res2$data %>% 
  mutate(across(contains("index"), ~rank(-.x), .names = "rank.{.col}")) %>% 
  rename_with(~gsub(".index", "", .x), contains("rank.index"))

lower_cluster <- with_rank %>% 
  filter(rank5 < 30, rank4 > 50) %>% 
  select(country, life_exp, sch, gni_pc, index3, index4, rank3, rank4)

with_highlight <- with_rank %>% 
  mutate(highlight = ifelse(country %in% lower_cluster$country, TRUE, FALSE)) %>% 
  rename(`geometric mean` = rank0, `arithmetic mean` = rank1,
         `PCA weights` = rank2, `weight 0.8 on life expec.` = rank3,
         `weight 0.8 on education`  = rank4, `weight 0.8 on GNI per capita` = rank5)

p1 <- ggpairs(
  with_highlight, columns = 14:19,
  lower = list(continuous = "points", combo = "facethist", 
               discrete = "facetbar", na = "na"),
  upper = NULL, diag = NULL,  switch = "both", 
  labeller = label_wrap_gen(width = 15, multi_line = TRUE) ) +
  theme(aspect.ratio = 1)

# https://stackoverflow.com/questions/65091593/how-to-mark-certain-point-in-ggpairs
p1$plots[[35]]$mapping <- 
    `class<-`(c(p1$plots[[35]]$mapping, aes(color = highlight)), "uneval")
p1 <- p1 + scale_color_manual(values = c("black", "red"))
p1
```

```{r echo = FALSE}
#| tbl-cap: A selected number of countries with low rankings when education is given a heavy weight (`index4`) while ranks high when a heavier weight is given on GNI per capital (`index5`).
#| label: tbl-hdi-expr
lower_cluster %>% 
  rename( `life expectancy` = life_exp, education = sch, `GNI per capita` = gni_pc) %>% 
  knitr::kable(digits = 3)
```

<!-- \newpage -->

<!-- *what if the minimum value for GNI per capita is changed?* -->

<!-- The lower limit for GNI per capita is set to be the common log of $100, with justification provided in the technical notes as: -->

<!--   > The low minimum value for gross national income (GNI) per capita, $100, is justified by the considerable amount of unmeasured subsistence and nonmarket production in economies close to the minimum, which is not captured in the official data -->

<!-- The open source data provided by United Nations Development Programme suggest the minimum GNI *per capita* is `$732` and this opens the questions on how the index/ranking will change if the lower minimum (`$100`) is raised or decreased. We can answer this question by testing on a set of different values using `.values` in the function `swap_values()`: -->

<!-- ```{r echo = TRUE, message = TRUE} -->
<!-- (res3 <- res %>% -->
<!--   swap_values( -->
<!--     .module = var_trans, .step = rescale_minmax, .res = gni_pc, -->
<!--     .var = max, .values = log10(150000), -->
<!--     .raw_data = dt) -->
<!--    )  -->
<!-- ``` -->

<!-- \newpage -->

<!-- The function computes the GNI *per capita* in both the original value (`gni_pc0`) and the new value provided (`gni_pc1`). All the subsequent operations are also computed for both cases and result in `index0` and `index1`. -->

<!-- ```{r} -->
<!-- #| fig-height: 6 -->
<!-- #| fig-width: 6 -->
<!-- #| fig-align: center -->
<!-- new_rank <- res3$data %>% -->
<!--   mutate(across(contains("index"), ~rank(-.x), .names = "rank.{.col}")) %>% -->
<!--   rename_with(~gsub(".index", "", .x), contains("rank.index")) %>%  -->
<!--   mutate(difference = rank1 - rank0) -->

<!-- try <- new_rank %>% -->
<!--   dplyr::select(id, country, contains("gni_pc"), contains("index"), contains("rank"), difference) %>%  -->
<!--   arrange(-abs(difference)) -->

<!-- new_rank %>%  -->
<!--   ggplot(aes(x = rank0, y = rank1)) +  -->
<!--   geom_point() +  -->
<!--   geom_abline(slope = 1, intercept = 0, color = "blue") + -->
<!--   # geom_text(data = try %>% filter(difference > 15), color = "red", aes(label = country)) + -->
<!--   theme(aspect.ratio = 1) + -->
<!--   xlab("log10(100) as the rescale minimum for GNI per capita") + -->
<!--   ylab("log10(700) as the rescale minimum for GNI per capita")  -->
<!-- ``` -->

\newpage 

# Conclusion

The paper presents a data pipeline with nine modules for constructing and analysing indexes. The pipeline increases transparency in the practice for index analysts to experiment with different index design and parameter choices to better design and apply their indexes. The significance of this work is its ability to provide a universal framework for index construction, which can be applied across different domains.

Examples have been given in the drought indexes and human development index to demonstrate computing of indexes with different parameters combinations and how alternative index design can provide insights to understand distinctive country characteristics that could sometimes be overlooked. The accompanied package, tidyindex, is not meant to provide comprehensive implementation for all indexes across all domains. Instead, it demonstrates implementing individual pipeline steps that are versatile to multiple indexes and composing new indexes from existing steps. Domain experts are welcomed to adopt the pipeline approach to develop specialised packages for specific-domains indexes.


# Reference
