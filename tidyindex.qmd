---
title: "A Tidy Framework and Infrastructure to Systematically Assemble Spatio-temporal Indexes from Multivariate Data"
format:
  tandf-pdf:
    keep-tex: true  
pdf-engine: pdflatex
author:
  - name: H. Sherry Zhang
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0002-7122-1463
    email: huize.zhang@monash.edu
  - name: Dianne Cook
    affiliation: 
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0002-3813-7155
  - name: Ursula Laa
    affiliation: 
      - name: University of Natural Resources and Life Sciences
        department: Institute of Statistics
        address: Gregor-Mendel-Straße 33, 1180 Wien, Austria
        city: Vienna
        country: Austria
    orcid: 0000-0002-0249-6439
  - name: Nicolas Langrené
    affiliation: 
      - name: BNU-HKBU United International College
        department: Department of Mathematical Sciences
        address: 2000 Jintong Road, Tangjiawan, Zhuhai
        city: Zhuhai, Guangdong
        country: China
    orcid: 0000-0001-7601-4618
  - name: Patricia Menéndez
    affiliation:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0003-0701-6315
abstract: |
  - indexes, useful, quantify severity, early monitoring, 
  - A huge number of indexes have been proposed by domain experts, however, a large majority of them are not being adopted, reused, and compared in research or in practice. 
  - One of the reasons for this is the plenty of indexes are quite complex and there is no obvious easy-to-use implementation to apply them to user's data. 
  - The paper describes a general pipeline framework to construct indexes from spatio-temporal data, 
  - This allows all the indexes to be constructed through a uniform data pipeline and different indexes to vary on the details of each step in the data pipeline and their orders. 
  - The pipeline proposed aim to smooth the workflow of index construction through breaking down the complicated steps proposed by various indexes into small building blocks shared by most of the indexes.
  - The framework will be demonstrated with drought indexes as examples, but appliable in general to environmental indexes constructed from multivariate spatio-temporal data
keywords:
  - indexes
  - data pipeline
  - software design
bibliography: bibliography.bib  
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
#options(width = 70)
library(tidyverse)
library(latex2exp)
library(patchwork)
library(tidyindex)
library(lmomco)
library(lubridate)
library(SPEI)
library(tsibble)
library(GGally)
library(patchwork)
library(tourr)
```

# Introduction

  <!-- - Why index is useful, why people care about indexes -->
  <!-- - Define what is an index, what is not -->
  <!-- - What is the challenges with current index construction -->
  <!-- - what can be done if people adopt this pipeline/ why it is beneficial? -->
  <!-- - who would benefit from this paper -->

Indexes are commonly used to combine and summarize different sources of information into a single number for monitoring, communicating, and decision-making. Examples of those include the Air Quality Index, El Niño-Southern Oscillation Index, Consumer Price Index, QS University Rankings or Human Development Index among many others. 

To construct an index, experts typically start by defining a concept of interest that requires measurement. This concept often lacks a direct measurable attribute or can only be measure as a composite of various processes, yet it holds social and public significance. To create an index, once the underlying processes involved are identified, relevant and available variables are then defined, collected, and combined using statistical methods into an index that aims at measure the process of interest. The construction process is often not straightforward, and decisions need to be made, such as the selection of variables to be included, which might depend on data availability and the statistical definition of the index to be used, among others. For instance, the indexes constructed from linear combination of variables need to decide on the weight assigned to each variable. Some indexes have a spatial and/or temporal components, and variables can be aggregated to different spatial resolutions and temporal scales, leading to various indexes for different monitoring purposes. Hence, all these decisions can result in different index values and have different practical implications.

To be able to test different decision choices systematically for an index, the index needs to be broken down into its fundamental building blocks to analyse the contribution and effect of each component. Such decomposition of index components provides the means to standardise index construction via a pipeline and offers benefits for comparing among indexes and calculating index uncertainty. In social indexes for instance, the OECD handbook [@oecd_handbook_2008] has provided a set of steps and recommendations for constructing composite socio-economic indexes. However, there is still a need to extend these guidelines and methods to accommodate the inclusion of more methodological complex steps required for indexes in general.

In this work, we develop a data pipeline framework where indexes from different domains can be built from. Based on the pipeline, an R package, tidyindex, is developed for practitioners to construct indexes and understand how an index responds to changes in different components in the pipeline. This work provides researchers actively developing new indexes with tools to evaluate the proposed indexes on their robustness for wider adoption. It also helps index analysts to diagnose an index to identify its weakness for methodology improvement. 

The rest of the paper is structured as follows: @sec-pipeline reviews the concept of data pipeline in R. The pipeline framework for index construction is presented in @sec-index-pipeline. @sec-dev explains how to include a new building block in each pipeline module. Examples are given in @sec-examples to demonstrate the index construction with the pipeline built. 

<!-- Evaluating indexes is challenging because the underlying concept an index measures is often a latent variable and lacks direct benchmarks for comparison. In literature, researchers typically demonstrate the accuracy of an index through comparing it to some well-known indexes, such as SPI for drought indexes, or to a relevant response variable, such as crop yield for monitoring agricultural drought. For social indexes, @uncertainty proposes a Monte Carlo algorithm to compute index uncertainty through sampling the linear weights.  -->

<!-- These sometimes make indexes difficult to reach consensus for wider use and large institutes often prefer simple indexes for more stable performance and easier interpretation. -->

<!-- When indexes have a spatial and/or temporal component, different choices on how space and time is aggregated will introduce uncertainty. A slight change in the variable value can sometimes introduce a big difference for the index.  -->

# Data pipeline {#sec-pipeline}

*Think about if there is another word for data pipeline*

**Why you should care about pipeline**

Data pipeline is not a new concept to computing. It refers to a set of data processing elements connected in series, where the output of one element is the input of the next one.
@wickham_plumbing_2009 argues that whether made explicit or not, the pipeline has to be presented in every graphics program.
The paper also argues that breaking down graphic rendering into steps is beneficial for understanding the implementation and comparing between different graphic systems. 
The discussion on pipeline construction is well documented in early interactive graphics software: @buja_elements_1988, @sutherland_orca_2000, and @xie_reactive_2014 and their pipeline steps include non-linear transformation, variable standardization, randomization and dimension reduction.

**What is pipeline, its underlying software design philosophy, and how these are reflected in R**

One of the most commonly known pipeline examples is perhaps the Unix pipeline where programs can be concatenated with `|` to flow the output from the last program into the next program, i.e. 
    
````
command 1 | command 2 | command 3 | ...
````

To solve a complex problem, the Unix system builds simple programs that do one thing well and work well together.
This design is also reflected in the tidyverse ecosystem in R.
To solve a complicated data problem using tidyverse, analysts typically build the solution using a collection of tools from the tidyverse toolbox. 
The data object can flow smoothly from one command to the next, safeguarded by the tidy data format [@wickham_tidy_2014], which prescribes three rules on how to lay out tabular data.
The tidyverse tools also embrace a strong human-centered design where function names are intuitive and easy to reference through autocomplete.
With the tidyverse design principle in mind, the tidymodel suite enables analysts to build machine learning models through the data pipeline. 
It includes typical tasks required in machine learning like data resampling, feature engineering, model fitting, model tuning, and model evaluation.
An advantage of tidymodel pipeline over separate software for individual models is that analysts no longer need to write model-specific syntax to work with each model, but pipeline-specific syntax that is applicable to all the models implemented in tidymodel.
This allows users to easily experiment with a collection of machine learning models. 

**Constructing indexes would also benefit from pipeline and embracing the aforementioned design philosophy.**

In index construction, data pipeline is often presented in a workflow diagram in the research paper to illustrate how the raw data is transformed into the final indexes. 
This agrees with Wickham's argument on the presence of the data pipeline, however, more often than not, the pipeline is not made explicit in the software.
Often the time, all the steps are lumped into a single wrapper function, rather than being split into smaller, modulated functions. 
This increases the cost of maintaining and understanding the code base, gives analysts little freedom to customise the indexes for specific needs, and hinders reusing existing code for building new indexes.
A pipeline approach unites a range of indexes under a single data pipeline and analysts can compose indexes from pipeline steps like building Legos from individual bricks.
In this workflow, analysts are not limited by indexes that have been already proposed and can easily combine pipeline steps to compose novel indexes.  Analysis of the indexes (i.e. calculation of uncertainty) is also feasible by adding external code into the pipeline.

# A pipeline for building statistical indexes {#sec-index-pipeline}

## How does the pipeline constructin of an index look like? {#sec-toy-example}

Consider a commonly used drought index: Standardized Precipitation-Evapotranspiration Index (SPEI)  [@spei]. Its construction involves: 

  1) transform the average temperature (`TMED`) into potential evapotranspiration (`pet`) 
  2) combine precipitation (`prcp`) and potential evapotranspiration (`pet`) into a single variable `diff`
  3) aggregate the difference series with a sliding window (`.scale`) 
  4) fit a distribution to the aggregated series, and 
  5) derive the index value from the normal density values.

Conventionally approach may combine all these steps into a single function, with some level of modularity. However, these modules may only work for the selected index offered by the package.

Under the pipeline approach, analysts first need to identify which module each step belongs to. 

Below shows the pseudocode for constructing SPEI with the pipeline:

````
DATA %>% 
  var_trans(.method = thornthwaite, Tave = TMED, ..., .new_name = "pet") %>%
  dim_red(diff = prcp - pet) %>%
  aggregate(.var = diff, .scale = 12, .new_name = "agg") %>%
  dist_fit(.method = "lmoms", .var = agg, .dist = DIST) %>%
  augment(.var = agg)
````

<!-- *FWI:*  -->

<!--   - *verbs as name for each step: `aggregate`, `normalise`* -->
<!--   - *use NSE for variable name: `prcp`, `c(tavg, prcp)`* -->
<!--   - *created variables are prefixed with dot: `.agg_prcp`* -->
<!--   - *avoid string and use function to avoid "gamma" vs. "gam": `gamma()`, `lmom()`* -->

The pipeline construct allows for multiple `.scales` and `.dist` to be evaluated in `aggregate()` and `dist_fit()` to compare index under different parameterisations. The result can then be passed into the `ggplot2` to crease visualisation. @fig-toy-example compares the SPEI calculated with two distributions (log-logistic and pearson III). 

Apart from evaluating multiples parameters, the pipeline approach allows the the steps written for one index can be directly extrapolate to another index building within the pipeline. The flexibility of the pipeline also integrate well with other existing packages, for examples, fitting distributions using L-moment is commonly used when constructing drought indexes. The package `lmomco` provides general L-moment fits to a wide range of distributions and users can easily access to all the distributions within the pipeline. 

<!-- additional benefit: -->

<!--   - Multiple scales and multiple distributions can be fitted using the `c(...)` syntax to compare index values constructed from different parameterisations;  -->
<!--   - Intermediate results can be checked after each step; and -->
<!--   - Additional steps and analysis can be wired into the workflow for index diagnostics and customised user need. -->

```{r out.width = "100%"}
knitr::include_graphics(here::here("figures/toy-example-spei.png"))
```


```{r fig-toy-example, eval = FALSE, echo = FALSE}
#| fig-align: center
#| fig-width: 10
#| fig-height: 3
#| fig-cap: "Standardised Precipitation Evapotranspiration Index (SPEI) calculated with two distribution fits in Step 3 described above: generalised logistic and pearson III distribution, using the `wichita` data from the package `SPEI`."
library(tidyindex)
library(lmomco)
library(ggplot2)
library(tsibble)
library(SPEI)
library(tidyverse)
library(lubridate)
data(wichita)

dt <- wichita %>%
  mutate(ym = make_yearmonth(YEAR, MONTH), id = 1) %>% 
  init(id = id, time = ym) %>%
  var_trans(.pet_method = "thornthwaite", .vars = TMED, .lat = 37.6475) %>%
  dim_red(diff = PRCP - pet) %>%
  aggregate(.var = diff, .scale = 12) %>%
  dist_fit(.dist = list(loglogistic(), pearsonIII()), .method = "lmoms", .var = .agg) %>%
  augment(.var = .agg)

dt$data %>%
  mutate(.dist = ifelse(.dist == "parglo", "Log-logistic", "PearsonIII")) %>% 
  ggplot(aes(x = ym, y = .index, color = .dist)) +
  geom_line() + 
   theme_bw() +
  theme(
    panel.grid = element_blank()
  ) +
  scale_x_yearmonth(name = "Year", date_break = "2 years", date_label = "%Y") +
  scale_color_brewer(palette = "Dark2", name = "distribution") + 
  ylab("SPEI")

```

## Pipeline steps for constructing indexes

<!-- *constructing time series index should also be encapsulated in the framework* -->

*any index can be broken down into multiple steps and then we can do things with it: swap, change parameter, etc*


*variables == indicators*

An overview of the pipeline is given in @fig-pipeline-steps to illustrate the construction from raw data to the final indexes.
The pipeline includes eight modules for operations in the spatial, temporal, and multivariate aspects of the data as well as modules for comparing and communicating indexes.
Analysts can choose a subset of modules and reorder them as needed to construct an index.

Data pre-processing may happen before the start of the index pipeline to prepare multivariate data from different locations/ countries ready for constructing an index. For remote sensing data, it includes aligning the spatial resolution and temporal frequency of data collected from different satellite products, or merging in-situ stations with the satellite data. 

The notation used for introducing the pipeline modules is as follows. Let $\mathbf{x}(\mathbf{s};\mathbf{t})$ denote the raw data with spatial, temporal, and multivariate aspects: the spatial dimension $\mathbf{s} =  (s_1, s_2, \cdots, s_n)^\prime$ is defined in the 2D space: $\mathbf{s} \in \mathcal{D}_s \subseteq \mathbb{R}^2$, the temporal dimension $\mathbf{t} = (t_1, t_2, \cdots, t_J)^\prime$ is defined in the 1D space: $\mathbf{t} \in \mathcal{D}_t \subseteq \mathbb{R}$. When more than one variable is involved, the multivariate data can also be written as: $\mathbf{x}(\mathbf{s}; \mathbf{t}) = (x_1(\mathbf{s}; \mathbf{t}), x_2(\mathbf{s}; \mathbf{t}), \cdots, x_P(\mathbf{s}; \mathbf{t}))^\prime$.

```{r fig-pipeline-steps}
#| fig-align: center
#| fig-cap: "Diagram of pipeline steps for index construction. will need to be updated with better design and the distribution fitting step."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-steps.png"))
```

### Temporal processing

The construction of an index sometimes needs to consider information from neighbouring time periods.
The temporal processing is a general operator on the time dimension of the data in the form of  

\begin{equation}
f_{\mathcal{\psi}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where $\psi \in \Psi \subseteq \mathbb{R}^{d_{\psi}}$ is the parameters associated with the temporal operation and $d_{\psi}$ is the number of parameter of $\psi$. 
A typical example of temporal processing is aggregation, which is used in the drought index SPI to measure the lack of precipitation for meteorological drought.
In SPI, monthly precipitation is aggregated by a time scale parameter $k$: $x(s_i;t_{j^\prime}) = \sum_{j = j^\prime-k+1}^{j^\prime}x(s_i; t_j),$ where $j^\prime$ is the new time index after the aggregation. In this notation, each spatial location is separately aggregated and precipitation is summed from $k$ month back, $j^\prime - k + 1$, to the current period, $j^\prime$, to create the aggregated series, indexed by $j^\prime$. 

*more explicit on k will influence 1) long term vs. short term, 2) uncertainty*

The choice of time scales parameter $k$ can result in variation in the calculated index values: a small $k$ of 3 or 6 months produces the index more sensitive to individual months, while a large $k$ of 24 or 36, an equivalent to a 2- or 3-year aggregation, gives dryness information relative to the long term condition. As will be shown in section [SECTION EXAMPLE], this variation may even lead to conflicting conclusions on the dry/wet condition of the area, highlighting the importance to account for index uncertainty when interpreting index values for decision-making.

*Effective drought index*

### Spatial processing

Spatial processing may be needed when indexes are not calculated independently on each collected location or when variables collected from multiple sources need to be fused before further processing. The process can be written as a general operation in the form of 

\begin{equation}
x(\mathbf{s}^\prime;\mathbf{t}) = g_{\mathcal{\theta}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where $\theta \in \Theta \subseteq \mathbb{R}^{d_{\theta}}$ is the associated parameters in the process and $d_{\theta}$ is the number of parameter of $\theta$. 
An example of spatial processing is to align variables collected in different resolutions.
When variables are collected at different resolutions, analysts may choose to down-sample those in a finer resolution, $i$, to match those in a coarser resolution, $i^\prime$. This is a spatial aggregation and if aggregate using the mean, it can be written as 

\begin{equation}
g(x) = \frac{\sum_{i \in i^\prime}x}{n_{i^\prime}},
\end{equation}

where $i \in i^\prime$ includes all the cells from the finer resolution in the coarser grid and $n_{i^\prime}$ is the number of observations falls into the  coarser grid. Other examples of spatial processing include 1) borrowing information from neighbouring spatial locations to interpolate unobserved locations and 2) fusing variables from ground measures with satellite imageries.

### Variable transformation 

The purpose of variable transformation is to create variables that fits assumptions for further computing. These assumptions include a stable variance, normal distribution, or a certain scale required by some algorithms down the pipeline. Variable transformation is a general notion of a functional transformation on the variable: 

\begin{equation}
h_{\tau}(x(\mathbf{s};\mathbf{t})),
\end{equation} 

where $\tau \in T \subseteq \mathbb{R}^{d_{\tau}}$ is the parameter in the transformation if any, and $d_{\tau}$ is the number of parameter of $\tau$. Transformation is needed for data that are highly skewed and some common transformations include log, quadratic, and square root transformation.

<!-- A common assumption is normality in drought index.  -->

### Scaling

While scaling can be seen as a specific type of variable transformation, it is separated into its own step to make the step explicit in the pipeline.
The key difference between the two steps is that variable transformation typically changes the shape of the data while scaling only changes the data scale and can usually be written in the form of 

\begin{equation}
[x(s_i;t_j) - \alpha]/\gamma.
\end{equation}

For example, a z-score standardisation can be written in the above form with $\alpha = \bar{x}(s; t)$ and $\gamma = \sigma(s; t)$, a min-max standardisation uses $\alpha = \min[x(s_i, t_j)]$ and $\gamma = \max[x(s_i, t_j)] - \min[x(s_i, t_j)]$.
@fig-scale-var-trans-compare shows a collection of variable pre-processing operations and uses color to differentiate whether the operation is a variable transformation or a scaling step. While both variable transformation and scaling are pre-processing steps, the scaling operations in green show the same distribution as the original data. 

<!-- Take one or multiple variables to create a new variable,  -->
<!-- Restrict it to single variable, square root, log etc could be linearly, also non-linear -->

<!-- change the shape of the variable -->

<!-- <!-- A conversion of non-normal variables into (approximately) normal. wrong! -->

<!-- GAM, can you do additive model pairwise/ three-way -->

<!-- While being a common technique in data pre-processing, scaling has an impact on the outliers of the data and analysts need to be cautious on checking its effect. -->


```{r fig-scale-var-trans-compare}
#| fig-cap: "Comparison of operations in scaling (green) and variable transformation (orange) steps in free scale. Variables after the scaling operations have the same distribution as the origin, while the distribution changes after variable transformation. "
#| fig-height: 8
#| fig-width: 12
set.seed(123)
x <- rgamma(n = 1000, shape = 1, scale = 2)


trans_df <- tibble(origin = x, 
       `z-score` = (x - mean(x)/ sd(x)), 
       qudratic = x^2,
       log = log(x),
       `square root` = sqrt(x),
       `cubic root` = x^(1/3),
       minmax = (x - min(x))/ (max(x) - min(x)),
       boxcox = (x^0.5 - 1)/0.5,
       centering = x - mean(x)
       ) %>% 
  mutate(id = row_number(),
         ) %>% 
  pivot_longer(-id, names_to = "var", values_to = "value") %>% 
  mutate(category = ifelse(
    var %in% c("origin", "z-score", "minmax", "centering"),
    "Scaling", "Var. Trans.")
    ) 

latex_df <- tibble(
  var = c("origin", "z-score", "qudratic", "log", "square root", "cubic root",
          "minmax", "boxcox", "centering"),
  latex = c(
           r"($x$)",
           r"($\frac{x - \bar{x}}{sd(x)}$)",
           r"($x^2$)",
           r"($log(x)$)",
           r"($\sqrt{x}$)",
           r"($x^{1/3}$)",
           r"($\frac{x - min(x)}{max(x) - min(x)}$)",
           r"($\frac{x^{0.5} - 1}{0.5}$)",
           r"($x - \bar{x}$)"
         )
) %>% 
  left_join(trans_df %>% group_by(var) %>% summarise(max = max(value)))

trans_df %>% 
  ggplot() +
  geom_density(aes(x = value, color = category, y = after_stat(scaled))) + 
  geom_label(data = latex_df, 
             aes(x = 0.8 * max, y = 0.8, label = TeX(latex, output = "character")), 
             parse = TRUE) + 
  facet_wrap(vars(var), scales = "free") + 
  scale_color_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(legend.position = "bottom")
```


\newpage

### Dimension reduction

Dimension reduction summarises the multivariate information into univariate, which can be denoted as: 

\begin{equation}
x_p(\mathbf{s}; \mathbf{t}) \rightarrow x(\mathbf{s}; \mathbf{t}),
\end{equation}

where $p = 1, 2, \cdots, P$. The combination can be based on domain-specific knowledge, originated from theories describing the underlying physical process. For example, the SPEI uses a water balance model ($D = P - \text{PET}$) to calculate the difference series ($D$) from precipitation ($P$) and potential evapotranspiration ($\text{PET}$). 

Another widely used approach is linear combination, which aggregates a collection of variables in a linear additive structure, expressed as: $$x(\mathbf{s}; \mathbf{t}) = \sum_{p = 1}^{P}\lambda_{p}x_p(\mathbf{s};\mathbf{t}),$$

where $\lambda_p$ denotes the weight assigned to variable $x_p$. Most indexes uses an equal weight that sum to 1, which allows each variable to contribute equally to the index. A linear combination can also be viewed as a linear projection of multivariate information by the weight vector. Projecting data from higher to lower dimension inevitably leads to information loss. For example, two countries can receive similar index, but their components could significantly differ -- one with average scores across all components and another with extreme scores the opposite ends. The selected set of weights need to be examined to understand its effect on the index and its implications for decision-making. To do this, analysts can vary the coefficients in the linear projection to observe how the index value and countries' ranking change. These changes can also be visualized using a method called tour by generating an animation of the projections among different sets of weights.

<!-- Other weight constraint used includes $\sum_{p=1}^P\lambda_p^2 = 1$, is used by principal component analysis.  -->


<!-- Spectral satellite imageries often collected at different bands and it is common to combine values from multiple bands to compose new variables relevant to the index. The most common example of this is the Normalized Difference Vegetation Index (NDVI), which uses near-infrared (NIR) ($x_1(s;t)$) and red channel ($x_2(s;t)$): -->

<!-- \begin{equation} -->
<!-- \text{NDVI}(s;t) = \frac{x_1(s;t) - x_2(s;t)}{x_1(s;t) + x_2(s;t)}, -->
<!-- \end{equation} -->

<!-- which is used to evaluate the vegetation from near-infrared (NIR) and red channel. -->



### Distribution fit

*model fit? *

<!-- With a probability model imposed,  -->
Distribution fit can be seen as the model fitting in its simplest term. It can be represented by 

\begin{equation}
F_{\eta}(x(\mathbf{s}; \mathbf{t})), 
\end{equation}

where $\eta \in H \subseteq \mathbb{R}^{d_{\eta}}$ is the distribution parameter and $d_{\eta}$ is the number of parameter of $\eta$. A distribution fit typically aims at finding the distribution that best fits the data. 
Analysts may start from a pool of candidate distributions with a chosen fitting method and goodness of fit measure. 
While it is useful to find the ultimate best distribution to fits the data, from a probabilistic perspective, the fitting procedure itself has an uncertainty associated with the data fed and the parameter chosen. A reasonable alternative is to understand how much the index values can vary given different distributions, fitting methods, and goodness of fit tests, and whether these variations are negligible in a given application.

### Normalising

This step maps the univariate series into a different scale, typically for ease of comparison across regions. 
For example, a normal scale, [0, 1], or [0, 100] may be favored for reporting certain indexes. 
In drought indexes, i.e. SPI or SPEI, the quantiles from the fitted distribution are converted into the normal scale via the normal reverse CDF function: $\Phi^{-1}(.)$. 
Normalising is usually used at the end of the pipeline and its main difference from the scaling step is that here the change of scale also changes the distribution of the variable. 
While being commonly used, this step can get criticism from analysts for forcing the data into the decided scale, which can be either unnecessary or inaccurately exaggerate or downplay the outliers. 
Also, the use of a normal scale needs to be interpreted with caution. 
@fig-normalising illustrates the normal density not being directly proportional to its probability of occurrence.
This is concerning, especially at the extreme values, since a small difference in the tail density can have magnitudes of difference in its probability of occurrence.

```{r fig-normalising}
#| fig-cap: "Scatterplot of normal quantiles against their density values. THree tail density values are highlighted with its probability of occurence labelled. Probability is calculated assuming monthly data: with a density of -2, the probability of occurrence is 1/pnorm(-2)/12 = 4 years. The non-linear relationship between the two quantities suggests normalised indexes need to be interpreted with caution since a slight change in the tail distribution can result in magnitudes of difference in its probability of occurrence."
#| fig-width: 9
library(tidyverse)
all <- tibble(x = seq(-3, 3, 0.01), y = pnorm(x)) 
dt <- tibble(x = c(-3, -2.5, -2), y = pnorm(x)) 
label <- tibble(x = c(-3, -2.5, -2), y = pnorm(x), 
                label = c("once in 62 years", "once in 13 years", "once in 4 years"))

dt %>% 
  ggplot() + 
  geom_point(aes(x= x, y = y)) + 
  geom_line(data = all, aes(x = x, y = y)) + 
  ggrepel::geom_label_repel(
    data = label, aes(x = x, y = y, label =label),
    nudge_y = 0.2, nudge_x = 0.1,
    arrow = arrow(length = unit(0.08, "inches")), min.segment.length = 0) +
  scale_x_continuous(breaks = seq(-3, 3, 1)) + 
  xlab("Normalised Score") + 
  ylab("Probability") + 
  theme_bw() + 
  theme(panel.grid.minor = element_blank())
```

### Benchmarking

Benchmarking sets a constant value to allow the constructed index to be compared across time.
Here we denote it with $u[x(s_i, t_j)]$ where $u$ is a scalar of interest in the index constructed. A benchmark value could be a constant or a function of the data, i.e. mean.

### Simplification

In public communication, the index values are usually accompanied by a categorical grade. The categorised grades are an ordered set of descriptive words or colors to communicate the severity or guide the comprehension of the indexes. 
The mapping from continuous index values to the discrete grades is called simplification in the pipeline and it can be written as a piece-wise function: 

\begin{equation}
\begin{cases}
C_0 & c_1 \leq (s_i; t_j) < c_0 \\
C_1 & c_2 \leq x(s_i; t_j) < c_1 \\
C_2 & c_3 \leq x(s_i; t_j) < c_2 \\
\cdots \\
C_z & c_z \leq x(s_i; t_j)
\end{cases}
\end{equation}

where $C_0, C_1,\cdots ,C_z$ are the categories and $c_0, c_1, \cdots, c_z$ are the thresholds for each category. In SPI, droughts are sorted into four categories: mild drought: $[-0.99, 0]$; moderate drought: $[-1.49, -1]$; severe drought: $[-1.99, -1.5]$, and extreme drought: $[-\infty, -2]$. In this case, $C_0, C_1, C_2, C_3$ are the drought categories: mild, moderate, severe, and extreme drought ($z = 3$) and $c_0 =0, c_1 = -1, c_2 = -1.5, c_3 = -2$ are the cutoff value for each class.

# Incorporating alternative methods into the pipeline components {#sec-dev}


<!-- # 1. unpack or the argumebt s  -->
<!-- # 2. construct the syntax to evalute: ~rescale_minmax(life_exp, min = xxx, max = xxxx) -->
<!-- # 3. evalute  -->
<!-- # 4. update the data object  -->




<!-- # [Extending the pipeline] a different name here: alternate a module with a different (method) -->

# Examples {#sec-examples}

This section uses the example of drought and social indexes to show the analysis made possible with the index pipeline. The drought index example computes two indexes with various time scales and distributions simultaneously using the pipeline framework to understand the flood and drought events in Queensland. The social index example focuses on the dimension reduction in Global Gender Gap Index to explore the impact of weight changes in linear combination on index value and country ranking. 

## Every distribution, every scale, every index all at once

A common task for drought researchers is to compute indexes at different parameter combinations. This can be used to identify the spatial and temporal extent of drought events, recommend the best parameter choice, or compare the effectiveness of indexes for monitoring drought. The example below computes two indexes: SPI and SPEI, at various time scales and fitted distributions, for stations in the state of Queensland in Australia. The purpose of the example is to demonstrate the interfaces the tidyindex package built to allow easy computing at different parameter combinations. 

The state of Queensland in Australia is frequently affected by natural disaster events such as flood and drought, which can have significant impacts on its agricultural industry.  This study uses daily data from Global Historical Climatology Network Daily (GHCND), accessed via the package `rnoaa` to examine drought/flood condition in Queensland. Daily data is average into monthly and stations are excluded if monthly data contains missings, which is required for calculating both SPI and SPEI. This gives `r length(unique(queensland$id))` stations with complete records from 1990 January to 2022 April.

The function `compute_indexes()` can be used to collectively compute multiple indexes. The tidyindex offers wrapper functions, with the prefix idx_, that simplify the calculation of commonly used indexes by combining a set of pipeline steps into a single function. For example, the function `idx_spei()` includes the five steps previously described in @sec-toy-example (variable transformation, dimension reduction, temporal aggregation, distribution fit, and normalise). Each `idx_xxx()` function specifies the relevant parameters relevant to the index: the thornthwaite method is used to calculate PET in SPEI, with the average temperature (`tavg`) and latitude (`lat`) used as inputs. The SPEI is computed at four time scales (6, 12, 24, and 36 months) and fitted with two distributions (Log-logistic and General Extreme Value (GEV)). The SPI is also computed at the same four time scales and uses the default gamma distribution to fit the aggregated series. 

```{r cache=TRUE, echo = TRUE}
.scale <- c(6, 12, 24, 36)
(idx <- queensland %>%
  init(id = id, time = ym) %>%
  compute_indexes(
    spei = idx_spei(
      .pet_method = "thornthwaite", .tavg = tavg, .lat = lat,
      .scale = .scale, .dist = c(gev(), loglogistic())),
    spi = idx_spi(.scale = .scale)
  ))
```

The output from `compute_indexes()` contains index values and associated parameter in a long tibble. It includes the original variables (`id`, `ym`, `prcp`, `tmax`, `tmin`, `tavg`, `long`, `lat`, and `name`), index parameters (`.idx`, `.scale`, `.method`, and `.dist`), intermediate variables (`pet`, `.agg`, and `.fitted`), and the final index (`.index`). This data can be visualised across space or time, or simultaneously, to explore the wet/dry condition in Queensland. @fig-compute-spatial visualises the spatial distribution of SPI at two periods (2010 October - 2011 March and 2019 October - 2020 March) with significant natural disaster events: 2010/11 Queensland flood and 2019 Australia drought, which contributes to the notorious 2019/20 bushfire. @fig-compute-temporal displays the sensitivity of the SPEI series for one particular station, Texas post office, at different time scales and fitted distributions. These two plots demonstrate some possibilities to explore the indexes after they are computed from `compute_indexes()`. 


```{r}
library(sf)
qld_lga <- absmapsdata::lga2018 %>%
  filter(state_name_2016 == "Queensland") %>%
  rmapshaper::ms_simplify(keep = 0.3) %>%
  mutate(lga_name_2018 = str_replace(lga_name_2018, " \\([S|R|C|]\\)", ""),
         lga_name_2018 = str_replace(lga_name_2018, " \\(Qld\\)", ""))
```

```{r fig-compute-spatial}
#| fig-cap: "Spatial distribution of Standardized Precipitation Index (SPI-12) in Queensland, Australia during two major flood and drought events: 2010/11 and 2019/20. The map shows a continous wet period during the 2010/11 flood period and a mitigated drought situation, after its worst in 2019 December and 2020 Janurary, likely due to the increased rainfall in February from the meteorological record."
#| fig-width: 12
#| fig-height: 5
queensland_map <- ozmaps::abs_ste %>% filter(NAME == "Queensland") %>% 
  rmapshaper::ms_simplify(keep = 0.02)
queensland_map %>%
  ggplot() +
  geom_sf(fill = "transparent", color = "grey50", linewidth = 0.4) +
  geom_point(
    data = idx %>%
      filter(.idx == "spi", .scale == 12) %>%
      filter(((year(ym) %in% c(2010, 2019) & month(ym) >= 10) )|
               ((year(ym) %in% c(2011, 2020) & month(ym) <= 3) )), 
    aes(x = long, y = lat, color = .index)) +
  colorspace::scale_color_continuous_divergingx(palette = "Geyser", rev = TRUE, name = "Index") + 
  theme_void() +
  facet_wrap(vars(ym), ncol = 6)
```

```{r fig-compute-temporal}
#| fig-cap: 'Time series plot of Standardized Precipitation-Evapotranspiration Index (SPEI) at the Texas post office station (highlighted by a diamond shape in panel a). The SPEI is calculated at four time scales (6, 12, 24, and 36 months) and fitted with two distributions (Log Logistic and GEV). The dashed line at -2 represents the class "extreme drought" by the SPEI. A larger time scale gives a smoother index series, while also takes longer to recover from an extreme situation as seen in the  2019/20 drought period. The SPEI values from two distribution fits mostly agree, while GEV can results in more extreme values, i.e. in 1998 and 2020.'
#| fig-width: 12
#| fig-height: 6
texas <- idx %>%
    filter(name == "TEXAS POST OFFICE", .idx == "spei") %>% distinct(long, lat)
p1 <- queensland_map %>%
  ggplot() +
  geom_sf(fill = "transparent") +
  geom_point(data = queensland %>%
               distinct(id, long, lat, name),
             aes(x = long, y = lat)) +
  geom_point(data = texas, aes(x = long, y = lat),
             color = "orange", shape = 18, fill = "orange", size = 4) +  
  theme_void()


p2 <- idx %>%
  filter(name == "TEXAS POST OFFICE", .idx == "spei") %>%
  mutate(.dist = ifelse(.dist == "pargev", "GEV", "Log Logistic")) %>%
  rename(scale = .scale) %>% 
  ggplot(aes(x = ym, y = .index, color = .dist, group = .dist)) +
  geom_line() +
  theme_benchmark() +
  facet_wrap(vars(scale), labeller = label_both, ncol = 1) +
  scale_x_yearmonth(breaks = "2 year", date_labels = "%Y") +
  scale_color_brewer(palette = "Dark2", name = "Distribution") + 
  xlab("Year") + 
  ylab("Index")

(p1 | p2)  + 
  patchwork::plot_annotation(tag_levels = "a") + 
  patchwork::plot_layout(guides = "collect")  &
  theme(legend.position = "bottom") 
  
```

## Does a minor change in variable weights cause a tornado? 

The Global Gender Gap Index (GGGI), published annually by the World Economic Forum, measures gender parity by assessing relative gaps between men and women in four key areas: Economic Participation and Opportunity, Educational Attainment, Health and Survival, and Political Empowerment [@WEF2023]. The index is composed of 14 variables, expressed as female-to-male ratios, which are first aggregated in a linear combination into the four dimensions using the weight from the `V-weight` column in @tbl-gggi-weights. The weight is calculated as the inverse of the standard deviation of each variable and scaling to sum to 1 within each dimension to allow a one percentage point change in the standard deviation of each variable to contribute equally to the index. The four dimensions are then aggregated in another linear combination with equal weight to obtain the index. The 2023 GGGI data is available from the Global Gender Gap Report 2023 in the country's economy profile and can be accessed in R via the `tidyindex` package as `gggi`, along with the corresponding weights `gggi_weights`.

```{r tbl-gggi-weights}
#| tbl-cap: Weights of the fourteen variables in Global Gender Gap Index
#| tbl-cap-location: bottom
gggi_weights %>%  
   mutate(variable = str_replace_all(variable, "_", " ") %>%
            str_to_sentence()) %>% 
  select(c(1,2, 5:7)) %>% 
  mutate(dimension = c(rep("Economy", 5), rep("Education" ,4), 
                       rep("Health", 2), rep("Politics", 3))) %>% 
  knitr::kable(
    digits = 3, 
    col.names = c("Variable", "Dimension", "V-weight",
        "D-weight", "Weight")
    ) 
```

A natural thing to do when provided with the index data is to reproduce the index. This helps index analysts to verify the index calculation and become familiar with the methodology. For GGGI, the construction can be simplified as a single linear aggregation step in the dimension reduction module, with the `Weight` column in @tbl-gggi-weights, which is the product of the variable weight (`V-weight`) and dimension weight (`D-weight`). 

````
gggi %>% 
  init(id = country) %>%
  add_meta(gggi_weights, var_col = variable) %>% 
  dimension_reduction(
    index_new = aggregate_linear(
      ~labour_force_participation:years_with_female_head_of_state,
      weight = weight)) 
````

The result can be compared with the GGGI values available in the report as shown in @fig-compare-gggi, validating the reproducibility of the index for country with no missing variables.

```{r eval = FALSE}
a <- dim_weights %>% select(-variable) %>% t() %>% as_tibble(rownames = "id")
colnames(a) <- c("id", dim_weights$variable)
a %>% 
  pivot_longer(-id, names_to = "vars", values_to = "value") %>% 
  ggplot(aes(x = value, group = vars)) + 
  geom_histogram() + 
  facet_wrap(vars(vars))
```

```{r fig-compare-gggi}
#| fig-cap: "Verifying the calculation of the Global Gender Gap Index (GGGI). The index can be reproduced from the methodology described in the Global Gender Gap Report 2023 after removing the countries with missing variables, the treatment of which is unclear."
set.seed(123)
n <- 1000
w_idx <- map(1:n, function(x){
  w_idx <- stats::runif(4, min = 0.1, max = 1)
  w_idx <- w_idx/sum(w_idx)
  w_idx %>% as.matrix(nrow = 4, ncol = 1)
  })
rand_w <- reduce(w_idx, cbind) %>% as_tibble()
colnames(rand_w) <- paste0("V", 1:n)

dt2 <- gggi %>% 
  init(id = country) %>%
  add_meta(gggi_weights, var_col = variable) %>% 
  dimension_reduction(
    index_new = aggregate_linear(
      ~labour_force_participation:years_with_female_head_of_state,
      weight = weight)) 
  
dt2$data %>%
  filter(!is.na(index_new)) %>% 
  ggplot(aes(x = index, y = index_new)) +
  geom_abline(slope = 1, intercept = 0, color = "grey") + 
  geom_point() +
  theme_bw() + 
  theme(aspect.ratio = 1, panel.grid.minor = element_blank()) + 
  xlab("Benchmark") + 
  ylab("Index calculated")
```

```{r eval = FALSE, gggi-weights, fig.height=20, fig.align='center', fig.width=10}
dt3 <- dt2 %>%
  swap_values(.id = 1, .param = weight,
              .value = list(pillar_weight, paste0("V", 2:100)),
              .raw_data = dt)

idx_wide <- dt3$data %>% select(country, dplyr::contains("index_new"))
idx_wide2 <- idx_wide %>% mutate(index_avg = rowMeans(idx_wide[,2:(ncol(idx_wide))]))
idx_long2 <- idx_wide2 %>% pivot_longer(-country, names_to = "index", values_to = "value") %>% 
  left_join(gggi %>% select(country, rank), by = "country")

idx_long2 %>%
  ggplot(aes(x = value, y = fct_reorder(country, -rank), group = country)) +
  geom_boxplot() + 
  #geom_point(color = "grey80", size = 0.5) +
  geom_point(data = idx_long2 %>% filter(index == "index_new1"), color = "red") + 
  theme_bw() + 
  ylab("Country") + 
  xlab("Index")
```

To understand the uncertainty of this dimension reduction step while avoiding the missingness issue on the variable level, we can run a local tour to slightly vary the weight of each dimension to see how index value and country ranking changes. We select countries in the South Asia and Sub-Saharan Africa region and gradually increase the weight of one variable and reduce back to equal weight all four dimensions, one at a time, to produce an animation of how GGGI changes in each country. Five frames (equal weights and one for each dimension with a relatively higher weight) selected from the tour animation are shown in @fig-idx-tour and you can find the link to the full animation in figure caption. 

Many insights can be derived from the animation and these selected frames. These can be helpful in understanding the sensitivity of the index value and country ranking to each variable and identifying the strengths and weaknesses of each country relative to others. When economy and education are given a greater weight, the variation in index values and ranking highlights countries that performs relatively better (moving right) or worse (moving left) in these two single dimensions. For example, the index moves to the left for Ethiopia and Senegal when economy is given a higher weight. This reveals the economy as a weakness for these two countries compared to their similarly-ranked peers. When health receives a higher weight, the impact on the index value is minimal. Notably, increasing the weight in politics leads to a pronounced decrease of index value for almost all the countries. Given the index value has a direct interpretation on how much of the gender gap has been closed, such a weight variation can cast a negative light on the progression to gender parity. Analysts need to further investigate the construction of the politics dimension and consider whether this is desirable for the index.
 
```{r eval = FALSE}
dt <- tidyindex::gggi %>% 
  dplyr::select(country: political_empowerment) %>%
  filter(country %in% c("Namibia", "South Africa", "Bangladesh", "Botswana",
                        "Ethiopia", "Kenya", "Senegal", "Afghanistan")) %>%
  arrange(index) 
colnames(dt) <- c(colnames(dt)[1:4], "economy", "education", "health", "politics")

w_proj2idx <- function(w){w/sum(w)}
w_idx2proj <- function(w){w/(sqrt(sum(w^2))) %>% matrix(nrow = length(w))}
find_next <- function(matrix){
  start <- matrix(c(0.25, 0.25, 0.25, 0.25), ncol = 1)
  tourr:::step_angle(tourr:::geodesic_info(start, matrix), 0.4)
}
b <- array(dim = c(4, 1, 9))
b[,,c(1,3,5,7,9)] <- matrix(c(0.25, 0.25, 0.25, 0.25), ncol = 1) %>% w_idx2proj()
b[,,2] <- matrix(c(0.7, 0.1, 0.1, 0.1), ncol = 1) %>% find_next()
b[,,4] <- matrix(c(0.1, 0.7, 0.1, 0.1), ncol= 1) %>% find_next()
b[,,6] <- matrix(c(0.1, 0.1, 0.5, 0.1), ncol= 1) %>% find_next()
b[,,8] <- matrix(c(0.1, 0.1, 0.1, 0.5), ncol = 1) %>% find_next()

render(
  dt[,5:8], planned2_tour(b), dev = "png",
  display_idx(cex = 4, label_cex = 3.5, panel_height_ratio = c(3,1), 
              axis_cex = 2.5, weight_cex = 3, label_col = "grey70", 
              label = dt$country, abb_vars = FALSE),
  width = 900, height = 900, apf = 1/20, frames = 100,
  here::here("figures/idx-tour/idx-tour-%03d.png")
)

gifski::gifski(
  list.files(here::here("figures/idx-tour"), full.names = TRUE),
  gif_file = here::here("figures/idx-tour.gif"),
  delay = 0.15, width = 900, height = 900)

```

```{r fig-idx-tour, fig.align='center', fig.height=8} 
#| fig-cap: "Five frames selected from varying the linear weights of four dimensions in Global Gender Gap Index. The weights vary slightly from the official simple average weights (0.25, 0.25, 0.25, 0.25) to observe how the index and ranking reponse. Full animation is available at https://vimeo.com/847874016?share=copy."
frames <- c("001", "013", "037", "061", "085")
ani <- paste0(here::here("figures/"), "idx-tour/", "idx-tour-", frames, ".png")
rl <- lapply(ani, png::readPNG)
gl <-  lapply(rl, grid::rasterGrob)
wrap_plots(gl, ncol = 2)  
```

# Conclusion

The paper presents a data pipeline with nine modules for constructing and analysing indexes. The pipeline increases transparency in the practice for index analysts to experiment with different index design and parameter choices to better design and apply their indexes. The significance of this work is its ability to provide a universal framework for index construction, which can be applied across different domains.

Examples have been given in the drought indexes and human development index to demonstrate computing of indexes with different parameters combinations and how alternative index design can provide insights to understand distinctive country characteristics that could sometimes be overlooked. The accompanied package, tidyindex, is not meant to provide comprehensive implementation for all indexes across all domains. Instead, it demonstrates implementing individual pipeline steps that are versatile to multiple indexes and composing new indexes from existing steps. Domain experts are welcomed to adopt the pipeline approach to develop specialised packages for specific-domains indexes.

Future work:
  - integrate more complex dimension reduction methods to calculate weights 
  - strengthen the spatial processing module


# Reference
