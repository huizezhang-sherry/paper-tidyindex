---
title: "A Tidy Framework and Infrastructure to Systematically Assemble Spatio-temporal Indexes from Multivariate Data"
format:
  tandf-pdf:
    keep-tex: true  
pdf-engine: pdflatex
author:
  - name: H. Sherry Zhang
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0002-7122-1463
    email: huize.zhang@monash.edu
  - name: Dianne Cook
    affiliation: 
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0002-3813-7155
  - name: Ursula Laa
    affiliation: 
      - name: University of Natural Resources and Life Sciences
        department: Institute of Statistics
        address: Gregor-Mendel-Straße 33, 1180 Wien, Austria
        city: Vienna
        country: Austria
    orcid: 0000-0002-0249-6439
  - name: Nicolas Langrené
    affiliation: 
      - name: BNU-HKBU United International College
        department: Department of Mathematical Sciences
        address: 2000 Jintong Road, Tangjiawan, Zhuhai
        city: Zhuhai, Guangdong
        country: China
    orcid: 0000-0001-7601-4618
  - name: Patricia Menéndez
    affiliation:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0003-0701-6315
abstract: |
  Indexes are useful for summarizing multivariate information into single metrics for monitoring, communicating, and decision-making. While most work has focused on defining new indexes for specific purposes, more attention needs to be directed towards making it possible to understand index behavior in different data conditions, and to determine how their structure affects their values and variation in values. Here we discuss a modular data pipeline recommendation to assemble indexes. It is universally applicable to index computation and allows investigation of index behavior as part of the development procedure. One can compute indexes with different parameter choices, adjust steps in the index definition by adding, removing, and swapping them to experiment with various index designs, calculate uncertainty measures, and assess indexes' robustness. The paper presents three examples to illustrate the pipeline framework usage: comparison of two different indexes designed to monitor the spatio-temporal distribution of drought in Queensland, Australia; the effect of dimension reduction choices on the Global Gender Gap Index (GGGI) on countries' ranking; and how to calculate bootstrap confidence intervals for the Standardized Precipitation Index (SPI). The methods are supported by a new R package, called `tidyindex`.
keywords:
  - indexes
  - data pipeline
  - software design
  - uncertainty
  - decision-making
bibliography: bibliography.bib  
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
options(width = 70)
library(tidyverse)
library(latex2exp)
library(patchwork)
library(tidyindex)
library(lmomco)
library(lubridate)
library(SPEI)
library(tsibble)
library(GGally)
library(patchwork)
library(tourr)
```

# Introduction

<!-- - Why index is useful, why people care about indexes -->

<!-- - Define what is an index, what is not -->

<!-- - What is the challenges with current index construction -->

<!-- - what can be done if people adopt this pipeline/ why it is beneficial? -->

<!-- - who would benefit from this paper -->

Indexes are commonly used to combine and summarize different sources of information into a single number for monitoring, communicating, and decision-making. They serve as critical tools across the natural and social sciences. Examples include the Air Quality Index, El Niño-Southern Oscillation Index, Consumer Price Index, QS University Rankings, and the Human Development Index. In environmental science, climate indexes are produced by major monitoring centers, like the United States Drought Monitor and National Oceanic and Atmospheric Administration, to facilitate agricultural planning and early detection of natural disasters.  In economics, indexes provide insight into market trends through combining prices of a basket of goods and services. In social sciences, indexes are used to monitor human development, gender equity, or university quality. The problem is that every index is developed in its own unique way, by different researchers or organizations, and often indexes designed for the same purpose cannot easily be compared. 

To construct an index, experts typically start by defining a concept of interest that requires measurement. This concept often lacks a direct measurable attribute or can only be measured as a composite of various processes, yet it holds social and public significance. To create an index, once the underlying processes involved are identified, relevant and available variables are then defined, collected, and combined using statistical methods into an index that aims to measure the process of interest. The construction process is often not straightforward, and decisions need to be made, such as the selection of variables to be included, which might depend on data availability and the statistical definition of the index to be used, among others. For instance, the indexes constructed from a linear combination of variables require to decide the weight assigned to each variable. Some indexes have a spatial and/or temporal component, and variables can be aggregated to different spatial resolutions and temporal scales, leading to various indexes for different monitoring purposes. Hence, all these decisions can result in different index values and have different practical implications.

To be able to test different decision choices systematically for an index, the index needs to be broken down into its fundamental building blocks to analyze the contribution and effect of each component. We call this process of breaking the index construction into different steps the index pipeline. Such decomposition of index components provides the means to standardize index construction via a pipeline and offers benefits for comparing among indexes,  calculating index uncertainty, and assessing index robustness. 

In this work, we provide statistical and computational methods for developing a data pipeline framework to construct and customize indexes using data. The proposed pipeline comprises various modules, including temporal and spatial aggregation, variable transformation and combination, distribution fitting, benchmark setting, and index communication. Given the decisions analysts need to make when combining multivariate data into indexes, the proposed pipeline enables the evaluation of how the specific choice can affect the index, as well as how the index may appear under alternative options. Furthermore, uncertainty calculation can also flow through the pipeline, providing the index with confidence measures. 

The rest of the paper is structured as follows. @sec-idx-dev provides background about the development of indexes. @sec-tidy reviews the tidy framework in R and how index construction can benefit from such a framework. The details of the pipeline modules are presented in @sec-pipeline. @sec-software explains the design of the `tidyindex` package that implements the modules. Examples are given in @sec-examples to illustrate three use cases of the pipeline.

<!-- Evaluating indexes is challenging because the underlying concept an index measures is often a latent variable and lacks direct benchmarks for comparison. In literature, researchers typically demonstrate the accuracy of an index through comparing it to some well-known indexes, such as SPI for drought indexes, or to a relevant response variable, such as crop yield for monitoring agricultural drought. For social indexes, @uncertainty proposes a Monte Carlo algorithm to compute index uncertainty through sampling the linear weights.  -->

<!-- These sometimes make indexes difficult to reach consensus for wider use and large institutes often prefer simple indexes for more stable performance and easier interpretation. -->

<!-- When indexes have a spatial and/or temporal component, different choices on how space and time is aggregated will introduce uncertainty. A slight change in the variable value can sometimes introduce a big difference for the index.  -->

# Background to index development {#sec-idx-dev}

<!-- * point to major review papers describing index dev  -->
<!-- * point to major uses of indexes  -->

There are many documents providing advice on how to construct indexes for different fields, and review articles describing the range of available indexes for specific purposes. The OECD handbook [@oecd_handbook_2008] provides a comprehensive guide for computing socio-economic composite indexes, with detailed steps and recommendations. The drought index handbook [@svoboda2016handbook] provides details of various drought indexes and recommendations from the World Meteorology Organization.  @zargar2011review, @hao_drought_2015 and @alahacoon_comprehensive_2022 are review papers describing the range of possible drought indexes. 

There is also some attention being given to the diagnosis of indexes, and incorporation of uncertainty. @jones_vulnerability_2007 investigates the methodological choices made in the development of indexes for assessing vulnerable neighborhoods. @uncertainty describes incorporating uncertainty estimates and conducting sensitivity analysis on composite indexes. @tate_social_2012 and @tate_uncertainty_2013, similarly, make a comparative assessment of social vulnerability indexes based on uncertainty estimation and sensitivity analysis. @laimighofer_how_2022 studies five uncertainty sources (record length, observation period, distribution choice, parameter estimation method, and GOF-test) of drought indexes. 

There are also a few R packages supporting index calculation. The  `SPEI` package  [@spei] computes two drought indexes. The `gpindex` package [@gpindex] computes price indexes, and the `fundiversity` package [@fundiversity] computes functional diversity indexes for ecological study. The package `COINr` [@COINr] is more ambitious, making a start on following the broader guidelines in the OECD handbook to construct, analyze, and visualize composite indexes. 

From reviewing this literature, and in the process of developing methods for making it easier to work with multivariate spatio-temporal data, it seems possible to think about indexes in a more organised, cohesive and standard manner. Actually, the area could benefit from a *tidy* approach. 

# Tidy framework {#sec-tidy}

<!-- **Why you should care about pipeline** -->

The tidy framework consists of two key components: tidy data and tidy tools. The concept of tidy data [@wickham_tidy_2014] prescribes specific rules for organizing data in an analysis, with observations as rows, variables as columns, and types of observational units as tables. Tidy tools, on the other hand, are concatenated in a sequence through which the tidy data flows, creating a pipeline for data processing and modeling. These pipelines are data-centric, meaning all the tidy tools or functions take a tidy data object as input and return a processed tidy data object, directly ready for the next operations to be applied. Also, the pipeline approach corresponds to the modular programming practice, which breaks down complex problems into smaller and more manageable pieces, as opposed to a monolithic design, where all the steps are predetermined and integrated into a single piece. The flexibility provided by the modularity makes it easier to modify certain steps in the pipeline and to maintain and extend the code base.

<!-- **What is pipeline, its underlying software design philosophy, and how these are reflected in R** -->

Examples of using a pipeline approach for data analysis can be traced back to the interactive graphics literature, including @buja_elements_1988; @sutherland_orca_2000; @wickham_plumbing_2009; @xie_reactive_2014. @wickham_plumbing_2009 argue that whether made explicit or not, a pipeline has to be presented in every graphics program, and making them explicit is beneficial for understanding the implementation and comparing between different graphic systems. While this comment is made in the context of interactive graphics programs, it is also applicable generally to any data analysis workflow. More recently, the tidyverse suite [@wickham_welcome_2019] takes the pipeline approach for general-purpose data wrangling and has gained popularity within the R community. The pipeline-style code can be directly read as a series of operations applied successively on tidy data objects, offering a method to document the data wrangling process with all the computational details for reproducibility.

Since the success of tidyverse, more packages have been developed to analyze data using the tidy framework for domains specific applications, a noticeable example of which is `tidymodels` for building machine learning models [@tidymodels]. To create a tidy workflow tailored to a specific domain, developers first need to identify the fundamental building blocks to create a workflow. These components are then implemented as modules, which can be combined to form the pipeline. For example, in supervised machine learning models, steps such as data splitting, model training, and model evaluation are commonly used in most workflow. In the `tidymodels`, these steps are correspondingly implemented as package `rsample`, `parsnip`, and `yardstick`, agnostic to the specific model chosen. The uniform interface in tidymodels frees analysts from recalling model-specific syntax for performing the same operation across different models, increasing the efficiency to work with different models simultaneously.

<!-- **Constructing indexes would also benefit from pipeline and embracing the aforementioned design philosophy.** -->

For constructing indexes, the pipeline approach adopts explicit and standalone modules that can be assembled in different ways. Index developers can choose the appropriate modules and arrange them accordingly to generate the data pipeline that is needed for their purpose. The pipeline approach provides many advantages:

-   makes the computation more transparent, and thus more easily debugged.
-   allows for rapidly processing new data to check how different features, like outliers, might affect the index value.
-   provides the capacity to measure uncertainty by computing confidence intervals from multiple samples as generated by bootstrapping to original data.
-   enables systematic comparison of surrogate indexes designed to measure the same phenomenon.
-   it may even be possible to automate diagrammatic explanations and documentation of the index.

Adoption of this pipeline approach would provide uniformity to the field of index development, research, and application to improve comparability, reporducibility, and communication

<!-- They might even  insert new modules to experiment with possible new index definitions or to understand an index's behaviour. Also, the readability of the pipeline code can complement conventional diagrams and textual information to understand the how indexes are constructed. The uniform interface of pipeline allows analysts to assemble a diverse array of indexes with a shared set of components, reducing the learning curve for adopting new implementation for different indexes.-->

<!-- This approach allows for quick experimentation with alternatives and facilitates building new indexes. Consequently, maintenance is streamlined in the long term, as all the indexes are developed in-house using a unified syntax. -->

# Details of the index pipeline {#sec-pipeline}

In constructing various indexes, the primary aim is to transform the data, often multivariate, into a univariate index. Spatial and temporal considerations are also factored into the process when observational units and time periods are not independent. However, despite the variations in contextual information for indexes in different fields, the underlying statistical methodology remains consistent across diverse domains. Each index can be represented as a series of modular statistical operations on the data. This allows us to decompose the index construction process into a unified pipeline workflow with a standardized set of data processing steps to be applied across different indexes.

An overview of the pipeline is presented in @fig-pipeline-steps, illustrating the nine available modules designed to obtain the index from the data. These modules include operations for temporal and spatial aggregation, variable transformation and combination, distribution fitting, benchmark setting, and index communication. Analysts have the flexibility to construct indexes by connecting modules according to their preferences.

Now, we introduce the notation used for describing pipeline modules. Consider a multivariate spatio-temporal process,

```{=tex}
\begin{equation}
\mathbf{x}(s;t) = \{x_1(s;t), x_2(s;t), \cdots, x_p(s;t)\} \qquad s \in D_s \subseteq \mathbb{R}^m, t \in D_t \subseteq \mathbb{R}^n 
\end{equation}
```
where:

-   $x_j(s, t)$ represents a variable of interest for example precipitation, $j = 1, \cdots, p$ and

-   $s$ represents the geographic locations in the space $D_s \subseteq \mathbb{R}^m$. Examples of geographic locations include a collection of countries, longitude and latitude coordinates or regions of interest and,

-   $t$ denotes the temporal order in $D_t \subseteq \mathbb{R}^n$. For instance, time measurements could be recorded hourly, yearly, monthly, quarterly, or by season.

In what follows when geographic or temporal components of the $x_j(s,t)$ process are fixed we will be using suffix notation. For example, $x_{sj}(t)$ represents the data for a fixed location $s$ as a function of time $t$. While $x_{tj}(s)$ denotes the spatial varying process for a fixed $t$. An overview of the notation for pipeline input, operation, and output is present in @tbl-notation.

| Section | Module                  | Input                                                                   | Operation                           | Output                                                                        |
|-------|---------------|---------|---------------|--------------------------|
| 3.1   | Temporal processing     | $x_{sj}(t)$                                                             | $f[x_{sj}(t)]$                      | $x^{\text{Temp}}_{sj}(t^\prime) \quad t^\prime \in D_{t^\prime}$ |
| 3.2   | Spatial processing      | $x_{tj}(s)$                                                             | $g[x_{tj}(s)]$                      | $x^{\text{Spat}}_{tj}(s^\prime) \quad s^\prime \in D_{s^\prime}$                 |
| 3.3   | Variable transformation | $x_{j}(s; t)$                                                           | $T[x_j(s;t)]$                       | $x^{\text{Trans}}_j(s;t)$                                                     |
| 3.4   | Scaling                 | $x_j(s; t)$                                                             | $[x_j(s;t) - \alpha]/\gamma$        | $x^{\text{Scale}}_j(s;t)$                                                     |
| 3.5   | Dimension reduction     | $\mathbf{x}(s;t)$ | $h[\mathbf{x}(s;t)]$                | $\mathbf{y}(s;t) \quad \mathbf{y} \subseteq \mathbb{R}^d, d < p$      |
| 3.6   | Distribution fit        | $x_j(s; t)$                                                             | $F[x_j(s;t)]$ | $P_j(s;t) \quad P(.) \in [0, 1]$                   |
| 3.7   | Normalising             | $x_j(s; t)$                                                             | $\Phi^{-1}[x_j(s; t)]$              | $z_j(s; t)$                              |
| 3.8   | Benchmarking            | $x_j(s; t)$                                                             | $u[x_j(s;t)]$                       | $b_j(s;t)$                                                                    |
| 3.9   | Simplification          | $x_j(s; t)$                                                             | $v[x_j(s;t)]$                       | $A_j(s;t) \in \{a_1, a_2, \cdots, a_z\}$                                      |

: Summary of the notation for input, operation, and output of each pipeline module. {#tbl-notation}

```{r fig-pipeline-steps}
#| fig-align: center
#| fig-cap: "Diagram of pipeline modules for index construction. The highlighted path illustrates one possible construction using the dimension reduction and simplification module."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-overall.png"))
```

## Temporal processing

The temporal processing module takes as input argument a single variable $x_{sj}(t)$ at location $s$ as a function of time. In this step the original time series can be transformed or summarized into a new one via time aggregation, the transformation is represented by the function $f$, $x^{\text{Temp}}_{sj}(t^\prime) = f[x_{sj}(t)]$ where $t^\prime$ refers to the new temporal resolution after aggregation. An example of temporal processing done in the computation of the Standardized Precipitation Index (SPI) [@mckee1993relationship], consists of summing the monthly precipitation series over a rolling time window of size $k$. That is also known as the time scale. For SPI, the choice of the time scale $k$ is used to control the accumulation period for the water deficit, enabling the assessment of drought severity across various types (meteorological, agricultural, and hydrological).

## Spatial processing

The spatial processing module takes a single variable with a fixed temporal dimension, $x_{tj}(s)$, as input. This step transforms the variable from the original spatial dimension $s$ into the new dimension $s^\prime \in D_{s^\prime}$ through $x^{\text{Spat}}_{tj}(s^\prime) = g[x_{tj}(s)]$. The change of spatial dimension allows for the alignment of variables collected from different measurements, such as in-situ stations and satellite imagery, or originating from different resolutions. This also includes the aggregation of variables into different levels, such as city, state, and country scales.

## Variable transformation

Variable transformation takes the input of a single variable $x_j(s;t)$ and reshapes its distribution using the function $T[.]$ to produce $x^{\text{Trans}}_{j}(s;t)$. When a variable has a skewed distribution, transformations such as log, square root, or cubic root can adjust the distribution towards normality. For example, in the Human Development Index (HDI), a logarithmic transformation is applied to the variable Gross National Income per capita (GNI), to reduce its impact on HDI, particularly for countries with high GNI values.

## Scaling

Unlike variable transformation, scaling maintains the distributional shape of the variable. It includes techniques such as centering, z-score standardization, and min-max standardization and can be expressed as $[x_{j}(s;t) - \alpha]/\gamma$. In the Human Development Index (HDI), the three dimensions (health, education, and economy) are converted into the same scale (0-1) using min-max standardization.

```{r fig-scale-var-trans-compare}
#| fig-cap: "Comparison of the module scaling (green) and variable transformation (orange). While both modules change the variable range, scaling maintains the same distributional shape, which is not the case with variable transformation. "
#| fig-height: 6
#| fig-width: 8
set.seed(123)
x <- rgamma(n = 1000, shape = 1, scale = 2)


trans_df <- tibble(origin = x, 
       `z-score` = (x - mean(x)/ sd(x)), 
       quadratic = x^2,
       log = log(x),
       `square root` = sqrt(x),
       `cubic root` = x^(1/3),
       minmax = (x - min(x))/ (max(x) - min(x)),
       boxcox = (x^0.5 - 1)/0.5,
       centering = x - mean(x)
       ) %>% 
  mutate(id = row_number(),
         ) %>% 
  pivot_longer(-id, names_to = "var", values_to = "value") %>% 
  mutate(category = ifelse(
    var %in% c("origin", "z-score", "minmax", "centering"),
    "Scaling", "Variable Transformation")
    )

latex_df <- tibble(
  var = c("origin", "z-score", "quadratic", "log", "square root",
      "cubic root","minmax", "boxcox", "centering"),
  latex = c(
           r"($x$)",
           r"($\frac{x - \bar{x}}{sd(x)}$)",
           r"($x^2$)",
           r"($log(x)$)",
           r"($\sqrt{x}$)",
           r"($x^{1/3}$)",
           r"($\frac{x - x_{min}}{x_{max} - x_{min}}$)",
           r"($\frac{x^{0.5} - 1}{0.5}$)",
           r"($x - \bar{x}$)"
         )
) %>% 
  left_join(trans_df %>% group_by(var) %>% summarise(max = max(value)))

trans_df %>% 
  ggplot() +
  geom_density(aes(x = value, color = category, y = after_stat(scaled))) + 
  geom_label(data = latex_df, 
             aes(x = 0.8 * max, y = 0.8, label = TeX(latex, output = "character")), 
             parse = TRUE) + 
  facet_wrap(vars(factor(var, levels = c(
      "origin", "centering", "minmax","z-score", "boxcox", "cubic root",
      "log", "quadratic", "square root"))), scales = "free") + 
  scale_color_brewer(palette = "Dark2", name = "module") + 
  theme_bw() + 
  theme(legend.position = "bottom")
```

## Dimension reduction


Dimension reduction takes the multivariate information $\mathbf{x}(s;t)$, where $\mathbf{x} \subseteq \mathbb{R}^p$, or a subset of variables in $\mathbf{x}(s;t)$, as the input. It summarises the high-dimensional information into a lower-dimension representation $\mathbf{y}(s;t)$, where $y \subseteq \mathbb{R}^d$ and $d < p$, as the output. The transformation can be based on domain-specific knowledge, originating from theories describing the underlying physical processes, or guided by statistical methods. For example, the Standardized Precipitation-Evapotranspiration Index (SPEI) [@SPEI] calculates the difference $D$ between precipitation ($P$) and potential evapotranspiration ($\text{PET}$), using a water balance model ($D = P - \text{PET}$). This is the only step that differs from the Standardized Precipitation Index (SPI), and can be considered to be a dimension reduction using a particular linear combination. 

Linear combinations of variables are commonly used to reduce the dimension in statistical methodology, and chosen using a method like principal component analysis (PCA) [@hotelling1933analysis] or linear discriminant analysis [@fisher1936use], preparing contrasts to test particular elements in analysis of variance (XXX-ref), or hand-crafted by a content-area expert. Linear combinations also form the basis for visualizing multivariate data, in methods such as tours [@wickham_tourr_2011].  This dimension reduction method can accommodate linear combinations as provided by any method, and hence is linear by design. The transformation module provides variable-wise non-linear transformation.

<!--While all the three methods can lead in the same result due to their shared linear approach in combining variables in an additive structure: $\sum_{j = 1}^{p}\lambda_{j}x_j(s;t)$ where $\lambda$ is the weight or principal component, they offer different interpretation of the weight. Linear combination typically use equal contribution or use expert suggested weights for simplicity. Both PCA and linear projections determine their weights through optimizing a function of the data. In PCA, this function is the variance while linear projections allow any function of the data to serve as the objective function in the optimization.-->

## Distribution fit

Distribution fit applies the Cumulative Distribution Function (CDF) $F$ of a distribution on the variable $x_j(s; t)$ to obtain the probability values $P_j(s;t) \in [0, 1]$. In SPEI, many distributions, including log-logistic, Pearson III, lognormal, and general extreme distribution, are candidates for the aggregated series. Different fitting methods and different goodness of fit tests may be used to compare the distribution choice on the index value.

## Normalising

Normalizing applies the inverse normal CDF $\Phi^{-1}$ on the input data to obtain the normal density $z_{j}(s;t)$. Normalizing can sometimes be confused with the scaling or variable transformation module, which does not involve using a normal distribution to transform the variable. It is arguably whether normalizing and distribution fit should be combined or separated into two modules. A decision has been made to separate them into two modules given the different types of output each module presents (probability values for distribution fit and normal density values for normalizing).

## Benchmarking

Benchmark sets a value $b_j(s,t)$ for comparing against the original variable $x_j(s;t)$. This benchmark can be a fixed value consistently across space and time or determined by the data through the function $u[x_j(s;t)]$. Once a benchmark is set, observations can be highlighted for adjustments in other modules or can serve as targets for monitoring and planning.

## Simplification

Simplification takes a continuous variable $x_j(s;t)$ and categorises it into a discrete set $A_j(s;t) \in \{a_1, a_2, \cdots, a_z\}$ through a piecewise constant function,

```{=tex}
\begin{equation}
v[x_i(s;t)] = 
\begin{cases}
a_0 & C_1 \leq x^i(s; t) < C_0 \\
a_1 & C_2 \leq x^i(s; t) < C_1 \\
a_2 & C_3 \leq x^i(s; t) < C_2 \\
\cdots \\
a_z & C_z \leq x^i(s; t)
\end{cases}
\end{equation}
```
This is typically used at the end of the index pipeline to simplify the index to communicate to the public the severity of the concept of interest measured by the index. An example of simplification is to map the calculated SPI to four categories: mild, moderate, severe, and extreme drought.

# Software design {#sec-software}

The R package `tidyindex` implements a proof-of-concept of the index pipeline modules described in @sec-pipeline. These modules compute an index in a sequential manner, as shown below:

```{r eval = FALSE, echo = TRUE}
DATA |> 
  module1(...) |>
  module2(...) |>
  module3(...) |>
  ...
```

Each module offers a variety of alternatives, each represented by a distinct function. For example, within the `dimension_reduction()` module, three methods are available: `aggregate_linear()`, `aggregate_geometrical()`, and `manual_input()` and they can be used as: 

```{r eval = FALSE, , echo = TRUE}
dimension_reduction(V1 = aggregate_linear(...))
dimension_reduction(V2 = aggregate_geometrical(...))
dimension_reduction(V3 = manual_input(...))
```

Each method can be independently evaluated as a recipe, for example, 

```{r eval = FALSE, , echo = TRUE}
manual_input(~x1 + x2)
``` 

takes a formula to combine the variables `x1` and `x2` and return:

````
[1] "manual_input"
attr(,"formula")
[1] "x1 + x2"
attr(,"class")
[1] "dim_red"
````

This recipe will then be evaluated in the pipeline module with data to obtain numerical results. The package also offers wrapper functions that combine multiple steps for specific indexes. For instance, the `idx_spi()` function bundles three steps (temporal aggregation, distribution fit, and normalizing) into a single command, simplifying the syntax for computation. Analysts are also encouraged to create customized indexes from existing modules.

```{r eval = FALSE, echo = TRUE}
idx_spi <- function(...){
  DATA |> 
    aggregate(...) |>
    dist_fit(...) |> 
    augment(...)
}
```

(more changes) The accompanied package, tidyindex, is not intended to offer an exhaustive implementation for all indexes across every domains. Instead, it provides a realization of the pipeline framework proposed in the paper. When adopting the pipeline approach to construct indexes, analysts may consider developing software that can be readily deployed in the cloud for production purposes. 

# Examples {#sec-examples}

This section uses the example of drought and social indexes to show the analysis made possible with the index pipeline. The drought index example computes two indexes (SPI and SPEI) with various time scales and distributions simultaneously using the pipeline framework to understand the flood and drought events in Queensland. The second example focuses on the dimension reduction step in the Global Gender Gap Index to explore how the changes in linear combination weights affect the index values and country rankings.

## Every distribution, every scale, every index all at once {#sec-example1}

The state of Queensland in Australia frequently experiences natural disaster events such as flood and drought, which can significantly impact its agricultural industry. This example uses daily data from Global Historical Climatology Network Daily (GHCND), aggregated into monthly precipitation, to compute two drought indexes -- SPI and SPEI -- at various time scales and fitted distributions, for  `r length(unique(queensland$id))` stations in the state of Queensland in Australia, spanning from January 1990 to April 2022. This example showcases the basic calculation of indexes with different parameter specifications within the pipeline framework. The dataset used in this example is available in the `tidyindex` package as `queensland` and blow prints the first few rows of the data: 

```{r}
queensland %>% head(5)
```


```{r fig-spei}
#| fig-align: center
#| fig-cap: "Index pipeline for two drought indexes: the Standardized Precipitation Index (SPI) and the Standardized Precipitation-Evapotranspiration Index (SPEI). Both indexes share similar construction steps with SPEI having two steps additional steps (variable transformation and dimension reduction) to convert temperature into evapotranspiration and combine it with the precipitation series. "
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(path = here::here("figures/pipeline-spei.png"))
```

@fig-spei illustrates the pipeline steps of the two indexes. The two indexes are similar with the distinct that SPEI involves two additional steps -- variable transformation and dimension reduction -- prior to temporal processing. As introduced in @sec-software, wrapper functions are available for both indexes as `idx_spi()` and `idx_spei()`, which allows for the specification of different time scales and distributions for fitting the aggregated series. In `tidyindex`, multiple indexes can be calculated collectively using the function `compute_indexes()`. Both SPI and SPEI are calculated across four time scales (6, 12, 24, and 36 months). The SPEI is fitted with two distributions (log-logistic and general extreme value distribution) and the gamma distribution is used for SPI: 

```{r cache=TRUE, echo = TRUE, message=FALSE, eval=FALSE}
.scale <- c(6, 12, 24, 36)
idx <- queensland %>%
  init(id = id, time = ym) %>%
  compute_indexes(
    spei = idx_spei(
      .pet_method = "thornthwaite", .tavg = tavg, .lat = lat, 
      .scale = .scale, .dist = c(gev(), loglogistic())),
    spi = idx_spi(.scale = .scale)
  )
```

```{r eval = FALSE}
save(idx, file = here::here("data/idx.rda"))
```

```{r}
load(here::here("data/idx.rda"))
```

```{r}
idx
```

The output contains the original data, index values (`.index`), parameters used (`.scale`, `.method`, and `.dist`), and all the intermediate variables (`pet`, `.agg`, and `.fitted`). This data can be visualized to investigate the spatio-temporal distribution of the drought/flood events, as well as the response of index values to different time scales and distribution parameters at specific single locations. @fig-compute-spatial and @fig-compute-temporal exemplify two possibilities. @fig-compute-spatial presents the spatial distribution of SPI during two periods: October 2010 to March 2011 for the 2010/11 Queensland flood and October 2019 to March 2020 for the 2019 Australia drought, which contributes to the notorious 2019/20 bushfire season. @fig-compute-temporal displays the sensitivity of the SPEI series at the Texas post office to different time scales and fitted distributions. Larger time scales produce a smoother index across time, however, all time scales indicate an extreme drought (corresponding to -2 in SPEI) in 2020, confirming the severity of the drought across different time horizons. Moreover, the chosen distribution has less influence on the index, with general extreme value distribution tending to produce more extreme outcomes than log-logistic distribution for the extreme events (index > 2 or <-2).

```{r fig-compute-spatial}
#| fig-cap: "Spatial distribution of Standardized Precipitation Index (SPI-12) in Queensland, Australia during two major flood and drought events: 2010/11 and 2019/20. The map shows a continuous wet period during the 2010/11 flood period and a mitigated drought situation, after its worst in 2019 December and 2020 January, likely due to the increased rainfall in February from the meteorological record."
#| fig-width: 6
#| fig-height: 3
queensland_map <- ozmaps::abs_ste %>% filter(NAME == "Queensland") %>% 
  rmapshaper::ms_simplify(keep = 0.02)
queensland_map %>%
  ggplot() +
  geom_sf(fill = "transparent", color = "grey50", linewidth = 0.4) +
  geom_point(
    data = idx %>%
      filter(.idx == "spi", .scale == 12) %>%
      filter(((year(ym) %in% c(2010, 2019) & month(ym) >= 10) )|
               ((year(ym) %in% c(2011, 2020) & month(ym) <= 3) )),
    aes(x = long, y = lat, color = .index)) +
  colorspace::scale_color_continuous_divergingx(palette = "Geyser", rev = TRUE, name = "Index") + 
  theme_void() +
  facet_wrap(vars(ym), ncol = 6)
```

```{r fig-compute-temporal}
#| fig-cap: 'Time series plot of Standardized Precipitation-Evapotranspiration Index (SPEI) at the Texas post office station (highlighted by a diamond shape in panel a). The SPEI is calculated at four time scales (6, 12, 24, and 36 months) and fitted with two distributions (Log Logistic and GEV). The dashed line at -2 represents the class "extreme drought" by the SPEI. A larger time scale gives a smoother index series, while also taking longer to recover from an extreme situation as seen in the  2019/20 drought period. The SPEI values from the two distribution fit mostly agree, while GEV can result in more extreme values, i.e. in 1998 and 2020.'
#| fig-width: 8
#| fig-height: 5
texas <- idx %>%
    filter(name == "TEXAS POST OFFICE", .idx == "spei") %>% distinct(long, lat)
p1 <- queensland_map %>%
  ggplot() +
  geom_sf(fill = "transparent") +
  geom_point(data = queensland %>%
               distinct(id, long, lat, name),
             aes(x = long, y = lat)) +
  geom_point(data = texas, aes(x = long, y = lat),
             color = "orange", shape = 18, fill = "orange", size = 4) +  
  theme_void()


p2 <- idx %>%
  filter(name == "TEXAS POST OFFICE", .idx == "spei") %>%
  mutate(.dist = ifelse(.dist == "pargev", "GEV", "Log Logistic")) %>%
  rename(scale = .scale) %>% 
  ggplot(aes(x = ym, y = .index, color = .dist, group = .dist)) +
  geom_line() +
  theme_benchmark() +
  facet_wrap(vars(scale), labeller = label_both, ncol = 1) +
  scale_x_yearmonth(breaks = "4 year", date_labels = "%Y") +
  scale_color_brewer(palette = "Dark2", name = "Distribution") + 
  xlab("Year") + 
  ylab("Index")

(p1 | p2)  + 
  patchwork::plot_annotation(tag_levels = "a") + 
  patchwork::plot_layout(guides = "collect", widths = c(1, 4))  &
  theme(legend.position = "bottom") 
  
```

## Does a puff of change in variable weights cause a tornado in ranks?

The Global Gender Gap Index (GGGI), published annually by the World Economic Forum, measures gender parity by assessing relative gaps between men and women in four key areas: Economic Participation and Opportunity, Educational Attainment, Health and Survival, and Political Empowerment [@WEF2023]. The index, defined on 14 variables measuring female-to-male ratios, first aggregates these variables into four dimensions (using the linear combination given by `V-wgt` in @tbl-gggi-weights). The weights are the inverse of the standard deviation of each variable, scaled to sum to 1, thus ensuring equal relative contribution of each variable to each of the four new variables.  These new variables are then combined through another linear combination (`D-wgt` in @tbl-gggi-weights) to form the final index value. @fig-pp-gggi illustrates that the pipeline is constructed by applying the dimension reduction module twice on the data. <!--Dimension weights are equal across the four dimensions and the last column, `weight`, multiples the variable and dimension weights to produce a single set of weights.--> The data for GGGI does not needs to be transformed or scaled so these steps are not included, but they might still need to be used for other similar indexes.

```{r fig-pp-gggi}
#| fig-align: center
#| fig-cap: "Index pipeline for the Global Gender Gap Index (GGGI). The index is constructed as applying the module dimension reduction twice on the data."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-gggi.png"))
```

```{r tbl-gggi-weights}
#| tbl-cap: "Weights for the two applications of dimension reduction to compute the Global Gender Gap Index. V-wgt is used to compute four new variables from the original 14. These are then equally combined to get the final index value."
#| tbl-cap-location: bottom
# table cap location is not working: https://github.com/quarto-dev/quarto-cli/issues/6417
library(kableExtra)
gggi_weights %>%  
   mutate(variable = str_replace_all(variable, "_", " ") %>%
            str_to_sentence()) %>% 
  select(c(1, 5, 2, 6, 7)) %>% 
  mutate(dimension = c("Economy", rep("", 4), 
                       "Education", rep("" , 3), 
                       "Health", "", 
                       "Politics", rep("", 2)),
         dim_weight = c(0.25, rep("", 4),
                        0.25, rep("", 3),
                        0.25, "",
                        0.25, rep("", 2))) %>% 
  knitr::kable(
    digits = 3, booktabs = TRUE, 
    col.names = c("Variable", "V-wgt", "Dimension", "D-wgt", "wgt"),
    linesep = c("", "", "", "", "\\hline",
                "", "", "", "\\hline",
                "", "\\hline",
                "", "", "\\hline"),
      align = c("l", "r", "l", "r", "r")
    ) %>%
  kable_styling(font_size = 11) %>%
  row_spec(0, bold = TRUE) %>% 
  column_spec(1, width = "17.5em") %>% 
  column_spec(3, width = "3.5em") %>% 
  column_spec(c(2, 4), width = "4em") %>% 
  column_spec(5, width = "3em")
```

The 2023 GGGI data is available from the Global Gender Gap Report 2023 in the country's economy profile and can be accessed in the `tidyindex` package as `gggi` with @tbl-gggi-weights as `gggi_weights`. The index can be reproduced with:

```{r eval = FALSE, echo = TRUE}
gggi %>% 
  init(id = country) %>%
  add_meta(gggi_weights, var_col = variable) %>% 
  dimension_reduction(
    index_new = aggregate_linear(
      ~labour_force_participation:years_with_female_head_of_state,
      weight = weight)) 
```

After initializing the `gggi` object and attaching the `gggi_weights` as meta-data, a single linear combination within the dimension reduction module is applied to the 14 variables (from column `labour_force_participation` to `years_with_female_head_of_state`), using the weight specified in the `wgt` column of the attached metadata. While computing the index from the original 14 variables, it remains unclear how the missing values are handled within the index, which impacts 68 out of the total 146 countries. However, after aggregating variables into the four dimensions, where no missing values exist, the index is reproducible for all the countries.

@fig-idx-tour illustrates doing sensitivity analysis for GGGI, for a subset of 16 countries. Frame 12 shows the dot plot of the original index values sorted from highest to lowest. Leading the rankings are the Nordic countries (Iceland, Norway, and Finland) and New Zealand. The index values are between 0 and 1, and indicate proportional difference between men and women, with a value of 0.8 indicating women are 80% of the way to equality of these measures. There is a gap in values between these countries and the middle group (Brazil, Panama, Poland, Bangladesh, Kazakhstan, Armenia, Slovakia), and another big drop to the next group (Pakistan, Iran, Algeria, Chad). Afghanistan lags much further behind.

To make a simple illustration of sensitivity analysis, we slightly vary the weight for politics, between 0.07 and 0.52, while maintain equal weights among other dimensions. This can be viewed as an animation to examine change in relative index values as a response to the changing weights. This visualization technique, which presents a sequence of data projections, is referred to as a "tour" and the specific kind of tour used here to move between nearby projections is known as a "radial tour" (see @buja2005computational, @wickham_tourr_2011 and @radial-tour for more details].

Frames 1 and 6 show linear combinations where politics contributes less than the original. It is interesting to note that the gap between the nordic countries and the middle countries dissipates, indicating that this component was one reason for the relatively higher GGGI values of these countries. Also interesting is the large drop in value for Bangladesh. 

Frames 18, 24, 29 show linear combinations where politics contributes more than the original. The most notable feature is that Bangladesh retains it's high index value whereas the other middle group countries decline, indicating that the politics score is a major component for Bangladesh's index value. 

<!--In @fig-idx-tour, six frames have been chosen from the local tour to explore the role of the politics dimension to the index. When the weight of politics is reduced (Frame 1 and 6 vs. Frame 12), the difference in GGGI values between the Nordic and other countries narrows, suggesting a smaller variation among countries in achieving gender parity. Conversely, when the weight of politics increases (Frame 18, 24, and 29 vs. Frame 12), nearly all the countries experience a decrease in GGGI values. A noticeable exception to this trend is Bangladesh, where its index value moves in the same direction as the politics weight, which leads to an almost similar index value to the Nordics when the politics dimension is assigned a weight of 0.52 in Frame 29.-->

Ideally, an index should be robust against minor changes in its construction components. This is not the case with GGGI, where small changes to one component lead to fairly large change in the index. The modular pipeline framework for computing the index makes it easy to conduct this type of sensitivity analysis, where one or more components are perturbed and the index re-calculated. 
One aspect of the GGGI not well-described in the Global Gender Gap Report is handling of missing values that are present in the initial variables for many countries, something that is common for this type of data. This could also be made more transparent with the dimension reduction module, by specifying an imputation method or providing warnings about missing values.

<!--For other indexes that use linear combination to reduce variable dimensions, the demonstrated local tour can be applied similarly to assess the index robustness through observing how index values respond to perturbation of the weights.-->


```{r eval = FALSE}
dt <- tidyindex::gggi %>% 
  dplyr::select(country: political_empowerment) %>%
  filter(between(index, 0.72, 0.729)  | index < 0.6 | index >= 0.85) %>%
  arrange(index) 
colnames(dt) <- c(colnames(dt)[1:4], "economy", "education", "health", "politics")

w_proj2idx <- function(w){w/sum(w)}
w_idx2proj <- function(w){w/(sqrt(sum(w^2))) %>% matrix(nrow = length(w))}
find_next <- function(matrix, angle = 0.4){
  start <- matrix(c(0.25, 0.25, 0.25, 0.25), ncol = 1)
  tourr:::step_angle(tourr:::geodesic_info(start, matrix), angle = angle)
}
b <- array(dim = c(4, 1, 3))
b[,,1] <- matrix(c(0.3, 0.3, 0.3, 0.1), ncol = 1) %>% find_next()
b[,,2] <- matrix(c(0.25, 0.25, 0.25, 0.25), ncol = 1) %>% w_idx2proj()
b[,,3] <- matrix(c(0.1, 0.1, 0.1, 0.7), ncol= 1) %>% find_next(angle = 0.6)

render(
  dt[,5:8], planned2_tour(b), dev = "png",
  display_idx(half_range = 2,cex = 4, label_cex = 3.5, col = "black", 
              panel_height_ratio = c(4,1),
              axis_bar_lwd = c(rep(3, 3), 7),
              axis_bar_col = c(rep("#000000", 3), "red"),
              axis_bar_label_cex = c(rep(2.5, 3), 3),
              axis_bar_label_col = c(rep("#000000", 3), "red"), 
              axis_label_cex = 3, frame_x_pos = 0.16,  frame_cex = 4,
              axis_var_cex = 3.6, 
              label = dt$country, abb_vars = FALSE),
  width = 900, height = 1400, apf = 1/20, frames = 120,
  here::here("figures/idx-tour/idx-tour-%03d.png")
)

gifski::gifski(
  list.files(here::here("figures/idx-tour"), full.names = TRUE),
  gif_file = here::here("figures/idx-tour.gif"),
  delay = 0.15, width = 900, height = 1400)

```

```{r fig-idx-tour, fig.align='center', fig.height=5} 
#| fig-cap: "Six frames selected to explore how varying the weights of the politics dimension changes the index values and country rankings in Global Gender Gap Index (GGGI). The top panel shows the GGGI value against the country, ranked by its original index value in Frame 12. The bottom panel displays the weight used to produce the index values in the top panel, with each frame corresponds to a set of weights. Countries selected includes 1) top-ranked countries with GGGI > 0.85, 2) countries ranked between 57 and 62 with GGGI from 0.72 to 0.73, and 3) low-ranked countries with GGGI < 0.6. Compared to Frame 12 where equal weigts are used for the four dimension, a reduced weight in politics (Frame 1 and 6) shows narrower gaps between top and mid- or lower- ranked countries, while an increase in the politics weight (Frame 18, 24, and 29) leads to a systematic decrease of GGGI values across all the countries, except for Bangladesh. Full animation is available at https://vimeo.com/847874016."  
#frames <- c("001", "013", "037", "061", "085", "109")
frames <- c("001", "006", "012", "018", "024", "029")
ani <- paste0(here::here("figures/"), "idx-tour/", "idx-tour-", frames, ".png")
rl <- lapply(ani, png::readPNG)
gl <-  lapply(rl, grid::rasterGrob)
wrap_plots(gl, ncol = 3)  
```

## Decoding uncertainty through the wisdom of the crowd 

Errors in measurement, variability and sampling error, may arise at various stages of the pipeline calculation, including from different parameterisation choices, as illustrated from @sec-example1, or from the statistical summarization procedures applied in the pipeline. Although it may not be possible to perfectly measure these errors, it is important that they are recognised and estimated for an index, especially to provide potential confidence intervals. In this example, the Texas post office station highlighted in @fig-compute-temporal is used to illustrate one possibility to compute a confidence interval for the Standardized Precipitation Index (SPI). Bootstrapping is used to account for the sampling uncertainty in the distribution fit step of the index pipeline and to assess its impact on the SPI series. 

In SPI, the distribution fit step fits the gamma distribution to the aggregated precipitation series separately for each month. This results in  31 or 32 points, from January 1990 to April 2022, for estimating each set of distribution parameters. To account for this sampling uncertainty with these samples, bootstrapping is used to generate replicates of the aggregated series. In the `tidyindex` package, this bootstrap sampling is activated when the argument `.n_boot` is set to a value other than the default of 1. In the following code, the Standardized Precipitation Index (SPI) is calculated using a time scale of 24. The bootstrap procedure samples the aggregated precipitation (`.agg`) for 100 iterations (`.n_boot = 100`) and then fits the gamma distribution. The resulting gamma probabilities are then transformed into normal densities in the normalizing step with `augment()`.

```{r echo = TRUE, eval = FALSE}
DATA %>% 
  aggregate(.var = prcp, .scale = 24) %>% 
  dist_fit(.dist = gamma(), .var = .agg, .n_boot = 100) %>% 
  augment(.var = .agg)
```

The confidence interval can then be calculated using the quantile method from the bootstrap samples. @fig-conf-interval presents the 80% and 95% confidence interval for the Texas post office station, in Queensland, Australia. From the start of 2019 to 2020,  the majority of the confidence intervals lie below the extreme drought line (SPI = -2), suggesting a high level of certainty that the Texas post office is suffering from a drastic drought. The relatively wide confidence interval, as well as during the excessive precipitation events in 1996-1998 and 1999-2000, suggests a high variation of the gamma parameters estimated from the bootstrap samples and its difficulty to accurately quantify the drought and flood severity in extreme events.

```{r fig-conf-interval}
#| fig-width: 8
#| fig-height: 2
#| fig-cap: "80% and 95% confidence interval of the Standardized Precipitation Index (SPI-24) for the Texas post office station, in Queensland, Australia. A bootstrap sample of 100 is taken from the aggregated pricipitation series to estimate gamma parameters and to calculate the index. The dashed line at SPI = -2 represents an extreme drought as defined by the SPI. Most parts of the confidence intervals from 2019 to 2020 sit below the extreme drought line and are relatively wide compared to other time periods. This suggests that while it is certain that the Texas post office is suffering from a drastic drought, there is considerable uncertainty in quantifying its severity, given the extremity of the event."
res <- queensland %>% 
  filter(name == "TEXAS POST OFFICE") %>% 
  init(id = id, time = ym) %>% 
  aggregate(.var = prcp, .scale = 24) %>% 
  dist_fit(.dist = gamma(), .var = .agg, .n_boot = 100) %>% 
  augment(.var = .agg)

res_band <- res$data %>% 
  group_by(ym, id, name) %>% 
  summarise(
    q025 = quantile(.index, 0.025),
    q10 = quantile(.index, 0.1),
    q90 = quantile(.index, 0.9),
    q975 = quantile(.index, 0.975)
    ) %>% 
  ungroup()

all <- idx %>% 
  filter(name == "TEXAS POST OFFICE", .idx == "spi", .scale == 24) %>% 
  select(ym, .index) %>% 
  left_join(res_band, by = "ym")

loc <- make_yearmonth(2022, 12)

# the sampling uncertainty is quite large here
# and this is mainly due to we only have 30 year worth of data
# each month fit has only 30 points
all %>% 
  ggplot(aes(x = ym)) + 
  geom_hline(yintercept = -2, linetype = "dashed", color = "grey50") + 
  #geom_text(aes(x = loc, y = -2.7, label = "SPI = -2"), color = "grey50") + 
  geom_ribbon(aes(ymin = q025, ymax = q975), fill = "steelblue1", alpha = 0.5) + 
  geom_ribbon(aes(ymin = q10, ymax = q90), fill = "steelblue4", alpha = 0.5) + 
  geom_line(aes(y = .index)) + 
  scale_x_yearmonth(name = "Year", date_break = "2 years", date_label = "%Y") +
  theme_bw() + 
  facet_wrap(vars(name), ncol = 1) + 
  theme(panel.grid = element_blank(), 
        legend.position = "bottom") + 
  ylab("SPI")

```



# Conclusion

The paper introduces a data pipeline comprising nine modules designed for the construction and analysis of indexes within the tidy framework. The pipeline offers a modular workflow to allow compute index with different parameterizations, to test minor changes to the original index definition, and to quantify uncertainties. The framework proposed in the paper is universal to index across diverse domains. Examples are provided, including the drought indexes (SPI and SPEI) and Global Gender Gap Index (GGGI), to demonstrate the index calculation with different time scales and distributions, to illustrate how slight adjustment of linear combination weights impact the index, and to calculate confidence intervals on the index.  

<!-- When adopting the pipeline approach to construct indexes, analysts may consider developing software that can be readily deployed in the cloud for production purposes. -->

# Acknowledgement

This work is funded by a Commonwealth Scientific and Industrial Research Organisation (CSIRO) Data61 Scholarship. The article is created using Quarto [@Allaire_Quarto_2022] in R [@R]. The source code for reproducing this paper can be found at: https://github.com/huizezhang-sherry/paper-tidyindex.

# Reference
