---
title: "A Tidy Framework and Infrastructure to Systematically Assemble Spatio-temporal Indexes from Multivariate Data"
format:
  tandf-pdf:
    keep-tex: true  
pdf-engine: pdflatex
author:
  - name: H. Sherry Zhang
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0002-7122-1463
    email: huize.zhang@monash.edu
  - name: Dianne Cook
    affiliation: 
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0002-3813-7155
  - name: Ursula Laa
    affiliation: 
      - name: University of Natural Resources and Life Sciences
        department: Institute of Statistics
        address: Gregor-Mendel-Straße 33, 1180 Wien, Austria
        city: Vienna
        country: Austria
    orcid: 0000-0002-0249-6439
  - name: Nicolas Langrené
    affiliation: 
      - name: BNU-HKBU United International College
        department: Department of Mathematical Sciences
        address: 2000 Jintong Road, Tangjiawan, Zhuhai
        city: Zhuhai, Guangdong
        country: China
    orcid: 0000-0001-7601-4618
  - name: Patricia Menéndez
    affiliation:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0003-0701-6315
abstract: |
  Indexes are useful for summarizing multivariate information into single metrics for monitoring, communicating, and decision-making. While many work has focused on defining new indexes for the application of interest, less attention has been directed towards understanding how the a given index behave under different scenarios. This research introduces a data pipeline comparising 9 modules to assemble indexes from a series of modular building blocks. This modular data pipeline is universally applicable to index computation and allows analysts to incorporate the analysis on index into the construction pipeline. Analysts can compute with different index parameters, adjust steps in the index definition by adding, removing, and swapping them to experiment with various index designs, calculate uncertainty measures, and assess index robustness . The paper presents two examples to illustrate the usage of the pipeline. The first example claculates two drought indexes with different parameters to investigate the spatio-temporal distribution of drought in Queensland, Australia. The second example on the Global Gender Gap Index (GGGI) explores the effects of adjusting linear combination weights on a country's index values and rankings.
keywords:
  - indexes
  - data pipeline
  - software design
bibliography: bibliography.bib  
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
#options(width = 70)
library(tidyverse)
library(latex2exp)
library(patchwork)
library(tidyindex)
library(lmomco)
library(lubridate)
library(SPEI)
library(tsibble)
library(GGally)
library(patchwork)
library(tourr)
```

# Introduction

<!-- - Why index is useful, why people care about indexes -->

<!-- - Define what is an index, what is not -->

<!-- - What is the challenges with current index construction -->

<!-- - what can be done if people adopt this pipeline/ why it is beneficial? -->

<!-- - who would benefit from this paper -->

Indexes are commonly used to combine and summarize different sources of information into a single number for monitoring, communicating, and decision-making. Examples of those include the Air Quality Index, El Niño-Southern Oscillation Index, Consumer Price Index, QS University Rankings or Human Development Index among many others.

To construct an index, experts typically start by defining a concept of interest that requires measurement. This concept often lacks a direct measurable attribute or can only be measure as a composite of various processes, yet it holds social and public significance. To create an index, once the underlying processes involved are identified, relevant and available variables are then defined, collected, and combined using statistical methods into an index that aims at measure the process of interest. The construction process is often not straightforward, and decisions need to be made, such as the selection of variables to be included, which might depend on data availability and the statistical definition of the index to be used, among others. For instance, the indexes constructed from linear combination of variables need to decide on the weight assigned to each variable. Some indexes have a spatial and/or temporal components, and variables can be aggregated to different spatial resolutions and temporal scales, leading to various indexes for different monitoring purposes. Hence, all these decisions can result in different index values and have different practical implications.

To be able to test different decision choices systematically for an index, the index needs to be broken down into its fundamental building blocks to analyse the contribution and effect of each component. We call this process of breaking the index construction into different steps the index pipeline. Such decomposition of index components provides the means to standardise index construction via a pipeline and offers benefits for comparing among indexes and calculating index uncertainty. In social indexes for instance, the OECD handbook [@oecd_handbook_2008] has provided a set of steps and recommendations for constructing composite socio-economic indexes. However, there is still a need to extend these guidelines and methods to accommodate the inclusion of more methodological complex steps required for indexes in general.

In this work, we aim at providing statistical and computational methods to develop a data pipeline framework for constructing and customize indexes using data. As a companion the R package, tidyindex, is developed to construct and explore different. Furthermore, the tidyindex package can be used to explore the effects of changes on index definition, index construction steps, and construction methods. This work provides researchers actively developing new indexes or aiming at improving indexes with the tools to easily modify and evaluate indexes. It also helps index analysts diagnose indexes, carry out sensitivity analysis to assess small perturbations in the index, and identify potential weaknesses for methodology improvement.

The rest of the paper is structured as follows: @sec-pipeline reviews the concept of data pipeline in R. The pipeline framework for index construction is presented in @sec-index-pipeline. @sec-dev explains how to add new steps into the index methodology in the form of new building blocks inside the index construction pipeline. Examples are given in @sec-examples to demonstrate the index construction with the pipeline built.

<!-- Evaluating indexes is challenging because the underlying concept an index measures is often a latent variable and lacks direct benchmarks for comparison. In literature, researchers typically demonstrate the accuracy of an index through comparing it to some well-known indexes, such as SPI for drought indexes, or to a relevant response variable, such as crop yield for monitoring agricultural drought. For social indexes, @uncertainty proposes a Monte Carlo algorithm to compute index uncertainty through sampling the linear weights.  -->

<!-- These sometimes make indexes difficult to reach consensus for wider use and large institutes often prefer simple indexes for more stable performance and easier interpretation. -->

<!-- When indexes have a spatial and/or temporal component, different choices on how space and time is aggregated will introduce uncertainty. A slight change in the variable value can sometimes introduce a big difference for the index.  -->

# Tidy framework {#sec-pipeline}

<!-- **Why you should care about pipeline** -->

The tidy framework consists of two key components: tidy data and tidy tools. The concept of tidy data [@wickham_tidy_2014] prescribes specific rules for organising data in an analysis, with observations as rows, variables as columns, and types of observational units as tables. Tidy tools, on the other hand, are concatenated in a sequence through which the tidy data flows, creating a pipeline for data processing and modelling. These pipelines are data centric, meaning all the tidy tools or functions take a tidy data object as inputs and return a processed tidy data object, directly ready for the next operations to be applied. Also, the pipeline approach corresponds to the modular programming practice, which breaks down complex problems into smaller and more manageable pieces, as oppose to a monolithic design, where all the steps are predetermined and integrated into a single piece. The flexibility provided by the modularity makes it easier to modify certain steps in the pipeline and to maintain and extend the code base.

<!-- **What is pipeline, its underlying software design philosophy, and how these are reflected in R** -->

Examples of using a pipeline approach for data analysis can be traced back to the interactive graphics literature, including @buja_elements_1988; @sutherland_orca_2000; @xie_reactive_2014; @wickham_plumbing_2009. @wickham_plumbing_2009 argues that whether made explicit or not, a pipeline has to be presented in every graphics program and making them explicit is beneficial for understanding the implementation and comparing between different graphic systems. While this comment is made in the context of interactive graphics program, it is also applicable generally to any data analysis workflow. More recently, the tidyverse suite [@wickham_welcome_2019] takes the pipeline approach for general-purpose data wrangling and has gained popularity within the R community. The pipeline-style code can be directly read as a series of operations applied successively on tidy data object, offering a method to document the data wrangling process with all the computational details for reproducibility.

Since the success of tidyverse, more packages have been developed to analyze data using the tidy framework for domains specific applications, a noticeable example of which is `tidymodels` for building machine learning models [@tidymodels]. To create a tidy workflow tailored to a specific domain, developers first need to identify the fundamental building blocks to create a workflow. These components are then implemented as modules, which can be combined to form the pipeline. For example, in supervised machine learning models, steps such as data splitting, model training and model evaluation are commonly used in most workflow. In the `tidymodels`, these steps are correspondingly implemented as package `rsample`, `parsnip`, and `yardstick`, agnostic to the specific model chosen. The uniform interface in tidymodels frees analysts from recalling model-specific syntax for performing the same operation across different models, increasing the efficiency to work with different models simultaneously.

<!-- **Constructing indexes would also benefit from pipeline and embracing the aforementioned design philosophy.** -->

<!-- For constructing indexes, many indexes have provided available publicly scripts for computation, i.e. Human Development Index (HDI) [@hdi]. Some have made the first level of generalisation from scripts to functions and creates R packages specialised for calculating particular indexes, i.e. the `SPEI` package [@SPEI] for calculating drought indexes SPI and SPEI.  -->

For constructing indexes, the pipeline approach adopts explicit and standalone modules that can be assembled in different ways. Index developers can choose the appropriate modules and arrange them accordingly to generate the data pipeline that is needed for their purpose. The pipeline approach provides many advantages:

-   makes the computation more transparent, and thus more easily debugged.
-   allows for rapidly processing new data to check how different features, like outliers, might affect the index value.
-   provides the capacity to measure uncertainty by computing confidence intervals from multiple samples as generated by bootstrapping to original data.
-   enables systematic comparison of surrogate indexes designed to measure the same phenomenon.
-   it may even be possible to automate diagramatic explanations and documentation of the index.

Adoption of this pipeline approach would provide uniformity to the field of index development, research and application.

<!-- They might even  insert new modules to experiment with possible new index definitions or to understand an index's behaviour. Also, the readability of the pipeline code can complement conventional diagrams and textual information to understand the how indexes are constructed. The uniform interface of pipeline allows analysts to assemble a diverse array of indexes with a shared set of components, reducing the learning curve for adopting new implementation for different indexes.-->

<!-- This approach allows for quick experimentation with alternatives and facilitates building new indexes. Consequently, maintenance is streamlined in the long term, as all the indexes are developed in-house using a unified syntax. -->

# Details of the index pipeline {#sec-pipeline}

In constructing various indexes, the primary aim is to transform the data, often multivariate, into a univariate index. Spatial and temporal considerations are also factored into the process when observational units and time periods are not independent. However, despite the variations in contextual information for indexes in different fields, the underlying statistical methodology remains consistent across diverse domains. Each index can be represented as a series of modular statistical operations on the data. This allows us to decompose the index construction process into a unified pipeline workflow with a standardized set of data processing steps to be applied across different indexes.

An overview of the pipeline is presented in @fig-pipeline-steps, illustrating the nine available modules designed to obtain the index from the data. These modules include operations for temporal and spatial aggregation, variable transformation and combination, distribution fitting, benchmark setting, and index communication. Analysts have the flexibility to construct indexes by connecting modules according to their preferences.

Now, we introduce the notation used for describing pipeline modules. Consider multivariate spatio-temporal process,

```{=tex}
\begin{equation}
\mathbf{x}(s;t) = \{x_1(s;t), x_2(s;t), \cdots, x_p(s;t)\} \qquad s \in D_s \subseteq \mathbb{R}^m, t \in D_t \subseteq \mathbb{R}^n 
\end{equation}
```
where:

-   $x_j(s, t)$ represents a variable of interest for example precipitation, $j = 1, \cdots, p$ and

-   $s$ represents the geographic locations in the space $D_s \subseteq \mathbb{R}^m$. Examples of geographic locations include a collection of countries, longitude and latitude coordinates or regions of interest and,

-   $t$ denotes the temporal order in $D_t \subseteq \mathbb{R}^n$. For instance, time measurements could be recorded hourly, yearly, monthly, quaterly or by season.

In what follows when geographic or temporal components of the $x_j(s,t)$ process are fixed we will be using suffix notation. For example, $x_{sj}(t)$ represents the data for a fixed location $s$ as a function of time $t$. While $x_{tj}(s)$ denotes the spatial varying process for a fixed $t$. An overview of the notation for pipeline input, operation, and output is present in @tbl-notation.

| No. | Module                  | Input                                                                   | Operation                           | Output                                                                        |
|--|---------------|----------|---------------|--------------------------|
| 1   | Temporal processing     | $x_{sj}(t)$                                                             | $f[x_{sj}(t)]$                      | $x^{\text{Temp}}_{sj}(t^\prime) \quad t^\prime \in D_{t^\prime}$ |
| 2   | Spatial processing      | $x_{tj}(s)$                                                             | $g[x_{tj}(s)]$                      | $x^{\text{Spat}}_{tj}(s^\prime) \quad s^\prime \in D_{s^\prime}$                 |
| 3   | Variable transformation | $x_{j}(s; t)$                                                           | $T[x_j(s;t)]$                       | $x^{\text{Trans}}_j(s;t)$                                                     |
| 4   | Scaling                 | $x_j(s; t)$                                                             | $[x_j(s;t) - \alpha]/\gamma$        | $x^{\text{Scale}}_j(s;t)$                                                     |
| 5   | Dimension reduction     | $\mathbf{x}(s;t) \quad \mathbf{x} \subseteq \mathbb{R}^q,\; q\leq p$ | $h[\mathbf{x}(s;t)]$                | $\mathbf{y}(s;t) \quad \mathbf{y} \subseteq \mathbb{R}^d, d < q$      |
| 6   | Distribution fit        | $x_j(s; t)$                                                             | $F[x_j(s;t)]$ | $p_j(s;t) \quad p(.) \in [0, 1]$                   |
| 7   | Normalising             | $x_j(s; t)$                                                             | $\Phi^{-1}[x_j(s; t)]$              | $z_j(s; t)$                              |
| 8   | Benchmarking            | $x_j(s; t)$                                                             | $u[x_j(s;t)]$                       | $b_j(s;t)$                                                                    |
| 9   | Simplification          | $x_j(s; t)$                                                             | $v[x_j(s;t)]$                       | $A_j(s;t) \in \{a_1, a_2, \cdots, a_j\}$                                      |

: An notation overview of the input, operation, and output of each pipeline module. {#tbl-notation}

```{r fig-pipeline-steps}
#| fig-align: center
#| fig-cap: "Diagram of pipeline modules for index construction. The highlighted path illustrates one possible construction using the dimension reduction and simplification module."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-overall.png"))
```

## Temporal processing

The temporal processing module takes as input argument a single variable $x_{sj}(t)$ at location $s$ as a function of time. In this step the original time series can be transformed or summarized into a new one via time aggregation, the transformation is represented by the function $f$, $x^{\text{Temp}}_{sj}(t^\prime) = f[x_{sj}(t)]$ where $t^\prime$ refers to the new temporal resolution after aggregation. An example of a temporal processing done in the computation of the Standardized Precipitation Index (SPI), consists of summing the monthly precipitation series over a rolling time window of size $k$. That is also known as the time scale. For SPI, the choice of the time scale $k$ is used to control the accumulation period for the water deficit, enabling the assessment of drought severity across various types (meteorological, agricultural, and hydrological).

## Spatial processing

The spatial processing module takes a single variable with a fixed temporal dimension, $x_{tj}(s)$, as input. This step transforms the variable from the original spatial dimension $s$ into the new dimension $s^\prime \in D_{s^\prime}$ through $x^{\text{Spat}}_{tj}(s^\prime) = g[x_{tj}(s)]$. The change of spatial dimension allows for the alignment of variables collected from different measurements, such as in-situ stations and satellite imagery, or originating from different resolutions. This also includes the aggregation of variables into different levels, such as city, state, and countries scales.

## Variable transformation

Variable transformation takes input of a single variable $x_j(s;t)$ and reshapes its distribution using the function $T[.]$ to produce $x^{\text{Trans}}_{j}(s;t)$. When a variable has a skewed distribution, transformations such as log, square root, or cubic root can adjust the distribution towards normality. For example, in Human Development Index (HDI), a logarithmic transformation is applied to the variable Gross National Income per capita (GNI), to reduce its impact on HDI, particularly for countries with high GNI values.

## Scaling

Unlike variable transformation, scaling maintains the distributional shape of the variable. It includes techniques such as centering, z-score standardisation, and min-max standardisation and can be expressed as $[x_{j}(s;t) - \alpha]/\gamma$. In Human Development Index (HDI), the three dimensions (health, education, and economy) are converted into the same scale (0-1) using min-max standardisation.

```{r fig-scale-var-trans-compare}
#| fig-cap: "Comparison of the module scaling (green) and variable transformation (orange). While both modules change the variable range, scaling maintains the same distributional shape, which is not the case with variable transformation. "
#| fig-height: 8
#| fig-width: 12
set.seed(123)
x <- rgamma(n = 1000, shape = 1, scale = 2)


trans_df <- tibble(origin = x, 
       `z-score` = (x - mean(x)/ sd(x)), 
       qudratic = x^2,
       log = log(x),
       `square root` = sqrt(x),
       `cubic root` = x^(1/3),
       minmax = (x - min(x))/ (max(x) - min(x)),
       boxcox = (x^0.5 - 1)/0.5,
       centering = x - mean(x)
       ) %>% 
  mutate(id = row_number(),
         ) %>% 
  pivot_longer(-id, names_to = "var", values_to = "value") %>% 
  mutate(category = ifelse(
    var %in% c("origin", "z-score", "minmax", "centering"),
    "Scaling", "Var. Trans.")
    ) 

latex_df <- tibble(
  var = c("origin", "z-score", "qudratic", "log", "square root", "cubic root",
          "minmax", "boxcox", "centering"),
  latex = c(
           r"($x$)",
           r"($\frac{x - \bar{x}}{sd(x)}$)",
           r"($x^2$)",
           r"($log(x)$)",
           r"($\sqrt{x}$)",
           r"($x^{1/3}$)",
           r"($\frac{x - x_{min}}{x_{max} - x_{min}}$)",
           r"($\frac{x^{0.5} - 1}{0.5}$)",
           r"($x - \bar{x}$)"
         )
) %>% 
  left_join(trans_df %>% group_by(var) %>% summarise(max = max(value)))

trans_df %>% 
  ggplot() +
  geom_density(aes(x = value, color = category, y = after_stat(scaled))) + 
  geom_label(data = latex_df, 
             aes(x = 0.8 * max, y = 0.8, label = TeX(latex, output = "character")), 
             parse = TRUE) + 
  facet_wrap(vars(var), scales = "free") + 
  scale_color_brewer(palette = "Dark2", name = "module") + 
  theme_bw() + 
  theme(legend.position = "bottom")
```

## Dimension reduction

Dimension reduction takes the multivariate information $\mathbf{x}(s;t)$, where $\mathbf{x} \subseteq \mathbb{R}^q,\; q\leq p$, as the input. It summarises the high-dimensional information into a lower dimension representation $\mathbf{y}(s;t)$, where $y \subseteq \mathbb{R}^d$ and $d < q$, as the output. The transformation can be based on domain-specific knowledge, originating from theories describing the underlying physical processes, or guided in statistical methods. For example, the Standardized Precipitation-Evapotranspiration Index (SPEI) calculates the difference $D$ between precipitation ($P$) and potential evapotranspiration ($\text{PET}$), using a water balance model ($D = P - \text{PET}$). This is the only step that differs from the Standardized Precipitation Index (SPI).

## Distribution fit

Distribution fit applies the Cumulative Distribution Function (CDF) $F$ of a distribution on the variable $x_j(s; t)$ to obtain the probability values $p_j(s;t) \in [0, 1]$. In SPEI, many distributions, including log-logistic, pearson III, lognormal, and general extreme distribution, are candidates for the aggregated series. Different fitting methods, and different goodness of fit tests may be used to compare the distribution choice on the index value.

## Normalising

Normalising applies the inverse normal CDF $\Phi^{-1}$ on the input data to obtain the normal density $z_{j}(s;t)$. Normalising can sometimes be confused with the scaling or variable transformation module, which do not involve using a normal distribution to transform the variable. It is arguably whether normalising and distribution fit should be combined or separated into two modules. A decision has been made to separate them into two modules given the different types of output each module present (probability values for distribution fit and normal density values for normalising).

## Benchmarking

Benchmark sets a value $b_j(s,t)$ for comparing against the original variable $x_j(s;t)$. This benchmark can be a fixed value consistently across space and time or determined by the data through the function $u[x_j(s;t)]$. Once a benchmark being set, observations can be highlighted for adjustments in other modules or can serve targets for monitoring and planning.

## Simplification

Simplification takes a continuous variable $x_j(s;t)$ and categorises it into a discrete set $A_j(s;t) \in \{a_1, a_2, \cdots, a_j\}$ through a piecewise function,

```{=tex}
\begin{equation}
v[x_i(s;t)] = 
\begin{cases}
a_0 & C_1 \leq x^i(s; t) < C_0 \\
a_1 & C_2 \leq x^i(s; t) < C_1 \\
a_2 & C_3 \leq x^i(s; t) < C_2 \\
\cdots \\
a_z & C_z \leq x^i(s; t)
\end{cases}
\end{equation}
```
This is typically used at the end of the index pipeline to simplify the index to communicate to the public the severity of the concept of interest measured by the index. An example of simplification is to map the calculated SPI to four categories: mild, moderate, severe, and extreme drought.

# Software design {#sec-dev}

The R package `tidyindex` implements the index pipeline modules described in @sec-pipeline. These modules compute an index in a sequential manner, as shown below:

```{r eval = FALSE, echo = TRUE}
DATA |> 
  module1(...) |>
  module2(...) |>
  module3(...) |>
  ...
```

Each module offers a variety of alternatives, each represented by a distinct function. For example, within the `dimension_reduction()` module, three methods are available: `aggregate_linear()`, `aggregate_geometrical()`, and `manual_input()` and they can be used as: 

```{r eval = FALSE, , echo = TRUE}
dimension_reduction(V1 = aggregate_linear(...))
dimension_reduction(V2 = aggregate_geometrical(...))
dimension_reduction(V3 = manual_input(...))
```

Each method can be independently evaluated as a recipe, for example, 

```{r eval = FALSE, , echo = TRUE}
manual_input(~x1 + x2)
``` 

takes a formula to combine the variable `x1` and `x2` and will return:

````
[1] "manual_input"
attr(,"formula")
[1] "x1 + x2"
attr(,"class")
[1] "dim_red"
````

This recipe will then be evaluated in the pipeline module with data to obtain the numerical results. The package also offers wrapper functions that combine multiple steps for specific indexes. For instance, the `idx_spi()` function bundles three steps (temporal aggregation, distribution fit, and normalising) into a single command, simplifying the syntax for computation. Analysts are also encouraged to create their customised indexes from existing modules.

```{r eval = FALSE, echo = TRUE}
idx_spi <- function(...){
  DATA |> 
    aggregate(...) |>
    dist_fit(...) |> 
    augment(...)
}
```


# Examples {#sec-examples}

This section uses the example of drought and social indexes to show the analysis made possible with the index pipeline. The drought index example computes two indexes (SPI and SPEI) with various time scales and distributions simultaneously using the pipeline framework to understand the flood and drought events in Queensland. The second example focuses on the dimension reduction step in the Global Gender Gap Index to explore how the changes of linear combination weights affect the index values and country rankings.

## Every distribution, every scale, every index all at once

A common task for drought researchers is to compute indexes at different parameter combinations. This can be used to identify the spatial and temporal extent of drought events, recommend the best parameter choice, or compare the effectiveness of indexes for monitoring drought. The example below computes two indexes: SPI and SPEI, at various time scales and fitted distributions, for stations in the state of Queensland in Australia. The purpose of the example is to demonstrate the interfaces the tidyindex package built to allow easy computing at different parameter combinations.

The state of Queensland in Australia is frequently affected by natural disaster events such as flood and drought, which can have significant impacts on its agricultural industry. This study uses daily data from Global Historical Climatology Network Daily (GHCND), accessed via the package `rnoaa` to examine drought/flood condition in Queensland. Daily data is average into monthly and stations are excluded if monthly data contains missings, which is required for calculating both SPI and SPEI. This gives `r length(unique(queensland$id))` stations with complete records from 1990 January to 2022 April.

```{r fig-pipeline-spei}
#| fig-align: center
#| fig-cap: "Diagram of pipeline steps for index construction. will need to be updated with better design and the distribution fitting step."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-spei.png"))
```

The function `compute_indexes()` can be used to collectively compute multiple indexes. The tidyindex offers wrapper functions, with the prefix idx\_, that simplify the calculation of commonly used indexes by combining a set of pipeline steps into a single function. For example, the function `idx_spei()` includes the five steps previously described in @sec-toy-example (variable transformation, dimension reduction, temporal aggregation, distribution fit, and normalise). Each `idx_xxx()` function specifies the relevant parameters relevant to the index: the thornthwaite method is used to calculate PET in SPEI, with the average temperature (`tavg`) and latitude (`lat`) used as inputs. The SPEI is computed at four time scales (6, 12, 24, and 36 months) and fitted with two distributions (Log-logistic and General Extreme Value (GEV)). The SPI is also computed at the same four time scales and uses the default gamma distribution to fit the aggregated series.

```{r cache=TRUE, echo = TRUE, message=FALSE, eval=FALSE}
.scale <- c(6, 12, 24, 36)
idx <- queensland %>%
  init(id = id, time = ym) %>%
  compute_indexes(
    spei = idx_spei(
      .pet_method = "thornthwaite", .tavg = tavg, .lat = lat, 
      .scale = .scale, .dist = c(gev(), loglogistic())),
    spi = idx_spi(.scale = .scale)
  )
```

```{r eval = FALSE}
save(idx, file = here::here("data/idx.rda"))
```

The output from `compute_indexes()` contains index values and associated parameter in a long tibble. It includes the original variables (`id`, `ym`, `prcp`, `tmax`, `tmin`, `tavg`, `long`, `lat`, and `name`), index parameters (`.idx`, `.scale`, `.method`, and `.dist`), intermediate variables (`pet`, `.agg`, and `.fitted`), and the final index (`.index`). This data can be visualised across space or time, or simultaneously, to explore the wet/dry condition in Queensland. @fig-compute-spatial visualises the spatial distribution of SPI at two periods (2010 October - 2011 March and 2019 October - 2020 March) with significant natural disaster events: 2010/11 Queensland flood and 2019 Australia drought, which contributes to the notorious 2019/20 bushfire. @fig-compute-temporal displays the sensitivity of the SPEI series for one particular station, Texas post office, at different time scales and fitted distributions. These two plots demonstrate some possibilities to explore the indexes after they are computed from `compute_indexes()`.

```{r fig-compute-spatial}
#| fig-cap: "Spatial distribution of Standardized Precipitation Index (SPI-12) in Queensland, Australia during two major flood and drought events: 2010/11 and 2019/20. The map shows a continous wet period during the 2010/11 flood period and a mitigated drought situation, after its worst in 2019 December and 2020 Janurary, likely due to the increased rainfall in February from the meteorological record."
#| fig-width: 6
load(here::here("data/idx.rda"))
queensland_map <- ozmaps::abs_ste %>% filter(NAME == "Queensland") %>% 
  rmapshaper::ms_simplify(keep = 0.02)
queensland_map %>%
  ggplot() +
  geom_sf(fill = "transparent", color = "grey50", linewidth = 0.4) +
  geom_point(
    data = idx %>%
      filter(.idx == "spi", .scale == 12) %>%
      filter(((year(ym) %in% c(2010, 2019) & month(ym) >= 10) )|
               ((year(ym) %in% c(2011, 2020) & month(ym) <= 3) )), 
    aes(x = long, y = lat, color = .index)) +
  colorspace::scale_color_continuous_divergingx(palette = "Geyser", rev = TRUE, name = "Index") + 
  theme_void() +
  facet_wrap(vars(ym), ncol = 6)
```

```{r fig-compute-temporal}
#| fig-cap: 'Time series plot of Standardized Precipitation-Evapotranspiration Index (SPEI) at the Texas post office station (highlighted by a diamond shape in panel a). The SPEI is calculated at four time scales (6, 12, 24, and 36 months) and fitted with two distributions (Log Logistic and GEV). The dashed line at -2 represents the class "extreme drought" by the SPEI. A larger time scale gives a smoother index series, while also takes longer to recover from an extreme situation as seen in the  2019/20 drought period. The SPEI values from two distribution fits mostly agree, while GEV can results in more extreme values, i.e. in 1998 and 2020.'
#| fig-width: 8
#| fig-height: 5
texas <- idx %>%
    filter(name == "TEXAS POST OFFICE", .idx == "spei") %>% distinct(long, lat)
p1 <- queensland_map %>%
  ggplot() +
  geom_sf(fill = "transparent") +
  geom_point(data = queensland %>%
               distinct(id, long, lat, name),
             aes(x = long, y = lat)) +
  geom_point(data = texas, aes(x = long, y = lat),
             color = "orange", shape = 18, fill = "orange", size = 4) +  
  theme_void()


p2 <- idx %>%
  filter(name == "TEXAS POST OFFICE", .idx == "spei") %>%
  mutate(.dist = ifelse(.dist == "pargev", "GEV", "Log Logistic")) %>%
  rename(scale = .scale) %>% 
  ggplot(aes(x = ym, y = .index, color = .dist, group = .dist)) +
  geom_line() +
  theme_benchmark() +
  facet_wrap(vars(scale), labeller = label_both, ncol = 1) +
  scale_x_yearmonth(breaks = "2 year", date_labels = "%Y") +
  scale_color_brewer(palette = "Dark2", name = "Distribution") + 
  xlab("Year") + 
  ylab("Index")

(p1 | p2)  + 
  patchwork::plot_annotation(tag_levels = "a") + 
  patchwork::plot_layout(guides = "collect", widths = c(1, 4))  &
  theme(legend.position = "bottom") 
  
```

## Does a minor change in variable weights cause a tornado?

The Global Gender Gap Index (GGGI), published annually by the World Economic Forum, measures gender parity by assessing relative gaps between men and women in four key areas: Economic Participation and Opportunity, Educational Attainment, Health and Survival, and Political Empowerment [@WEF2023]. The index, compiled from 14 variables expressed as female-to-male ratios, first aggregates these variables into four dimensions through linear combinations. These dimensions are then combined though another linear combination to form the index. @fig-pp-gggi illustrates this pipeline construction by applying the dimension reduction module twice on the data to generate the index. @tbl-gggi-weights presents the variable weights (`V-weight`) and dimension weights (`D-weight`) used in the two dimension reduction. In the table, the variable weights are computed as the inverse of the standard deviation of each variable, scaled to sum to 1. These weights ensure that a one percentage point change in the standard deviation of each variable to contribute equally to the index. Dimension weights are equal across the four dimensions and the last column, `weight`, multiples the variable and dimension weights to produce a single set of weight.

```{r fig-pp-gggi}
#| fig-align: center
#| fig-cap: "Index pipeline for the Global Gender Gap Index (GGGI). The index is constructed as applying the module dimension reduction twice on the data."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-gggi.png"))
```

```{r tbl-gggi-weights}
#| tbl-cap: Weights used to compute the Global Gender Gap Index
#| tbl-cap-location: bottom
gggi_weights %>%  
   mutate(variable = str_replace_all(variable, "_", " ") %>%
            str_to_sentence()) %>% 
  select(c(1,2, 5:7)) %>% 
  mutate(dimension = c(rep("Economy", 5), rep("Education" ,4), 
                       rep("Health", 2), rep("Politics", 3))) %>% 
  knitr::kable(
    digits = 3, 
    col.names = c("Variable", "Dimension", "V-weight",
        "D-weight", "Weight")
    ) 
```

The 2023 GGGI data is available from the Global Gender Gap Report 2023 in the country's economy profile and can be accessed in R via the `tidyindex` package as `gggi` and @tbl-gggi-weights as `gggi_weights`. The index can be reproduced with the pacakge as:

```{r eval = FALSE, echo = TRUE}
gggi %>% 
  init(id = country) %>%
  add_meta(gggi_weights, var_col = variable) %>% 
  dimension_reduction(
    index_new = aggregate_linear(
      ~labour_force_participation:years_with_female_head_of_state,
      weight = weight)) 
```

After initialising the `gggi` object and attaching the `gggi_weights` as metadata, a single linear combination step within the dimension reduction module is applied to the 14 variables (from column `labour_force_participation` to `years_with_female_head_of_state`), using the weight specified in the `weight` column of the attached metadata. While computing the index from the original 14 variables, it remains unclear how the missing values are handled within the index, which impacts 68 out of the total 146 countries. However, after aggregating variables into the four dimensions, where no missing values exist, the index is reproducible for all the countries.

A linear combination can also be interpreted as a linear projection of multivariate information with a weight vector. Projecting data from higher to lower dimension unavoidably leads to information loss and the weights used require careful examination to understand their effects on the index and implications for interpretation and decision-making. By making slight adjustments to the weight vector, we can observe how index values and country rankings change. For illustration, we select a set countries including:

1)  Top-ranked countries with GGGI \> 0.85,
2)  Countries ranked between 57 and 62, with GGGI values from 0.72 to 0.73, and
3)  Low-ranked countries with GGGI \< 0.6.

We slightly vary the weight of the politics dimension from the original 0.25 while keeping the weights constant for the other three dimensions. This process creates an animation showing the movement of index values in response to changing weights. This visualisation technique, which presents a sequence of data projections, is referred to as a "tour". The specific kind of tour that moves between the original and nearby projections is known as a "local tour".

In @fig-idx-tour, six frames have been chosen from the animation available at https://vimeo.com/847874016. When the weight of politics is reduced (Frame 1 and 6 vs. Frame 12), the difference in GGGI values between the top Nordic countries and mid- or low-ranked countries narrows, suggesting a smaller variation among countries in achieving gender parity. Conversely, when the weight of politics increases (Frame 18, 24, and 29 vs. Frame 12), nearly all the countries in the three categories experience a decrease in GGGI values. A noticeable exception to this trend is Bangladesh, where its index value moves in the same direction as the politics weight. This leads to its index value being almost similar to those of the top-ranked Nordic countries when the politics dimension is assigned a weight of 0.52 in Frame 29.

In GGGI, the index value has a direct interpretation as the percentage of gender gap that has been closed. When countries exhibit closer index values or consistent, it can be directly interpreted as a reflection of a country's progress toward gender parity. Ideally, an index should be robust against minor changes in its construction components. This example provides researchers with means to observe changes in the index resulting from variations in the weights used in linear combination. This approach can be applied broadly to different sets of weights of interest, extending beyond the change in a single dimension illustrated by the example.

```{r eval = FALSE}
dt <- tidyindex::gggi %>% 
  dplyr::select(country: political_empowerment) %>%
  filter(between(index, 0.72, 0.729)  | index < 0.6 | index >= 0.85) %>%
  arrange(index) 
colnames(dt) <- c(colnames(dt)[1:4], "economy", "education", "health", "politics")

w_proj2idx <- function(w){w/sum(w)}
w_idx2proj <- function(w){w/(sqrt(sum(w^2))) %>% matrix(nrow = length(w))}
find_next <- function(matrix, angle = 0.4){
  start <- matrix(c(0.25, 0.25, 0.25, 0.25), ncol = 1)
  tourr:::step_angle(tourr:::geodesic_info(start, matrix), angle = angle)
}
b <- array(dim = c(4, 1, 3))
b[,,1] <- matrix(c(0.3, 0.3, 0.3, 0.1), ncol = 1) %>% find_next()
b[,,2] <- matrix(c(0.25, 0.25, 0.25, 0.25), ncol = 1) %>% w_idx2proj()
b[,,3] <- matrix(c(0.1, 0.1, 0.1, 0.7), ncol= 1) %>% find_next(angle = 0.6)

render(
  dt[,5:8], planned2_tour(b), dev = "png",
  display_idx(half_range = 2,cex = 4, label_cex = 3.5, col = "black",
              panel_height_ratio = c(4,1),
              axis_bar_lwd = c(rep(3, 3), 7),
              axis_bar_col = c(rep("#000000", 3), "red"),
              axis_bar_label_cex = c(rep(2.5, 3), 3),
              axis_bar_label_col = c(rep("#000000", 3), "red"), 
              axis_label_cex = 3, frame_x_pos = 0.16, 
              axis_var_cex = 2.5,
              label = dt$country, abb_vars = FALSE),
  width = 900, height = 1400, apf = 1/20, frames = 120,
  here::here("figures/idx-tour/idx-tour-%03d.png")
)

gifski::gifski(
  list.files(here::here("figures/idx-tour"), full.names = TRUE),
  gif_file = here::here("figures/idx-tour.gif"),
  delay = 0.15, width = 900, height = 1400)

```

```{r fig-idx-tour, fig.align='center', fig.height=5}
#| fig-cap: "Six frames selected to explore how varying the weights of the politics dimension changes the index values and country rankings in Global Gender Gap Index (GGGI). The top panel shows the GGGI value against the country, ranked by its original index value in Frame 12. The bottom panel displays the weight used to produce the index values in the top panel, with each frame corresponds to a set of weights. Countries selected includes 1) top-ranked countries with GGGI > 0.85, 2) countries ranked between 57 and 62 with GGGI from 0.72 to 0.73, and 3) low-ranked countries with GGGI < 0.6. Compared to Frame 12 where equal weigts are used for the four dimension, a reduced weight in politics (Frame 1 and 6) shows narrower gaps between top and mid- or lower- ranked countries, while an increase in the politics weight (Frame 18, 24, and 29) leads to a systematic decrease of GGGI values across all the countries, except for Bangladesh. Full animation is available at https://vimeo.com/847874016."  
#frames <- c("001", "013", "037", "061", "085", "109")
frames <- c("001", "006", "012", "018", "024", "029")
ani <- paste0(here::here("figures/"), "idx-tour/", "idx-tour-", frames, ".png")
rl <- lapply(ani, png::readPNG)
gl <-  lapply(rl, grid::rasterGrob)
wrap_plots(gl, ncol = 3)  
```

<!-- When economy and education are given a greater weight, the variation in index values and ranking highlights countries that performs relatively better (moving right) or worse (moving left) in these two single dimensions. For example, the index moves to the left for Ethiopia and Senegal when economy is given a higher weight. This reveals the economy as a weakness for these two countries compared to their similarly-ranked peers.  -->

<!-- These can be helpful in understanding the sensitivity of the index value and country ranking to each variable and identifying the strengths and weaknesses of each country relative to others.  -->

# Conclusion

The paper presents a data pipeline with nine modules for constructing and analysing indexes. The pipeline increases transparency in the practice for index analysts to experiment with different index design and parameter choices to better design and apply their indexes. The significance of this work is its ability to provide a universal framework for index construction, which can be applied across different domains.

Examples have been given in the drought indexes and human development index to demonstrate computing of indexes with different parameters combinations and how alternative index design can provide insights to understand distinctive country characteristics that could sometimes be overlooked. The accompanied package, tidyindex, is not meant to provide comprehensive implementation for all indexes across all domains. Instead, it demonstrates implementing individual pipeline steps that are versatile to multiple indexes and composing new indexes from existing steps. Domain experts are welcomed to adopt the pipeline approach to develop specialised packages for specific-domains indexes.

Future work: - integrate more complex dimension reduction methods to calculate weights - strengthen the spatial processing module

# Reference
