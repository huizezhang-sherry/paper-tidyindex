---
title: "A Tidy Framework and Infrastructure to Systematically Assemble Spatio-temporal Indexes from Multivariate Data"
format:
  tandf-pdf:
    keep-tex: true  
pdf-engine: pdflatex
author:
  - name: H. Sherry Zhang
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0002-7122-1463
    email: huize.zhang@monash.edu
  - name: Dianne Cook
    affiliation: 
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0002-3813-7155
  - name: Ursula Laa
    affiliation: 
      - name: University of Natural Resources and Life Sciences
        department: Institute of Statistics
        address: Gregor-Mendel-Straße 33, 1180 Wien, Austria
        city: Vienna
        country: Austria
    orcid: 0000-0002-0249-6439
  - name: Nicolas Langrené
    affiliation: 
      - name: BNU-HKBU United International College
        department: Department of Mathematical Sciences
        address: 2000 Jintong Road, Tangjiawan, Zhuhai
        city: Zhuhai, Guangdong
        country: China
    orcid: 0000-0001-7601-4618
  - name: Patricia Menéndez
    affiliation:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, Victoria
        country: Australia
    orcid: 0000-0003-0701-6315
abstract: |
  - indexes, useful, quantify severity, early monitoring, 
  - A huge number of indexes have been proposed by domain experts, however, a large majority of them are not being adopted, reused, and compared in research or in practice. 
  - One of the reasons for this is the plenty of indexes are quite complex and there is no obvious easy-to-use implementation to apply them to user's data. 
  - The paper describes a general pipeline framework to construct indexes from spatio-temporal data, 
  - This allows all the indexes to be constructed through a uniform data pipeline and different indexes to vary on the details of each step in the data pipeline and their orders. 
  - The pipeline proposed aim to smooth the workflow of index construction through breaking down the complicated steps proposed by various indexes into small building blocks shared by most of the indexes.
  - The framework will be demonstrated with drought indexes as examples, but appliable in general to environmental indexes constructed from multivariate spatio-temporal data
keywords:
  - indexes
  - data pipeline
  - software design
bibliography: bibliography.bib  
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
#options(width = 70)
library(tidyverse)
library(latex2exp)
library(patchwork)
library(tidyindex)
library(lmomco)
library(lubridate)
library(SPEI)
library(tsibble)
library(GGally)
library(patchwork)
library(tourr)
```

# Introduction

  <!-- - Why index is useful, why people care about indexes -->
  <!-- - Define what is an index, what is not -->
  <!-- - What is the challenges with current index construction -->
  <!-- - what can be done if people adopt this pipeline/ why it is beneficial? -->
  <!-- - who would benefit from this paper -->

Indexes are commonly used to combine and summarize different sources of information into a single number for monitoring, communicating, and decision-making. Examples of those include the Air Quality Index, El Niño-Southern Oscillation Index, Consumer Price Index, QS University Rankings or Human Development Index among many others. 

To construct an index, experts typically start by defining a concept of interest that requires measurement. This concept often lacks a direct measurable attribute or can only be measure as a composite of various processes, yet it holds social and public significance. To create an index, once the underlying processes involved are identified, relevant and available variables are then defined, collected, and combined using statistical methods into an index that aims at measure the process of interest. The construction process is often not straightforward, and decisions need to be made, such as the selection of variables to be included, which might depend on data availability and the statistical definition of the index to be used, among others. For instance, the indexes constructed from linear combination of variables need to decide on the weight assigned to each variable. Some indexes have a spatial and/or temporal components, and variables can be aggregated to different spatial resolutions and temporal scales, leading to various indexes for different monitoring purposes. Hence, all these decisions can result in different index values and have different practical implications.

To be able to test different decision choices systematically for an index, the index needs to be broken down into its fundamental building blocks to analyse the contribution and effect of each component. We call this process of breaking the index construction into different steps the index pipeline.  Such decomposition of index components provides the means to standardise index construction via a pipeline and offers benefits for comparing among indexes and calculating index uncertainty. In social indexes for instance, the OECD handbook [@oecd_handbook_2008] has provided a set of steps and recommendations for constructing composite socio-economic indexes. However, there is still a need to extend these guidelines and methods to accommodate the inclusion of more methodological complex steps required for indexes in general.

In this work, we aim at providing statistical and computational methods to develop a data pipeline framework for constructing and customize indexes using data. As a companion the R package, tidyindex, is developed  to construct and explore different. Furthermore, the tidyindex package can be used to explore the effects of changes on index definition, index construction steps, and construction methods. This work provides researchers actively developing new indexes or aiming at improving indexes with the tools to easily modify and evaluate indexes. It also helps index analysts diagnose indexes, carry out sensitivity analysis to assess small perturbations in the index, and identify potential weaknesses for methodology improvement.

The rest of the paper is structured as follows: @sec-pipeline reviews the concept of data pipeline in R. The pipeline framework for index construction is presented in @sec-index-pipeline. @sec-dev explains how to add new steps into the index methodology in the form of new building blocks inside the index construction pipeline. Examples are given in @sec-examples to demonstrate the index construction with the pipeline built. 

<!-- Evaluating indexes is challenging because the underlying concept an index measures is often a latent variable and lacks direct benchmarks for comparison. In literature, researchers typically demonstrate the accuracy of an index through comparing it to some well-known indexes, such as SPI for drought indexes, or to a relevant response variable, such as crop yield for monitoring agricultural drought. For social indexes, @uncertainty proposes a Monte Carlo algorithm to compute index uncertainty through sampling the linear weights.  -->

<!-- These sometimes make indexes difficult to reach consensus for wider use and large institutes often prefer simple indexes for more stable performance and easier interpretation. -->

<!-- When indexes have a spatial and/or temporal component, different choices on how space and time is aggregated will introduce uncertainty. A slight change in the variable value can sometimes introduce a big difference for the index.  -->

# Tidy framework {#sec-pipeline}

<!-- **Why you should care about pipeline** -->

The tidy framework consists of two key components: tidy data and tidy tools. The concept of tidy data [@wickham_tidy_2014] prescribes specific rules for organising data in an analysis, with observations as rows, variables as columns, and types of observational units as tables. Tidy tools, on the other hand, are concatenated in a sequence through which the tidy data flows, creating a pipeline for data processing and modelling. These pipelines are data centric, meaning all the tidy tools or functions take a tidy data object as inputs and return a processed tidy data object, directly ready for the next operations to be applied. Also, the pipeline approach corresponds to the modular programming practice, which breaks down complex problems into smaller and more manageable pieces, as oppose to a monolithic design, where all the steps are predetermined and integrated into a single piece. The flexibility provided by the modularity makes it easier to modify certain steps in the pipeline and to maintain and extend the code base.

<!-- **What is pipeline, its underlying software design philosophy, and how these are reflected in R** -->

Examples of using a pipeline approach for data analysis can be traced back to the interactive graphics literature, including @buja_elements_1988; @sutherland_orca_2000; @xie_reactive_2014; @wickham_plumbing_2009. @wickham_plumbing_2009 argues that whether made explicit or not, a pipeline has to be presented in every graphics program and making them explicit is beneficial for understanding the implementation and comparing between different graphic systems. While this comment is made in the context of interactive graphics program, it is also applicable generally to any data analysis workflow. More recently, the tidyverse suite [@wickham_welcome_2019] takes the pipeline approach for general-purpose data wrangling and has gained popularity within the R community. The pipeline-style code can be directly read as a series of operations applied successively on tidy data object, offering a method to document the data wrangling process with all the computational details for reproducibility. 

Since the success of tidyverse, more packages have been developed to analyze data using the tidy framework for domains specific applications, a noticeable example of which is `tidymodels` for building machine learning models [@tidymodels]. To create a tidy workflow tailored to a specific domain, developers first need to identify the fundamental building blocks to create a workflow. These components are then implemented as modules, which can be combined to form the pipeline. For example, in supervised machine learning models, steps such as data splitting, model training and model evaluation are commonly used in most workflow. In the `tidymodels`, these steps are correspondingly implemented as package `rsample`, `parsnip`, and `yardstick`, agnostic to the specific model chosen. The uniform interface in tidymodels frees analysts from recalling model-specific syntax for performing the same operation across different models, increasing the efficiency to work with different models simultaneously. 

<!-- **Constructing indexes would also benefit from pipeline and embracing the aforementioned design philosophy.** -->

<!-- For constructing indexes, many indexes have provided available publicly scripts for computation, i.e. Human Development Index (HDI) [@hdi]. Some have made the first level of generalisation from scripts to functions and creates R packages specialised for calculating particular indexes, i.e. the `SPEI` package [@SPEI] for calculating drought indexes SPI and SPEI.  -->

For constructing indexes, the pipeline approach adopts explicit and standalone modules that can be assembled in different ways. Index developers can choose the appropriate modules and arrange them accordingly to generate the data pipeline that is needed for their purpose. The pipeline approach provides many advantages: 

- makes the computation more transparent, and thus more easily debugged.
- allows for rapidly processing new data to check how different features, like outliers, might affect the index value.
- provides the capacity to measure uncertainty by computing confidence intervals from multiple samples as generated by bootstrapping to original data.
- enables systematic comparison of surrogate indexes designed to measure the same phenomenon.
- it may even be possible to automate diagramatic explanations and documentation of the index.

Adoption of this pipeline approach would provide uniformity to the field of index development, research and application.

<!-- They might even  insert new modules to experiment with possible new index definitions or to understand an index's behaviour. Also, the readability of the pipeline code can complement conventional diagrams and textual information to understand the how indexes are constructed. The uniform interface of pipeline allows analysts to assemble a diverse array of indexes with a shared set of components, reducing the learning curve for adopting new implementation for different indexes.-->

<!-- This approach allows for quick experimentation with alternatives and facilitates building new indexes. Consequently, maintenance is streamlined in the long term, as all the indexes are developed in-house using a unified syntax. -->

# Details of the index pipeline {#sec-index-pipeline}

In constructing various indexes, the primary aim is to transform the data, often multivariate, into a univariate index. Spatial and temporal considerations are also factored into the process when observational units and time periods are not independent. However, despite the variations in contextual information for indexes in different fields, the underlying statistical methodology remains consistent across diverse domains. Each index can be represented as a series of modular statistical operations on the data. This allows us to decompose the index construction process into a unified pipeline workflow with a standardized set of data processing steps to be applied across different indexes.

An overview of the pipeline is given in @fig-pipeline-steps to illustrate the construction from raw data to the final indexes.
The pipeline includes eight modules for operations in the spatial, temporal, and multivariate aspects of the data as well as modules for comparing and communicating indexes.
Analysts can choose a subset of modules and reorder them as needed to construct an index.

Data pre-processing may happen before the start of the index pipeline to prepare multivariate data from different locations/ countries ready for constructing an index. For remote sensing data, it includes aligning the spatial resolution and temporal frequency of data collected from different satellite products, or merging in-situ stations with the satellite data. 

The notation used for introducing the pipeline modules is as follows. Let $\mathbf{x}(\mathbf{s};\mathbf{t})$ denote the raw data with spatial, temporal, and multivariate aspects: the spatial dimension $\mathbf{s} =  (s_1, s_2, \cdots, s_n)^\prime$ is defined in the 2D space: $\mathbf{s} \in \mathcal{D}_s \subseteq \mathbb{R}^2$, the temporal dimension $\mathbf{t} = (t_1, t_2, \cdots, t_J)^\prime$ is defined in the 1D space: $\mathbf{t} \in \mathcal{D}_t \subseteq \mathbb{R}$. When more than one variable is involved, the multivariate data can also be written as: $\mathbf{x}(\mathbf{s}; \mathbf{t}) = (x_1(\mathbf{s}; \mathbf{t}), x_2(\mathbf{s}; \mathbf{t}), \cdots, x_P(\mathbf{s}; \mathbf{t}))^\prime$.

```{r fig-pipeline-steps}
#| fig-align: center
#| fig-cap: "Diagram of pipeline steps for index construction. will need to be updated with better design and the distribution fitting step."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-overall.png"))
```

## Temporal processing

**Input: ** $x(\mathbf{s};\mathbf{t})$: multivariate data with temporal coordinates 

**Output: ** $x(\mathbf{s};\mathbf{t}^\prime)$: multivariate data with aggregated temporal axis 

**Computation:** The construction of an index sometimes needs to consider information from neighbouring time periods.
The temporal processing is a general operator on the time dimension of the data in the form of  

\begin{equation}
x(\mathbf{s};\mathbf{t}^\prime) = f_{\mathcal{\psi}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where $\psi \in \Psi \subseteq \mathbb{R}^{d_{\psi}}$ is the parameters associated with the temporal operation and $d_{\psi}$ is the number of parameter of $\psi$. 
A typical example of temporal processing is aggregation and in the drought community, these indexes are often called multi-scalar indexes.  For example,  Standardized Precipitation Index (SPI) aggregates monthly precipitation using a time scale parameter $k$ in the form of 

$$x(s_i;t_{j^\prime}) = \sum_{j = j^\prime}^{j^\prime + k - 1}x(s_i; t_j),$$ 

where $j^\prime$ is the new time index after the aggregation. Multi-scalar drought indexes are widely used to study different types of drought events. A smaller value of the time scales parameter $k$ is used for short term drought events, while a large $k$ of 18 or 24, an equivalent to a 1.5- or 2-year aggregation, is often used for long term assessment. 

## Spatial processing

**Input: ** Multivariate data with coordinates

**Output: ** Multivariate data with a new set of coordinates

**Computation:** Spatial processing may be needed when indexes are not calculated independently on each collected location or when variables collected from multiple sources need to be fused before further processing. The process can be written as a general operation in the form of 

\begin{equation}
x(\mathbf{s}^\prime;\mathbf{t}) = g_{\mathcal{\theta}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where $\theta \in \Theta \subseteq \mathbb{R}^{d_{\theta}}$ is the associated parameters in the process and $d_{\theta}$ is the number of parameter of $\theta$. 
An example of spatial processing is to align variables collected in different resolutions.
When variables are collected at different resolutions, analysts may choose to down-sample those in a finer resolution, $i$, to match those in a coarser resolution, $i^\prime$. This is a spatial aggregation and if aggregate using the mean, it can be written as 

\begin{equation}
g(x) = \frac{\sum_{i \in i^\prime}x}{n_{i^\prime}},
\end{equation}

where $i \in i^\prime$ includes all the cells from the finer resolution in the coarser grid and $n_{i^\prime}$ is the number of observations falls into the  coarser grid. Other examples of spatial processing include 1) borrowing information from neighbouring spatial locations to interpolate unobserved locations and 2) fusing variables from ground measures with satellite imageries.

## Variable transformation 

**Input: **

**Output: **

**Computation:** The purpose of variable transformation is to create variables that fits assumptions for further computing. These assumptions include a stable variance, normal distribution, or a certain scale required by some algorithms down the pipeline. Variable transformation is a general notion of a functional transformation on the variable: 

\begin{equation}
h_{\tau}(x(\mathbf{s};\mathbf{t})),
\end{equation} 

where $\tau \in T \subseteq \mathbb{R}^{d_{\tau}}$ is the parameter in the transformation if any, and $d_{\tau}$ is the number of parameter of $\tau$. Transformation is needed for data that are highly skewed and some common transformations include log, quadratic, and square root transformation.

<!-- A common assumption is normality in drought index.  -->

## Scaling

**Input: **

**Output: **

**Computation:** While scaling can be seen as a specific type of variable transformation, it is separated into its own step to make the step explicit in the pipeline.
The key difference between the two steps is that variable transformation typically changes the shape of the data while scaling only changes the data scale and can usually be written in the form of 

\begin{equation}
[x(s_i;t_j) - \alpha]/\gamma.
\end{equation}

For example, a z-score standardisation can be written in the above form with $\alpha = \bar{x}(s; t)$ and $\gamma = \sigma(s; t)$, a min-max standardisation uses $\alpha = \min[x(s_i, t_j)]$ and $\gamma = \max[x(s_i, t_j)] - \min[x(s_i, t_j)]$.
@fig-scale-var-trans-compare shows a collection of variable pre-processing operations and uses color to differentiate whether the operation is a variable transformation or a scaling step. While both variable transformation and scaling are pre-processing steps, the scaling operations in green show the same distribution as the original data. 

<!-- Take one or multiple variables to create a new variable,  -->
<!-- Restrict it to single variable, square root, log etc could be linearly, also non-linear -->

<!-- change the shape of the variable -->

<!-- <!-- A conversion of non-normal variables into (approximately) normal. wrong! -->

<!-- GAM, can you do additive model pairwise/ three-way -->

<!-- While being a common technique in data pre-processing, scaling has an impact on the outliers of the data and analysts need to be cautious on checking its effect. -->


```{r fig-scale-var-trans-compare}
#| fig-cap: "Comparison of operations in scaling (green) and variable transformation (orange) steps in free scale. Variables after the scaling operations have the same distribution as the origin, while the distribution changes after variable transformation. "
#| fig-height: 8
#| fig-width: 12
set.seed(123)
x <- rgamma(n = 1000, shape = 1, scale = 2)


trans_df <- tibble(origin = x, 
       `z-score` = (x - mean(x)/ sd(x)), 
       qudratic = x^2,
       log = log(x),
       `square root` = sqrt(x),
       `cubic root` = x^(1/3),
       minmax = (x - min(x))/ (max(x) - min(x)),
       boxcox = (x^0.5 - 1)/0.5,
       centering = x - mean(x)
       ) %>% 
  mutate(id = row_number(),
         ) %>% 
  pivot_longer(-id, names_to = "var", values_to = "value") %>% 
  mutate(category = ifelse(
    var %in% c("origin", "z-score", "minmax", "centering"),
    "Scaling", "Var. Trans.")
    ) 

latex_df <- tibble(
  var = c("origin", "z-score", "qudratic", "log", "square root", "cubic root",
          "minmax", "boxcox", "centering"),
  latex = c(
           r"($x$)",
           r"($\frac{x - \bar{x}}{sd(x)}$)",
           r"($x^2$)",
           r"($log(x)$)",
           r"($\sqrt{x}$)",
           r"($x^{1/3}$)",
           r"($\frac{x - min(x)}{max(x) - min(x)}$)",
           r"($\frac{x^{0.5} - 1}{0.5}$)",
           r"($x - \bar{x}$)"
         )
) %>% 
  left_join(trans_df %>% group_by(var) %>% summarise(max = max(value)))

trans_df %>% 
  ggplot() +
  geom_density(aes(x = value, color = category, y = after_stat(scaled))) + 
  geom_label(data = latex_df, 
             aes(x = 0.8 * max, y = 0.8, label = TeX(latex, output = "character")), 
             parse = TRUE) + 
  facet_wrap(vars(var), scales = "free") + 
  scale_color_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(legend.position = "bottom")
```

## Dimension reduction

**Input: **

**Output: **

**Computation:** Dimension reduction summarises the multivariate information into univariate, which can be denoted as: 

\begin{equation}
x_p(\mathbf{s}; \mathbf{t}) \rightarrow x(\mathbf{s}; \mathbf{t}),
\end{equation}

where $p = 1, 2, \cdots, P$. The combination can be based on domain-specific knowledge, originated from theories describing the underlying physical process. For example, the SPEI uses a water balance model ($D = P - \text{PET}$) to calculate the difference series ($D$) from precipitation ($P$) and potential evapotranspiration ($\text{PET}$). 

Another widely used approach is linear combination, which aggregates a collection of variables in a linear additive structure, expressed as: $$x(\mathbf{s}; \mathbf{t}) = \sum_{p = 1}^{P}\lambda_{p}x_p(\mathbf{s};\mathbf{t}),$$

where $\lambda_p$ denotes the weight assigned to variable $x_p$. Most indexes uses an equal weight that sum to 1, which allows each variable to contribute equally to the index. A linear combination can also be viewed as a linear projection of multivariate information by the weight vector. Projecting data from higher to lower dimension inevitably leads to information loss. For example, two countries can receive similar index, but their components could significantly differ -- one with average scores across all components and another with extreme scores the opposite ends. The selected set of weights need to be examined to understand its effect on the index and its implications for decision-making. To do this, analysts can vary the coefficients in the linear projection to observe how the index value and countries' ranking change. These changes can also be visualized using a method called tour by generating an animation of the projections among different sets of weights.

<!-- Other weight constraint used includes $\sum_{p=1}^P\lambda_p^2 = 1$, is used by principal component analysis.  -->


<!-- Spectral satellite imageries often collected at different bands and it is common to combine values from multiple bands to compose new variables relevant to the index. The most common example of this is the Normalized Difference Vegetation Index (NDVI), which uses near-infrared (NIR) ($x_1(s;t)$) and red channel ($x_2(s;t)$): -->

<!-- \begin{equation} -->
<!-- \text{NDVI}(s;t) = \frac{x_1(s;t) - x_2(s;t)}{x_1(s;t) + x_2(s;t)}, -->
<!-- \end{equation} -->

<!-- which is used to evaluate the vegetation from near-infrared (NIR) and red channel. -->



## Distribution fit

*model fit? *

**Input: **

**Output: **

**Computation:** Distribution fit can be seen as the model fitting in its simplest term. It can be represented by 

\begin{equation}
F_{\eta}(x(\mathbf{s}; \mathbf{t})), 
\end{equation}

where $\eta \in H \subseteq \mathbb{R}^{d_{\eta}}$ is the distribution parameter and $d_{\eta}$ is the number of parameter of $\eta$. A distribution fit typically aims at finding the distribution that best fits the data. 
Analysts may start from a pool of candidate distributions with a chosen fitting method and goodness of fit measure. 
While it is useful to find the ultimate best distribution to fits the data, from a probabilistic perspective, the fitting procedure itself has an uncertainty associated with the data fed and the parameter chosen. A reasonable alternative is to understand how much the index values can vary given different distributions, fitting methods, and goodness of fit tests, and whether these variations are negligible in a given application.

## Normalising

**Input: **

**Output: **

**Computation:** This step maps the univariate series into a different scale, typically for ease of comparison across regions. 
For example, a normal scale, [0, 1], or [0, 100] may be favored for reporting certain indexes. 
In drought indexes, i.e. SPI or SPEI, the quantiles from the fitted distribution are converted into the normal scale via the normal reverse CDF function: $\Phi^{-1}(.)$. 
Normalising is usually used at the end of the pipeline and its main difference from the scaling step is that here the change of scale also changes the distribution of the variable. 
While being commonly used, this step can get criticism from analysts for forcing the data into the decided scale, which can be either unnecessary or inaccurately exaggerate or downplay the outliers. 
Also, the use of a normal scale needs to be interpreted with caution. 
@fig-normalising illustrates the normal density not being directly proportional to its probability of occurrence.
This is concerning, especially at the extreme values, since a small difference in the tail density can have magnitudes of difference in its probability of occurrence.

```{r fig-normalising}
#| fig-cap: "Scatterplot of normal quantiles against their density values. THree tail density values are highlighted with its probability of occurence labelled. Probability is calculated assuming monthly data: with a density of -2, the probability of occurrence is 1/pnorm(-2)/12 = 4 years. The non-linear relationship between the two quantities suggests normalised indexes need to be interpreted with caution since a slight change in the tail distribution can result in magnitudes of difference in its probability of occurrence."
#| fig-width: 9
library(tidyverse)
all <- tibble(x = seq(-3, 3, 0.01), y = pnorm(x)) 
dt <- tibble(x = c(-3, -2.5, -2), y = pnorm(x)) 
label <- tibble(x = c(-3, -2.5, -2), y = pnorm(x), 
                label = c("once in 62 years", "once in 13 years", "once in 4 years"))

dt %>% 
  ggplot() + 
  geom_point(aes(x= x, y = y)) + 
  geom_line(data = all, aes(x = x, y = y)) + 
  ggrepel::geom_label_repel(
    data = label, aes(x = x, y = y, label =label),
    nudge_y = 0.2, nudge_x = 0.1,
    arrow = arrow(length = unit(0.08, "inches")), min.segment.length = 0) +
  scale_x_continuous(breaks = seq(-3, 3, 1)) + 
  xlab("Normalised Score") + 
  ylab("Probability") + 
  theme_bw() + 
  theme(panel.grid.minor = element_blank())
```

## Benchmarking

**Input: **

**Output: **

**Computation:** Benchmarking sets a constant value to allow the constructed index to be compared across time.
Here we denote it with $u[x(s_i, t_j)]$ where $u$ is a scalar of interest in the index constructed. A benchmark value could be a constant or a function of the data, i.e. mean.

## Simplification

**Input: **

**Output: **

**Computation:** In public communication, the index values are usually accompanied by a categorical grade. The categorised grades are an ordered set of descriptive words or colors to communicate the severity or guide the comprehension of the indexes. 
The mapping from continuous index values to the discrete grades is called simplification in the pipeline and it can be written as a piece-wise function: 

\begin{equation}
\begin{cases}
C_0 & c_1 \leq (s_i; t_j) < c_0 \\
C_1 & c_2 \leq x(s_i; t_j) < c_1 \\
C_2 & c_3 \leq x(s_i; t_j) < c_2 \\
\cdots \\
C_z & c_z \leq x(s_i; t_j)
\end{cases}
\end{equation}

where $C_0, C_1,\cdots ,C_z$ are the categories and $c_0, c_1, \cdots, c_z$ are the thresholds for each category. In SPI, droughts are sorted into four categories: mild drought: $[-0.99, 0]$; moderate drought: $[-1.49, -1]$; severe drought: $[-1.99, -1.5]$, and extreme drought: $[-\infty, -2]$. In this case, $C_0, C_1, C_2, C_3$ are the drought categories: mild, moderate, severe, and extreme drought ($z = 3$) and $c_0 =0, c_1 = -1, c_2 = -1.5, c_3 = -2$ are the cutoff value for each class.

# Software design {#sec-dev}

<!-- # 1. unpack or the argumebt s  -->
<!-- # 2. construct the syntax to evalute: ~rescale_minmax(life_exp, min = xxx, max = xxxx) -->
<!-- # 3. evalute  -->
<!-- # 4. update the data object  -->




<!-- # [Extending the pipeline] a different name here: alternate a module with a different (method) -->

# Examples {#sec-examples}

This section uses the example of drought and social indexes to show the analysis made possible with the index pipeline. The drought index example computes two indexes with various time scales and distributions simultaneously using the pipeline framework to understand the flood and drought events in Queensland. The social index example focuses on the dimension reduction in Global Gender Gap Index to explore the impact of weight changes in linear combination on index value and country ranking. 

## Every distribution, every scale, every index all at once

A common task for drought researchers is to compute indexes at different parameter combinations. This can be used to identify the spatial and temporal extent of drought events, recommend the best parameter choice, or compare the effectiveness of indexes for monitoring drought. The example below computes two indexes: SPI and SPEI, at various time scales and fitted distributions, for stations in the state of Queensland in Australia. The purpose of the example is to demonstrate the interfaces the tidyindex package built to allow easy computing at different parameter combinations. 

The state of Queensland in Australia is frequently affected by natural disaster events such as flood and drought, which can have significant impacts on its agricultural industry.  This study uses daily data from Global Historical Climatology Network Daily (GHCND), accessed via the package `rnoaa` to examine drought/flood condition in Queensland. Daily data is average into monthly and stations are excluded if monthly data contains missings, which is required for calculating both SPI and SPEI. This gives `r length(unique(queensland$id))` stations with complete records from 1990 January to 2022 April.

```{r fig-pipeline-spei}
#| fig-align: center
#| fig-cap: "Diagram of pipeline steps for index construction. will need to be updated with better design and the distribution fitting step."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-spei.png"))
```

The function `compute_indexes()` can be used to collectively compute multiple indexes. The tidyindex offers wrapper functions, with the prefix idx_, that simplify the calculation of commonly used indexes by combining a set of pipeline steps into a single function. For example, the function `idx_spei()` includes the five steps previously described in @sec-toy-example (variable transformation, dimension reduction, temporal aggregation, distribution fit, and normalise). Each `idx_xxx()` function specifies the relevant parameters relevant to the index: the thornthwaite method is used to calculate PET in SPEI, with the average temperature (`tavg`) and latitude (`lat`) used as inputs. The SPEI is computed at four time scales (6, 12, 24, and 36 months) and fitted with two distributions (Log-logistic and General Extreme Value (GEV)). The SPI is also computed at the same four time scales and uses the default gamma distribution to fit the aggregated series. 

```{r cache=TRUE, echo = TRUE, message=FALSE}
.scale <- c(6, 12, 24, 36)
idx <- queensland %>%
  init(id = id, time = ym) %>%
  compute_indexes(
    spei = idx_spei(
      .pet_method = "thornthwaite", .tavg = tavg, .lat = lat,
      .scale = .scale, .dist = c(gev(), loglogistic())),
    spi = idx_spi(.scale = .scale)
  )
```

The output from `compute_indexes()` contains index values and associated parameter in a long tibble. It includes the original variables (`id`, `ym`, `prcp`, `tmax`, `tmin`, `tavg`, `long`, `lat`, and `name`), index parameters (`.idx`, `.scale`, `.method`, and `.dist`), intermediate variables (`pet`, `.agg`, and `.fitted`), and the final index (`.index`). This data can be visualised across space or time, or simultaneously, to explore the wet/dry condition in Queensland. @fig-compute-spatial visualises the spatial distribution of SPI at two periods (2010 October - 2011 March and 2019 October - 2020 March) with significant natural disaster events: 2010/11 Queensland flood and 2019 Australia drought, which contributes to the notorious 2019/20 bushfire. @fig-compute-temporal displays the sensitivity of the SPEI series for one particular station, Texas post office, at different time scales and fitted distributions. These two plots demonstrate some possibilities to explore the indexes after they are computed from `compute_indexes()`. 

```{r fig-compute-spatial}
#| fig-cap: "Spatial distribution of Standardized Precipitation Index (SPI-12) in Queensland, Australia during two major flood and drought events: 2010/11 and 2019/20. The map shows a continous wet period during the 2010/11 flood period and a mitigated drought situation, after its worst in 2019 December and 2020 Janurary, likely due to the increased rainfall in February from the meteorological record."
#| fig-width: 12
#| fig-height: 5
queensland_map <- ozmaps::abs_ste %>% filter(NAME == "Queensland") %>% 
  rmapshaper::ms_simplify(keep = 0.02)
queensland_map %>%
  ggplot() +
  geom_sf(fill = "transparent", color = "grey50", linewidth = 0.4) +
  geom_point(
    data = idx %>%
      filter(.idx == "spi", .scale == 12) %>%
      filter(((year(ym) %in% c(2010, 2019) & month(ym) >= 10) )|
               ((year(ym) %in% c(2011, 2020) & month(ym) <= 3) )), 
    aes(x = long, y = lat, color = .index)) +
  colorspace::scale_color_continuous_divergingx(palette = "Geyser", rev = TRUE, name = "Index") + 
  theme_void() +
  facet_wrap(vars(ym), ncol = 6)
```

```{r fig-compute-temporal}
#| fig-cap: 'Time series plot of Standardized Precipitation-Evapotranspiration Index (SPEI) at the Texas post office station (highlighted by a diamond shape in panel a). The SPEI is calculated at four time scales (6, 12, 24, and 36 months) and fitted with two distributions (Log Logistic and GEV). The dashed line at -2 represents the class "extreme drought" by the SPEI. A larger time scale gives a smoother index series, while also takes longer to recover from an extreme situation as seen in the  2019/20 drought period. The SPEI values from two distribution fits mostly agree, while GEV can results in more extreme values, i.e. in 1998 and 2020.'
#| fig-width: 12
#| fig-height: 6
texas <- idx %>%
    filter(name == "TEXAS POST OFFICE", .idx == "spei") %>% distinct(long, lat)
p1 <- queensland_map %>%
  ggplot() +
  geom_sf(fill = "transparent") +
  geom_point(data = queensland %>%
               distinct(id, long, lat, name),
             aes(x = long, y = lat)) +
  geom_point(data = texas, aes(x = long, y = lat),
             color = "orange", shape = 18, fill = "orange", size = 4) +  
  theme_void()


p2 <- idx %>%
  filter(name == "TEXAS POST OFFICE", .idx == "spei") %>%
  mutate(.dist = ifelse(.dist == "pargev", "GEV", "Log Logistic")) %>%
  rename(scale = .scale) %>% 
  ggplot(aes(x = ym, y = .index, color = .dist, group = .dist)) +
  geom_line() +
  theme_benchmark() +
  facet_wrap(vars(scale), labeller = label_both, ncol = 1) +
  scale_x_yearmonth(breaks = "2 year", date_labels = "%Y") +
  scale_color_brewer(palette = "Dark2", name = "Distribution") + 
  xlab("Year") + 
  ylab("Index")

(p1 | p2)  + 
  patchwork::plot_annotation(tag_levels = "a") + 
  patchwork::plot_layout(guides = "collect")  &
  theme(legend.position = "bottom") 
  
```

## Does a minor change in variable weights cause a tornado? 

The Global Gender Gap Index (GGGI), published annually by the World Economic Forum, measures gender parity by assessing relative gaps between men and women in four key areas: Economic Participation and Opportunity, Educational Attainment, Health and Survival, and Political Empowerment [@WEF2023]. The index is composed of 14 variables, expressed as female-to-male ratios, which are first aggregated in a linear combination into the four dimensions using the weight from the `V-weight` column in @tbl-gggi-weights. The weight is calculated as the inverse of the standard deviation of each variable and scaling to sum to 1 within each dimension to allow a one percentage point change in the standard deviation of each variable to contribute equally to the index. The four dimensions are then aggregated in another linear combination with equal weight to obtain the index. The 2023 GGGI data is available from the Global Gender Gap Report 2023 in the country's economy profile and can be accessed in R via the `tidyindex` package as `gggi`, along with the corresponding weights `gggi_weights`.

```{r tbl-gggi-weights}
#| tbl-cap: Weights of the fourteen variables in Global Gender Gap Index
#| tbl-cap-location: bottom
gggi_weights %>%  
   mutate(variable = str_replace_all(variable, "_", " ") %>%
            str_to_sentence()) %>% 
  select(c(1,2, 5:7)) %>% 
  mutate(dimension = c(rep("Economy", 5), rep("Education" ,4), 
                       rep("Health", 2), rep("Politics", 3))) %>% 
  knitr::kable(
    digits = 3, 
    col.names = c("Variable", "Dimension", "V-weight",
        "D-weight", "Weight")
    ) 
```

A natural thing to do when provided with the index data is to reproduce the index. This helps index analysts to verify the index calculation and become familiar with the methodology. For GGGI, the construction can be simplified as a single linear aggregation step in the dimension reduction module, with the `Weight` column in @tbl-gggi-weights, which is the product of the variable weight (`V-weight`) and dimension weight (`D-weight`). 

```{r fig-pipeline-steps-gggi}
#| fig-align: center
#| fig-cap: "Diagram of pipeline steps for index construction. will need to be updated with better design and the distribution fitting step."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-gggi.png"))
```

````
gggi %>% 
  init(id = country) %>%
  add_meta(gggi_weights, var_col = variable) %>% 
  dimension_reduction(
    index_new = aggregate_linear(
      ~labour_force_participation:years_with_female_head_of_state,
      weight = weight)) 
````

The result can be compared with the GGGI values available in the report as shown in @fig-compare-gggi, validating the reproducibility of the index for country with no missing variables.

```{r eval = FALSE}
a <- dim_weights %>% select(-variable) %>% t() %>% as_tibble(rownames = "id")
colnames(a) <- c("id", dim_weights$variable)
a %>% 
  pivot_longer(-id, names_to = "vars", values_to = "value") %>% 
  ggplot(aes(x = value, group = vars)) + 
  geom_histogram() + 
  facet_wrap(vars(vars))
```

```{r fig-compare-gggi}
#| fig-cap: "Verifying the calculation of the Global Gender Gap Index (GGGI). The index can be reproduced from the methodology described in the Global Gender Gap Report 2023 after removing the countries with missing variables, the treatment of which is unclear."
set.seed(123)
n <- 1000
w_idx <- map(1:n, function(x){
  w_idx <- stats::runif(4, min = 0.1, max = 1)
  w_idx <- w_idx/sum(w_idx)
  w_idx %>% as.matrix(nrow = 4, ncol = 1)
  })
rand_w <- reduce(w_idx, cbind) %>% as_tibble()
colnames(rand_w) <- paste0("V", 1:n)

dt2 <- gggi %>% 
  init(id = country) %>%
  add_meta(gggi_weights, var_col = variable) %>% 
  dimension_reduction(
    index_new = aggregate_linear(
      ~labour_force_participation:years_with_female_head_of_state,
      weight = weight)) 
  
dt2$data %>%
  filter(!is.na(index_new)) %>% 
  ggplot(aes(x = index, y = index_new)) +
  geom_abline(slope = 1, intercept = 0, color = "grey") + 
  geom_point() +
  theme_bw() + 
  theme(aspect.ratio = 1, panel.grid.minor = element_blank()) + 
  xlab("Benchmark") + 
  ylab("Index calculated")
```

```{r eval = FALSE, gggi-weights, fig.height=20, fig.align='center', fig.width=10}
dt3 <- dt2 %>%
  swap_values(.id = 1, .param = weight,
              .value = list(pillar_weight, paste0("V", 2:100)),
              .raw_data = dt)

idx_wide <- dt3$data %>% select(country, dplyr::contains("index_new"))
idx_wide2 <- idx_wide %>% mutate(index_avg = rowMeans(idx_wide[,2:(ncol(idx_wide))]))
idx_long2 <- idx_wide2 %>% pivot_longer(-country, names_to = "index", values_to = "value") %>% 
  left_join(gggi %>% select(country, rank), by = "country")

idx_long2 %>%
  ggplot(aes(x = value, y = fct_reorder(country, -rank), group = country)) +
  geom_boxplot() + 
  #geom_point(color = "grey80", size = 0.5) +
  geom_point(data = idx_long2 %>% filter(index == "index_new1"), color = "red") + 
  theme_bw() + 
  ylab("Country") + 
  xlab("Index")
```

To understand the uncertainty of this dimension reduction step while avoiding the missingness issue on the variable level, we can run a local tour to slightly vary the weight of each dimension to see how index value and country ranking changes. We select countries in the South Asia and Sub-Saharan Africa region and gradually increase the weight of one variable and reduce back to equal weight all four dimensions, one at a time, to produce an animation of how GGGI changes in each country. Five frames (equal weights and one for each dimension with a relatively higher weight) selected from the tour animation are shown in @fig-idx-tour and you can find the link to the full animation in figure caption. 

Many insights can be derived from the animation and these selected frames. These can be helpful in understanding the sensitivity of the index value and country ranking to each variable and identifying the strengths and weaknesses of each country relative to others. When economy and education are given a greater weight, the variation in index values and ranking highlights countries that performs relatively better (moving right) or worse (moving left) in these two single dimensions. For example, the index moves to the left for Ethiopia and Senegal when economy is given a higher weight. This reveals the economy as a weakness for these two countries compared to their similarly-ranked peers. When health receives a higher weight, the impact on the index value is minimal. Notably, increasing the weight in politics leads to a pronounced decrease of index value for almost all the countries. Given the index value has a direct interpretation on how much of the gender gap has been closed, such a weight variation can cast a negative light on the progression to gender parity. Analysts need to further investigate the construction of the politics dimension and consider whether this is desirable for the index.
 
```{r eval = FALSE}
dt <- tidyindex::gggi %>% 
  dplyr::select(country: political_empowerment) %>%
  filter(country %in% c("Namibia", "South Africa", "Bangladesh", "Botswana",
                        "Ethiopia", "Kenya", "Senegal", "Afghanistan")) %>%
  arrange(index) 
colnames(dt) <- c(colnames(dt)[1:4], "economy", "education", "health", "politics")

w_proj2idx <- function(w){w/sum(w)}
w_idx2proj <- function(w){w/(sqrt(sum(w^2))) %>% matrix(nrow = length(w))}
find_next <- function(matrix){
  start <- matrix(c(0.25, 0.25, 0.25, 0.25), ncol = 1)
  tourr:::step_angle(tourr:::geodesic_info(start, matrix), 0.4)
}
b <- array(dim = c(4, 1, 9))
b[,,c(1,3,5,7,9)] <- matrix(c(0.25, 0.25, 0.25, 0.25), ncol = 1) %>% w_idx2proj()
b[,,2] <- matrix(c(0.7, 0.1, 0.1, 0.1), ncol = 1) %>% find_next()
b[,,4] <- matrix(c(0.1, 0.7, 0.1, 0.1), ncol= 1) %>% find_next()
b[,,6] <- matrix(c(0.1, 0.1, 0.5, 0.1), ncol= 1) %>% find_next()
b[,,8] <- matrix(c(0.1, 0.1, 0.1, 0.5), ncol = 1) %>% find_next()

render(
  dt[,5:8], planned2_tour(b), dev = "png",
  display_idx(cex = 4, label_cex = 3.5, panel_height_ratio = c(3,1), 
              axis_cex = 2.5, weight_cex = 3, label_col = "grey70", 
              label = dt$country, abb_vars = FALSE),
  width = 900, height = 900, apf = 1/20, frames = 100,
  here::here("figures/idx-tour/idx-tour-%03d.png")
)

gifski::gifski(
  list.files(here::here("figures/idx-tour"), full.names = TRUE),
  gif_file = here::here("figures/idx-tour.gif"),
  delay = 0.15, width = 900, height = 900)

```

```{r fig-idx-tour, fig.align='center', fig.height=8} 
#| fig-cap: "Five frames selected from varying the linear weights of four dimensions in Global Gender Gap Index. The weights vary slightly from the official simple average weights (0.25, 0.25, 0.25, 0.25) to observe how the index and ranking reponse. Full animation is available at https://vimeo.com/847874016?share=copy."
frames <- c("001", "013", "037", "061", "085")
ani <- paste0(here::here("figures/"), "idx-tour/", "idx-tour-", frames, ".png")
rl <- lapply(ani, png::readPNG)
gl <-  lapply(rl, grid::rasterGrob)
wrap_plots(gl, ncol = 2)  
```

# Conclusion

The paper presents a data pipeline with nine modules for constructing and analysing indexes. The pipeline increases transparency in the practice for index analysts to experiment with different index design and parameter choices to better design and apply their indexes. The significance of this work is its ability to provide a universal framework for index construction, which can be applied across different domains.

Examples have been given in the drought indexes and human development index to demonstrate computing of indexes with different parameters combinations and how alternative index design can provide insights to understand distinctive country characteristics that could sometimes be overlooked. The accompanied package, tidyindex, is not meant to provide comprehensive implementation for all indexes across all domains. Instead, it demonstrates implementing individual pipeline steps that are versatile to multiple indexes and composing new indexes from existing steps. Domain experts are welcomed to adopt the pipeline approach to develop specialised packages for specific-domains indexes.

Future work:
  - integrate more complex dimension reduction methods to calculate weights 
  - strengthen the spatial processing module


# Reference


