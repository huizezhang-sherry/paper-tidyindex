---
title: "Demo arXiv template"
format:
  arxiv-pdf:
    keep-tex: true  
  arxiv-html: default
author:
  - name: H. Sherry Zhang
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, VIC
        country: Australia
    orcid: 0000-0002-7122-1463
    email: huize.zhang@monash.edu
    url: https://huizezhangsh.netlify.app/
  - name: Collaborators
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, VIC
        country: Australia
abstract: |
  - Indices, useful, quantify severity, early monitoring, 
  - A huge number of indices have been proposed by domain experts, however, a large majority of them are not being adopted, reused, and compared in research or in practice. 
  - One of the reasons for this is the plenty of indices are quite complex and there is no obvious easy-to-use implementation to apply them to user's data. 
  - The paper describes a general pipeline framework to construct indices from spatio-temporal data, 
  - This allows all the indices to be constructed through a uniform data pipeline and different indices to vary on the details of each step in the data pipeline and their orders. 
  - The pipeline proposed aim to smooth the workflow of index construction through breaking down the complicated steps proposed by various indices into small building blocks shared by most of the indices.
  - The framework will be demonstrated with drought indices as examples, but appliable in general to environmental indices constructed from multivariate spatio-temporal data
keywords: 
  - spatio-temporal data
  - indices
  - data pipeline
bibliography: bibliography.bib  
---

\newpage

# Introduction

<!-- Each individual index follows its own data pipeline and it can be difficult to evaluate how an index can be affected by tweaking parameters in a certain step, rearranging the order of steps, using to different method.  -->

<!-- However, even if most of the indices have available existing implementation available to users, there would be hundreds of them and it will be burdensome to remember, classify, and compare between. -->


**Why index is useful, why people care about indices**

*incorporate the following in why using index: multiple pieces of information (variables) that need to be taken into account*

Many concepts relevant to decision making cannot be directly measured, however, they are crucial for resource allocation, early prevention, and other operational purpose. 
For example, fire authorities would be interested to quantify fire risk since bushfires can have a huge impact on monetary loss, health, and the local ecosystem.
Climatologists would be interested in monitoring the change in global climate since variability in atmospheric and oceanic conditions has a direct impact on global weather and climate.
Usually this concept of interest is associated with more than one variables and these variables need to be integrated to make decisions on the subject matter.
<!-- From a modelling perspective, these unobserved quantities are called latent variables and existing methods can incorporate these latent variables in the model structure.  -->
A common approach to quantify concepts like these is to construct an index using these relevant variables. 
This allows researchers to compare the quantity of interest across entities (i.e. countries, regions) and also cross time.

**Define what is an index, what is not**

In this article, an index is defined as a tool to quantify a concept of interest that does not have a direct measure.
The concept of interest doesn't have a direct measure can because it is impractical to measure at the population level. 
For example, it would be nearly impossible to include all the available stocks in the market to characterise stock market behavior, so indices like Dow Jones Industrial Average, S&P 500, and Nasdaq Composite select a representative set of stocks to measure the overall market behavior. 
Also belonging to this category are the economic indices like the Consumer Price Index, where price changes of a basket of items are weighted to measure inflation. 
The lack of direct measure could also because the concept itself is an unobservable human construction, rather than a physical quantity that can be measured.
Many natural hazard and social concepts falls into this category. 
This includes drought indices constructed from meteorological, agricultural, hydrological, and social-economic variables, e.g. Standardised Precipitation Index (SPI) [@mckee1993relationship] and Aggregated Drought Index (ADI) [@keyantash_aggregate_2004] among others.
Social development indices like Human Development Index [@hdi] and Global Liveability Index [@gli] measure various aspects of the quality of human capital and urban life.

*still need to tweak the tone a bit: "they are called index, they are not the index we will talk about"*

Despite many quantity having the term *index* in their name, they cannot be technically classified as indices according to the definition given above. 
The reason for these quantities to lose their index memberships is that they are variables can be accurately measured given the instrument precision. 
This includes quantities like precipitation of the driest month or percentage of days when maximum temperature is below 10th percentile. 
They are measures of precipitation and percentage of days under specific conditions (dries month, maximum temperature below 10th percentile).
They are variables, or indicators, that can be used to construct indices but are not indices themselves. 
Similarly, a set of remote sensing indices are not indices, since they are measures of electromagnetic wave reflectance.
This includes Normalized Difference Vegetation Index (NDVI) [@tucker_red_1979], derived from the ratio of difference over sum on two segments in the spectrum, also called band: near-infrared (NIR) and red.
So are the "indices" derived from NDVI, e.g. Vegetation Condition Index [@kogan_application_1995]. 
Notice that this does not exclude all the construction derived from remotes sensor variables to be valid indices.
For example, Vegetation Drought Response Index [@brown_vegetation_2008] is a valid index since it integrates climate, satellite, and biophysical variables to quantify vegetation stress.

<!-- FYI:  -->

<!--   - A measure is a variable, but is not an index.  -->
<!--   - Not all multivariate quantities are indices (i.e. those remote sensor measures). It is possible that an index is constructed from a single variable (i.e. SPI). -->
<!--   - Indices is not a single/ set of measures -->

\newpage

**What is the problem with current index construction**

*indices constructed separately, but there are lots of are same [!], but hard to see from the current construction. "messy data are messy in there own way". Describe how they are same, so we can modular them*

There has not yet been a standardised workflow on index construction. 
For simple indices, users may be able to work out the calculation from the equations or workflow provided, while for more complicated construction, software is not even available for open-source consumption.
Even when the software implementation is available, most indices are computed through a single index command, [given examples but not too domain specific]. 
This software design focuses on the name of the index and includes all the steps in a single index function. 
With this design, there is little room for modification. For example, fit a new distribution that is not provided in the initial code. 
Also, when scaled to a large number of indices, it increases the cognitive load of remembering hundreds of index names. 
A better approach is to modularise these steps and construct a pipeline for users to build indices through choosing different building blocks from each module.
With this design, users focus on the operations involved in the index construction.
If an operation from another index is useful, users can borrow it to modify an existing index.



**who would benefit from this paper**

*this work could be interested to scientists actively developing new indices*

  - domain scientists who propose new indices and directly use indices in analysis. [use multiple indices to compare]
  - general scientists who uses indices as part of analysis, modelling process. The pipeline proposed in the paper opens up the possibility for general scientists to construct indices customised to their research purpose. 
  - statisticians, so as to [draw focus/ advocate] on statistical workflow and software design.
  

**what can be done if people adopt this pipeline/ why it is beneficial?**

- transparency: official published, can decompose the indices, 
- allow for calculation on uncertainty with randomization, 
- compare indices,
- tweak/ swap building blocks, 
- create new indices based on combinations of existing steps.


The rest of the paper is structured as follows: the concept of data pipeline in R is reviewed in @sec-data-pipeline-in-r.  @sec-a-pipeline-for-building-statistical-indices presents the data pipeline for index construction. @sec-incorporating-new-buliding-blocks-into-the-pipeline explains how to include a new building block in each pipeline module. 
Examples are given in @sec-examples to demonstrate the index construction with the pipeline built. 

# Data pipeline in R {#sec-data-pipeline-in-r}

## Tidy data {#sec-tidy-data}

Before the concept of tidy data [@wickham_tidy_2014], tabular data arrive at data analysts in all different ways. 
Different analysts would write customised scripts for analysing the specific data. 
These scripts can be extended to other data analysed by the same people or group but this is not generalizable directly to another dataset. 
When the tidy data concept comes, variables and values are arranged so that 1) Each variable forms a column, 2) Each observation forms a row, and; 3) Each type of observational unit forms a table.
With this specific layout, wrangling on tabular data can be standardised into a grammar of data manipulation in `dplyr` [@dplyr]. 

A similar issue happens with index construction where researchers construct their own indices in their own ways and there has not yet been a tidy principle on index construction.
Also, this tidy principle on index construction is more complex than those in tidy data and the `dplyr` package.
It has to encompass the workflow of transformation from the raw data towards the final index series.



## Data pipeline {#sec-data-pipeline}

Constructing a pipeline that divides a complex procedure into steps that can be concatenated has been adopted widely in the R community.

The data pipeline in interactive graphics is a set of steps that transform the raw data to the plots displayed on the screen. 
The initial pipeline proposed by @buja_elements_1988 involves the following steps: non-linear transformation, variables standardization, randomization, projection engine, and viewporting.
The initial pipeline proposed by @buja_elements_1988 involves the following steps: non-linear transformation, variables standardization, randomization, projection engine, and viewporting. 
Another example in the early work of pipeline by @sutherland_orca_2000 describes a three-step pipeline: variable standardization, dimension reduction, and scaling data into the viewing window. 
This pipeline also includes the transformation on spatial and temporal variables, i.e. computing time lag on temporal variables.
This pipeline also includes the transformation on spatial and temporal variables, i.e. computing time lag on temporal variables. 
@wickham_plumbing_2009 argues that whether made explicit or not, pipeline has to be presented in every graphics program and breaking down graphic rendering into steps is also beneficial for understand the implementation and compare between different graphic systems.

The data pipeline concept is further enhanced by the pipe operator (`%>%`) in R where a set of operations, or steps, can be chained together to form a set of instructions.

A more recent data pipeline is tidymodels [@tidymodels], a set of packages for machine learning models following the tidyverse principles [@tidyverse]. 
Steps: Exploratory data analysis (EDA), feature engineering, model tuning and selection, and model evaluation.

- easy to operate properly: should be designed that users know what is approriate to do
- promote good scientific methodology: should protect uses from doing stupid things,  doing right thing

good statistical practice


# A pipeline for building statistical indices {#sec-a-pipeline-for-building-statistical-indices}

The construction of natural hazard indices also follows a set of steps, which is usually illustrated using a flowchart in the paper. 
However, every researcher follows a certain design philosophy and steps taken in the index constructed by different researchers are not aligned. 
This discourages experiment with multiple indices. Initiate a new workflow when computing a new index.

The most popular indices (i.e. SPI, SPEI, etc) have existing software implementation (`SPEI`) to be applied to a different set of data.

constructing time series index should also be encapsulated in my framework

Here we assume a concept of interest is determined, relevant variables/ indicators are identified and available to construct indices.

## Raw data

**Another section on original data directly downloaded, can have different spatial resolution, temporal granularity, data quality problem. After processing them and align them together they become the "raw data"**

The data used to construct the natural hazard index usually have three dimensions, one for location, one for time, and one for multivariate. 
Mathematically, it can be written as $X_{j, s, t}$, where $j = 1, 2, \cdots, J$ for variable, $s = 1, 2, \cdots, S$ for location, and $t = 1, 2, \cdots, T$ for time.

The location $s$ can refer to vector points or areas characterised by longitude-latitude coordinates, or raster cells obtained from satellite images.

The time dimension $t$ can be daily, weekly, biweekly (14-16 days), monthly, or even quarterly

Variables

This multidimensional array structure is commonly used in geospatial analysis

Given the variety of data sources at different spatial resolution and temporal granularity, the raw data may first come in multiple pieces. 
Sometimes, even a considerable amount of work is needed to align the spatial and temporal extent of multivariate data.

A notation for different variables have different spatial and temporal granularity $X_{j_1, s_1, t_1}$???

## Spatial aggregation

mostly happen with raster data

## Scaling

A specific transformation on the scale of the data

z-score standardising, min-max standardisation into [0, 1] or [0, 100], percentage change on the baseline
close to variable transformation step

## Normalising

The purpose of normalising is for cross-comparison.
This step can get criticism from analysts for ...

specifically for converting from a fitted distribution to normal score via reverse CDF function, non-parametric formula, or empirical approximation, a common step in many index: SPI, SSI, Z score. The purpose of normalising is to convert the index into a standardised series after all the steps for the ease of comparison.

Normalising is usually the last step

## Variable transformation

<!-- Take one or multiple variables to create a new variable,  -->
Restrict it to single variable, square root, log etc could be linearly, also non-linear

change the shape of the variable

<!-- A conversion of non-normal variables into (approximately) normal. wrong! -->

GAM, can you do additive model pairwise/ three-way

## Temporal processing

## Dimension reduction

sometimes called feature extraction in the machine learning community 
With drought indices, the extraction of meaningful variables from the original data is usually supported by the water balance model, for example, in SPEI, the step that create $d$ out of precipitation and potential evapotranspiration (PET) has theoretical backup from [see paper.]

Also include weighting



## Benchmarking



## Simplification

Discretise the continuous index into a few labelled categories. For communicating the severity of natural hazard to general public.

uniform workflow to work with index construction.

-   illustration
-   math notation
-   benefit of the pipeline approach
    -   index diagnostic
    -   uncertainty

# Incorporating new buliding blocks into the pipeline {#sec-incorporating-new-buliding-blocks-into-the-pipeline}

<!-- # [Extending the pipeline] a different name here: alternate a module with a different (method) -->

# Examples {#sec-examples}

## Constructing Standardised Precipitation Index (SPI)

-   a basic workflow and congruence with results in the `SPEI` pkg
-   allow multiple distribution fit
-   allow bootstrap uncertainty

## Calculating SPEI with raster data

# Reference
