---
title: "Demo arXiv template"
format:
  arxiv-pdf:
    keep-tex: true  
pdf-engine: pdflatex
author:
  - name: H. Sherry Zhang
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, VIC
        country: Australia
    orcid: 0000-0002-7122-1463
    email: huize.zhang@monash.edu
    url: https://huizezhangsh.netlify.app/
  - name: Collaborators
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, VIC
        country: Australia
abstract: |
  - indexes, useful, quantify severity, early monitoring, 
  - A huge number of indexes have been proposed by domain experts, however, a large majority of them are not being adopted, reused, and compared in research or in practice. 
  - One of the reasons for this is the plenty of indexes are quite complex and there is no obvious easy-to-use implementation to apply them to user's data. 
  - The paper describes a general pipeline framework to construct indexes from spatio-temporal data, 
  - This allows all the indexes to be constructed through a uniform data pipeline and different indexes to vary on the details of each step in the data pipeline and their orders. 
  - The pipeline proposed aim to smooth the workflow of index construction through breaking down the complicated steps proposed by various indexes into small building blocks shared by most of the indexes.
  - The framework will be demonstrated with drought indexes as examples, but appliable in general to environmental indexes constructed from multivariate spatio-temporal data
keywords:
  - indexes
  - data pipeline
  - software design
bibliography: bibliography.bib  
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
options(width = 90)
library(tidyverse)
library(latex2exp)
library(patchwork)
library(indri)
library(GGally)
```

# Introduction

<!-- Each individual index follows its own data pipeline and it can be difficult to evaluate how an index can be affected by tweaking parameters in a certain step, rearranging the order of steps, using to different method.  -->

<!-- However, even if most of the indexes have available existing implementation available to users, there would be hundreds of them and it will be burdensome to remember, classify, and compare between. -->


**Why index is useful, why people care about indexes**

*incorporate the following in why using index: multiple pieces of information (variables) that need to be taken into account*

Many concepts relevant to decision making cannot be directly measured, however, they are crucial for resource allocation, early prevention, and other operational purpose. 
For example, fire authorities would be interested to quantify fire risk since bushfires can have a huge impact on monetary loss, health, and the local ecosystem.
Climatologists would be interested in monitoring the change in global climate since variability in atmospheric and oceanic conditions has a direct impact on global weather and climate.
Usually this concept of interest is associated with more than one variables and these variables need to be integrated to make decisions on the subject matter.
<!-- From a modelling perspective, these unobserved quantities are called latent variables and existing methods can incorporate these latent variables in the model structure.  -->
A common approach to quantify concepts like these is to construct an index using these relevant variables. 
This allows researchers to compare the quantity of interest across entities (i.e. countries, regions) and also cross time.

**Define what is an index, what is not**

In this article, an index is defined as a tool to quantify a concept of interest that does not have a direct measure.
The concept of interest doesn't have a direct measure can because it is impractical to measure at the population level. 
For example, it would be nearly impossible to include all the available stocks in the market to characterise stock market behavior, so indexes like Dow Jones Industrial Average, S&P 500, and Nasdaq Composite select a representative set of stocks to measure the overall market behavior. 
Also belonging to this category are the economic indexes like the Consumer Price Index, where price changes of a basket of items are weighted to measure inflation. 
The lack of direct measure could also because the concept itself is an unobservable human construction, rather than a physical quantity that can be measured.
Many natural hazard and social concepts falls into this category. 
This includes drought indexes constructed from meteorological, agricultural, hydrological, and social-economic variables, e.g. Standardised Precipitation Index (SPI) [@mckee1993relationship] and Aggregated Drought Index (ADI) [@keyantash_aggregate_2004] among others.
Social development indexes like Human Development Index [@hdi] and Global Liveability Index [@gli] measure various aspects of the quality of human capital and urban life.

*still need to tweak the tone a bit: "they are called index, they are not the index we will talk about"*

Despite many quantity having the term *index* in their name, they cannot be technically classified as indexes according to the definition given above. 
The reason for these quantities to lose their index memberships is that they are variables can be accurately measured given the instrument precision. 
This includes quantities like precipitation of the driest month or percentage of days when maximum temperature is below 10th percentile. 
They are measures of precipitation and percentage of days under specific conditions (dries month, maximum temperature below 10th percentile).
They are variables, or indicators, that can be used to construct indexes but are not indexes themselves. 
Similarly, a set of remote sensing indexes are not indexes, since they are measures of electromagnetic wave reflectance.
This includes Normalized Difference Vegetation Index (NDVI) [@tucker_red_1979], derived from the ratio of difference over sum on two segments in the spectrum, also called band: near-infrared (NIR) and red.
So are the "indexes" derived from NDVI, e.g. Vegetation Condition Index [@kogan_application_1995]. 
Notice that this does not exclude all the construction derived from remotes sensor variables to be valid indexes.
For example, Vegetation Drought Response Index [@brown_vegetation_2008] is a valid index since it integrates climate, satellite, and biophysical variables to quantify vegetation stress.

<!-- FYI:  -->

<!--   - A measure is a variable, but is not an index.  -->
<!--   - Not all multivariate quantities are indexes (i.e. those remote sensor measures). It is possible that an index is constructed from a single variable (i.e. SPI). -->
<!--   - indexes is not a single/ set of measures -->


**What is the challenges with current index construction**

*see if there is any paper describing this type of pains*

*useful to reference tidy data and tidy model that makes the workflow on modelling tidy somewhere in introduction*

Currently, index construction lacks a standardised workflow. 
It is often up to researchers or research institutions to decide whether to provide open source code on the new indexes, what would be the best user interface for other researchers to use the new indexes, and how easily the new indexes can be compared with other existing indexes. 
This makes the computation lack transparency and indexes cumbersome to experiment with: 
  
  - Researchers who wish to validate the indexes calculated from large institutes need to reinvent the wheels themselves since the source code used for computing is often not available for public consumption;
  - Open-source code provided by research groups has a narrow margin for exploring other options outside the provided;
  - Similar steps used by different indexes are difficult to spot since the design of the user interface for indexes often includes all the steps under a single function call; and
  - It is generally hard to inspect intermediate results during the index construction if users wish to check the output of a certain step. 

<!-- Institutes like National Oceanic and Atmospheric Administration (NOAA) and United States Drought Monitor (USDM) moderate indexes in their domains. -->
<!-- They provide public information to describe the methodology used in the calculation, publish the index values for public consumption while the source code used for computing is often not available for public consumption.  -->


<!-- For example, many indexes involve a step to fit variables into a particular distribution. -->
<!-- Index researchers may provide options for different fits, but it is generally hard to experiment with options outside the provided distributions. -->

<!-- For example, many indexes needs standardisation in its workflow, it can be used to rescale the input variables or the index series. -->
<!-- But the current construction does not make it clear through its user interface but sweep it, along with all the other steps in the index construction, under a single function call.  -->


**what can be done if people adopt this pipeline/ why it is beneficial?**

This paper proposes a data pipeline for index construction. 
By recognising the common steps shared by many indexes, we develop a pipeline that breaks down index construction into multiple modules and allow operations in various modules to be combined like building blocks to construct indexes.
The pipeline approach is general while adaptable to most index construction. 
It allows indexes to be created, studied, and compared in a structured tidy form and enables statistical analysis of indexes to be performed easily: 
More specifically, it enables researchers to 1) validate the indexes calculated from external organisations, 2) unify various indexes under the same framework for computing, 3) swap or adjust individual steps in the index construction to study their contribution, 4) calculate uncertainty on indexes through bootstrap or others, 5) enhance existing indexes through comparing and studying their statistical properties, and finally, 6) propose new indexes from combining different steps in existing indexes. 


**who would benefit from this paper**

This work is of interest to researchers actively developing new indexes since it encourages new indexes to be delivered in an easy-to-reproduce design. 
It would also provide analysts who wish to compute a range of indexes in their analysis a uniform interface to build relevant indexes from raw data. 
For statisticians and software developing engineers, this work frames the process of index construction in a more user-oriented workflow and could motivate similar research for other process in scientific computing.


The rest of the paper is structured as follows: @sec-data-pipeline-in-r reviews the concept of data pipeline in R. 
The pipeline framework for index construction is presented in @sec-a-pipeline-for-building-statistical-indexes.  @sec-incorporating-new-buliding-blocks-into-the-pipeline explains how to include a new building block in each pipeline module. 
Examples are given in @sec-examples to demonstrate the index construction with the pipeline built. 

# Data pipeline {#sec-data-pipeline-in-r}

*Think about if there is another word for data pipeline*

**Why you should care about pipeline**

Data pipeline is not a new concept to computing. It refers to a set of data processing elements connected in series, where the output of one element is the input of the next one.
@wickham_plumbing_2009 argues that whether made explicit or not, the pipeline has to be presented in every graphics program.
The paper also argues that breaking down graphic rendering into steps is beneficial for understanding the implementation and comparing between different graphic systems. 
The discussion on pipeline construction is well documented in early interactive graphics software: @buja_elements_1988, @sutherland_orca_2000, and @xie_reactive_2014 and their pipeline steps include non-linear transformation, variable standardization, randomization and dimension reduction.

**What is pipeline, its underlying software design philosophy, and how these are reflected in R**

One of the most commonly known pipeline examples is perhaps the Unix pipeline where programs can be concatenated with `|` to flow the output from the last program into the next program, i.e. 
    
````
command 1 | command 2 | command 3 | ...
````

To solve a complex problem, the Unix system builds simple programs that do one thing well and work well together.
This design is also reflected in the tidyverse ecosystem in R.
To solve a complicated data problem using tidyverse, analysts typically build the solution using a collection of tools from the tidyverse toolbox. 
The data object can flow smoothly from one command to the next, safeguarded by the tidy data format [@wickham_tidy_2014], which prescribes three rules on how to lay out tabular data.
The tidyverse tools also embrace a strong human-centered design where function names are intuitive and easy to reference through autocomplete.
With the tidyverse design principle in mind, the tidymodel suite enables analysts to build machine learning models through the data pipeline. 
It includes typical tasks required in machine learning like data resampling, feature engineering, model fitting, model tuning, and model evaluation.
An advantage of tidymodel pipeline over separate software for individual models is that analysts no longer need to write model-specific syntax to work with each model, but pipeline-specific syntax that is applicable to all the models implemented in tidymodel.
This allows users to easily experiment with a collection of machine learning models. 

**Constructing indexes would also benefit from pipeline and embracing the aforementioned design philosophy.**

In index construction, data pipeline is often presented in a workflow diagram in the research paper to illustrate how the raw data is transformed into the final indexes. 
This agrees with Wickham's argument on the presence of the data pipeline, however, more often than not, the pipeline is not made explicit in the software.
Often the time, all the steps are lumped into a single wrapper function, rather than being split into smaller, modulated functions. 
This increases the cost of maintaining and understanding the code base, gives analysts little freedom to customise the indexes for specific needs, and hinders reusing existing code for building new indexes.
A pipeline approach unites a range of indexes under a single data pipeline and analysts can compose indexes from pipeline steps like building Legos from individual bricks.
In this workflow, analysts are not limited by indexes that have been already proposed and can easily combine pipeline steps to compose novel indexes.  Analysis of the indexes (i.e. calculation of uncertainty) is also feasible by adding external code into the pipeline.

# A pipeline for building statistical indexes {#sec-a-pipeline-for-building-statistical-indexes}

## How does the pipeline constructin of an index look like? 

Consider a commonly used drought index: Standardized Precipitation-Evapotranspiration Index (SPEI)  [@spei]. Its construction involves: 1) calculating potential evapotranspiration (PET) from average temperature, `tavg`, and its difference, `d`, with precipitation, `prcp`, 2) aggregating difference series with a time scale, 3) fitting the aggregated series with a chosen distribution to get density values, and 4) converting the density values to normal quantiles. Under the pipeline approach, SPEI will be constructed as: 

```{r eval = FALSE, echo = TRUE}
DATA %>% 
  trans_pet(method = thornthwaite, var = tavg) %>% 
  aggregate(scale = 12, var = d) %>% 
  normalise(dist = gamma, fit_method = lmom) %>% 
  augment(var = .agg_prcp)
```

<!-- *FWI:*  -->

<!--   - *verbs as name for each step: `aggregate`, `normalise`* -->
<!--   - *use NSE for variable name: `prcp`, `c(tavg, prcp)`* -->
<!--   - *created variables are prefixed with dot: `.agg_prcp`* -->
<!--   - *avoid string and use function to avoid "gamma" vs. "gam": `gamma()`, `lmom()`* -->

*Change the examples to two scales values, compare how indexes look like*

Here each command corresponds to one step described above with additional arguments specific to the step. 
The pipeline construction produces the same index value as command like `spei(...)`, as shown in @fig-toy-example, but with additional benefit:

  - Multiple scales and multiple distributions can be fitted using the `c(...)` syntax to compare index values constructed from different parameterisations; 
  - Intermediate results can be checked after each step; and
  - Additional steps and analysis can be wired into the workflow for index diagnostics and customised user need.
  
  
  *everyone is providing single command function, this is also the same for spi*
  
  *may not worthwhile if only one index, as long as start changing*
  
  *just want to calculate index, single function is fine*

```{r fig-toy-example, eval = TRUE, echo = FALSE}
#| fig-align: center
#| fig-cap: "Standardised Precipitation Evapotranspiration Index (SPEI) calculated with two distribution fits in Step 3 described above: generalised logistic (`glogist`) and generalised extreme value (`gev`) distribution, using the `wichita` data from the package `SPEI`."
#| out-height: 30%
#| out-width: 100%
# library(indri)
# library(dplyr)
# library(lubridate)
# library(lmomco)
# library(ggplot2)
# library(tsibble)
# library(SPEI)
# data(wichita)
# 
# dt <- wichita %>% 
#   aggregate(var = prcp, scale = c(6, 12), index = ym, id = id, gran = month) %>%
#   filter(!is.na(.agg)) %>%
#   normalise(dist = list(gev(), loglogistic()), var = .agg) %>%
#   augment(col = .agg)
# 
# dt %>% 
#   ggplot(aes(x = ym, y = .index, color = .dist)) + 
#   geom_line()
knitr::include_graphics(here::here("figures/toy-example-spei.png"))
```


\newpage

## Pipeline steps for constructing indies

<!-- *constructing time series index should also be encapsulated in the framework* -->

*any index can be broken down into multiple steps and then we can do things with it: swap, change parameter, etc*


*variables == indicators*

An overview of the pipeline is given in @fig-pipeline-steps to illustrate the construction from raw data to the final indexes.
The pipeline includes eight modules for operations in the spatial, temporal, and multivariate aspects of the data as well as modules for comparing and communicating indexes.
Analysts are free to select the modules they need and arrange them in the order they see fit to construct indexes.
While the starting point of the pipeline is raw data, there are steps prior to this that are crucial to the success of an index. 
For example, the defined index needs to be useful for measuring the concept of interest and variables need to be collected from reliable sources with proper quality control. 

Before elaborating each of the eight pipeline modules as subsections, the data notation will be first introduced. Let $\mathbf{x}(\mathbf{s};\mathbf{t})$ denote the raw data with spatial, temporal, and multivariate aspects: the spatial dimension $\mathbf{s} =  (s_1, s_2, \cdots, s_n)^\prime$ is defined in the 2D space: $\mathbf{s} \in \mathcal{D}_s \subseteq \mathbb{R}^2$, the temporal dimension $\mathbf{t} = (t_1, t_2, \cdots, t_J)^\prime$ is defined in the 1D space: $\mathbf{t} \in \mathcal{D}_t \subseteq \mathbb{R}$. When more than one variable is involved, the multivariate data can also be written as: $\mathbf{x}(\mathbf{s}; \mathbf{t}) = (x_1(\mathbf{s}; \mathbf{t}), x_2(\mathbf{s}; \mathbf{t}), \cdots, x_P(\mathbf{s}; \mathbf{t}))^\prime$.

```{r fig-pipeline-steps}
#| fig-align: center
#| fig-cap: "Diagram of pipeline steps for index construction. will need to be updated with better design and the distribution fitting step."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-steps.png"))
```

### Temporal processing

The construction of an index sometimes needs to consider information from neighbouring time periods.
The temporal processing is a general operator on the time dimension of the data in the form of  

\begin{equation}
f_{\mathcal{\psi}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where $\psi \in \Psi \subseteq \mathbb{R}^{d_{\psi}}$ is the parameters associated with the temporal operation and $d_{\psi}$ is the number of parameter of $\psi$. 
A typical example of temporal processing is aggregation, which is used in the drought index SPI to measure the lack of precipitation for meteorological drought.
In SPI, monthly precipitation is aggregated by a time scale parameter $k$: $x(s_i;t_{j^\prime}) = \sum_{j = j^\prime-k+1}^{j^\prime}x(s_i; t_j),$ where $j^\prime$ is the new time index after the aggregation. In this notation, each spatial location is separately aggregated and precipitation is summed from $k$ month back, $j^\prime - k + 1$, to the current period, $j^\prime$, to create the aggregated series, indexed by $j^\prime$. 

*more explicit on k will influence 1) long term vs. short term, 2) uncertainty*

The choice of time scales parameter $k$ can result in variation in the calculated index values: a small $k$ of 3 or 6 months produces the index more sensitive to individual months, while a large $k$ of 24 or 36, an equivalent to a 2- or 3-year aggregation, gives dryness information relative to the long term condition. As will be shown in section [SECTION EXAMPLE], this variation may even lead to conflicting conclusions on the dry/wet condition of the area, highlighting the importance to account for index uncertainty when interpreting index values for decision-making.

*Effective drought index*

### Spatial processing

Spatial processing may be needed when indexes are not calculated independently on each collected location or when variables collected from multiple sources need to be fused before further processing. The process can be written as a general operation in the form of 

\begin{equation}
x(\mathbf{s}^\prime;\mathbf{t}) = g_{\mathcal{\theta}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where $\theta \in \Theta \subseteq \mathbb{R}^{d_{\theta}}$ is the associated parameters in the process and $d_{\theta}$ is the number of parameter of $\theta$. 
An example of spatial processing is to align variables collected in different resolutions.
When variables are collected at different resolutions, analysts may choose to down-sample those in a finer resolution, $i$, to match those in a coarser resolution, $i^\prime$. This is a spatial aggregation and if aggregate using the mean, it can be written as 

\begin{equation}
g(x) = \frac{\sum_{i \in i^\prime}x}{n_{i^\prime}},
\end{equation}

where $i \in i^\prime$ includes all the cells from the finer resolution in the coarser grid and $n_{i^\prime}$ is the number of observations falls into the  coarser grid. Other examples of spatial processing include 1) borrowing information from neighbouring spatial locations to interpolate unobserved locations and 2) fusing variables from ground measures with satellite imageries.

### Variable transformation 

The purpose of variable transformation is to create variables that fits assumptions for further computing. These assumptions include a stable variance, normal distribution, or a certain scale required by some algorithms down the pipeline. Variable transformation is a general notion of a functional transformation on the variable: 

\begin{equation}
h_{\tau}(x(\mathbf{s};\mathbf{t})),
\end{equation} 

where $\tau \in T \subseteq \mathbb{R}^{d_{\tau}}$ is the parameter in the transformation if any, and $d_{\tau}$ is the number of parameter of $\tau$. Transformation is needed for data that are highly skewed and some common transformations include log, quadratic, and square root transformation.

<!-- A common assumption is normality in drought index.  -->

### Scaling

While scaling can be seen as a specific type of variable transformation, it is separated into its own step to make the step explicit in the pipeline.
The key difference between the two steps is that variable transformation typically changes the shape of the data while scaling only changes the data scale and can usually be written in the form of 

\begin{equation}
[x(s_i;t_j) - \alpha]/\gamma.
\end{equation}

For example, a z-score standardisation can be written in the above form with $\alpha = \bar{x}(s; t)$ and $\gamma = \sigma(s; t)$, a min-max standardisation uses $\alpha = \min[x(s_i, t_j)]$ and $\gamma = \max[x(s_i, t_j)] - \min[x(s_i, t_j)]$.
@fig-scale-var-trans-compare shows a collection of variable pre-processing operations and uses color to differentiate whether the operation is a variable transformation or a scaling step. While both variable transformation and scaling are pre-processing steps, the scaling operations in green show the same distribution as the original data. 

<!-- Take one or multiple variables to create a new variable,  -->
<!-- Restrict it to single variable, square root, log etc could be linearly, also non-linear -->

<!-- change the shape of the variable -->

<!-- <!-- A conversion of non-normal variables into (approximately) normal. wrong! -->

<!-- GAM, can you do additive model pairwise/ three-way -->

<!-- While being a common technique in data pre-processing, scaling has an impact on the outliers of the data and analysts need to be cautious on checking its effect. -->


```{r fig-scale-var-trans-compare}
#| fig-cap: "Comparison of operations in scaling (green) and variable transformation (orange) steps in free scale. Variables after the scaling operations have the same distribution as the origin, while the distribution changes after variable transformation. "
#| fig-height: 8
#| fig-width: 12
set.seed(123)
x <- rgamma(n = 1000, shape = 1, scale = 2)


trans_df <- tibble(origin = x, 
       `z-score` = (x - mean(x)/ sd(x)), 
       qudratic = x^2,
       log = log(x),
       `square root` = sqrt(x),
       `cubic root` = x^(1/3),
       minmax = (x - min(x))/ (max(x) - min(x)),
       boxcox = (x^0.5 - 1)/0.5,
       centering = x - mean(x)
       ) %>% 
  mutate(id = row_number(),
         ) %>% 
  pivot_longer(-id, names_to = "var", values_to = "value") %>% 
  mutate(category = ifelse(
    var %in% c("origin", "z-score", "minmax", "centering"),
    "Scaling", "Var. Trans.")
    ) 

latex_df <- tibble(
  var = c("origin", "z-score", "qudratic", "log", "square root", "cubic root",
          "minmax", "boxcox", "centering"),
  latex = c(
           r"($x$)",
           r"($\frac{x - \bar{x}}{sd(x)}$)",
           r"($x^2$)",
           r"($log(x)$)",
           r"($\sqrt{x}$)",
           r"($x^{1/3}$)",
           r"($\frac{x - min(x)}{max(x) - min(x)}$)",
           r"($\frac{x^{0.5} - 1}{0.5}$)",
           r"($x - \bar{x}$)"
         )
) %>% 
  left_join(trans_df %>% group_by(var) %>% summarise(max = max(value)))

trans_df %>% 
  ggplot() +
  geom_density(aes(x = value, color = category, y = after_stat(scaled))) + 
  geom_label(data = latex_df, 
             aes(x = 0.8 * max, y = 0.8, label = TeX(latex, output = "character")), 
             parse = TRUE) + 
  facet_wrap(vars(var), scales = "free") + 
  scale_color_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(legend.position = "bottom")
```


\newpage

### Dimension reduction

When indexes are constructed from multivariate information, dimension reduction methods combine that information into a univariate series. In the pipeline, dimension reduction includes methods that take multivariate inputs and output the data in a lower dimension (often univariate): 

\begin{equation}
x_{p^*}(\mathbf{s}; \mathbf{t}) \rightarrow x_p(\mathbf{s}; \mathbf{t}),
\end{equation}

where $p^* = 1, 2, \cdots, P^*$ and $p = 1, 2, \cdots, P$ reduce the variable dimension from $P$ to $P^*$. 
The most commonly used dimension reduction technique is Principal Component Analysis (PCA), also called Empirical Orthogonal Function (EOF) in earth science. It can be seen as a special case of weighting, where variables are summed up in a linear combination: $$x_{p^*}(\mathbf{s}; \mathbf{t}) = \sum_{p = 1}^{P}\lambda_{p}x_p(\mathbf{s};\mathbf{t}),$$ with restrictions imposed on the weight coefficient: $\sum_{p=1}^P\lambda_p^2 = 1$. In other cases of weighting, the coefficients can be as simple as giving equal weight to each variables. 

Some dimension reduction can also be formulated from domain-specific knowledge. This can be theories that describe the physics of the phenomenon being indexed or practical formulations used to extract useful features from the raw variables. For example, in the index SPEI, a difference series is calculated between precipitation and potential evapotranspiration (PET) and the validity of this formulation is backed up by climate water balance model [Thornthwaite, 1948], which describes [...]. *Add another example of remote sensing variables i.e. NDVI = (NIR - Red) / (NIR + Red)?*


<!-- Spectral satellite imageries often collected at different bands and it is common to combine values from multiple bands to compose new variables relevant to the index. The most common example of this is the Normalized Difference Vegetation Index (NDVI), which uses near-infrared (NIR) ($x_1(s;t)$) and red channel ($x_2(s;t)$): -->

<!-- \begin{equation} -->
<!-- \text{NDVI}(s;t) = \frac{x_1(s;t) - x_2(s;t)}{x_1(s;t) + x_2(s;t)}, -->
<!-- \end{equation} -->

<!-- which is used to evaluate the vegetation from near-infrared (NIR) and red channel. -->

While suggested weights and formulas can indicate norms adored by practitioners, analysts should be given the flexibility to experiment with different combinations when constructing indexes. 
This could help understand index behavior from its sensitivity to the variables and suggest alternative weights that better suit the specific tasks.


### Distribution fit

*model fit? *

<!-- With a probability model imposed,  -->
Distribution fit can be seen as the model fitting in its simplest term. It can be represented by 

\begin{equation}
F_{\eta}(x(\mathbf{s}; \mathbf{t})), 
\end{equation}

where $\eta \in H \subseteq \mathbb{R}^{d_{\eta}}$ is the distribution parameter and $d_{\eta}$ is the number of parameter of $\eta$. A distribution fit typically aims at finding the distribution that best fits the data. 
Analysts may start from a pool of candidate distributions with a chosen fitting method and goodness of fit measure. 
While it is useful to find the ultimate best distribution to fits the data, from a probabilistic perspective, the fitting procedure itself has an uncertainty associated with the data fed and the parameter chosen. A reasonable alternative is to understand how much the index values can vary given different distributions, fitting methods, and goodness of fit tests, and whether these variations are negligible in a given application.

### Normalising

This step maps the univariate series into a different scale, typically for ease of comparison across regions. 
For example, a normal scale, [0, 1], or [0, 100] may be favored for reporting certain indexes. 
In drought indexes, i.e. SPI or SPEI, the quantiles from the fitted distribution are converted into the normal scale via the normal reverse CDF function: $\Phi^{-1}(.)$. 
Normalising is usually used at the end of the pipeline and its main difference from the scaling step is that here the change of scale also changes the distribution of the variable. 
While being commonly used, this step can get criticism from analysts for forcing the data into the decided scale, which can be either unnecessary or inaccurately exaggerate or downplay the outliers. 
Also, the use of a normal scale needs to be interpreted with caution. 
@fig-normalising illustrates the normal density not being directly proportional to its probability of occurrence.
This is concerning, especially at the extreme values, since a small difference in the tail density can have magnitudes of difference in its probability of occurrence.

```{r fig-normalising}
#| fig-cap: "Scatterplot of normal quantiles against their density values. THree tail density values are highlighted with its probability of occurence labelled. Probability is calculated assuming monthly data: with a density of -2, the probability of occurrence is 1/pnorm(-2)/12 = 4 years. The non-linear relationship between the two quantities suggests normalised indexes need to be interpreted with caution since a slight change in the tail distribution can result in magnitudes of difference in its probability of occurrence."
#| fig-width: 9
library(tidyverse)
all <- tibble(x = seq(-3, 3, 0.01), y = pnorm(x)) 
dt <- tibble(x = c(-3, -2.5, -2), y = pnorm(x)) 
label <- tibble(x = c(-3, -2.5, -2), y = pnorm(x), 
                label = c("once in 62 years", "once in 13 years", "once in 4 years"))

dt %>% 
  ggplot() + 
  geom_point(aes(x= x, y = y)) + 
  geom_line(data = all, aes(x = x, y = y)) + 
  ggrepel::geom_label_repel(
    data = label, aes(x = x, y = y, label =label),
    nudge_y = 0.2, nudge_x = 0.1,
    arrow = arrow(length = unit(0.08, "inches")), min.segment.length = 0) +
  scale_x_continuous(breaks = seq(-3, 3, 1)) + 
  xlab("Normalised Score") + 
  ylab("Probability") + 
  theme_bw() + 
  theme(panel.grid.minor = element_blank())
```

### Benchmarking

Benchmarking sets a constant value to allow the constructed index to be compared across time.
Here we denote it with $u[x(s_i, t_j)]$ where $u$ is a scalar of interest in the index constructed. A benchmark value could be a constant or a function of the data, i.e. mean.

### Simplification

In public communication, the index values are usually accompanied by a categorical grade. The categorised grades are an ordered set of descriptive words or colors to communicate the severity or guide the comprehension of the indexes. 
The mapping from continuous index values to the discrete grades is called simplification in the pipeline and it can be written as a piece-wise function: 

\begin{equation}
\begin{cases}
C_0 & c_1 \leq (s_i; t_j) < c_0 \\
C_1 & c_2 \leq x(s_i; t_j) < c_1 \\
C_2 & c_3 \leq x(s_i; t_j) < c_2 \\
\cdots \\
C_z & c_z \leq x(s_i; t_j)
\end{cases}
\end{equation}

where $C_0, C_1,\cdots ,C_z$ are the categories and $c_0, c_1, \cdots, c_z$ are the thresholds for each category. In SPI, droughts are sorted into four categories: mild drought: $[-0.99, 0]$; moderate drought: $[-1.49, -1]$; severe drought: $[-1.99, -1.5]$, and extreme drought: $[-\infty, -2]$. In this case, $C_0, C_1, C_2, C_3$ are the drought categories: mild, moderate, severe, and extreme drought ($z = 3$) and $c_0 =0, c_1 = -1, c_2 = -1.5, c_3 = -2$ are the cutoff value for each class.

# Incorporating alternative methods into the pipeline components {#sec-incorporating-new-buliding-blocks-into-the-pipeline}

<!-- # [Extending the pipeline] a different name here: alternate a module with a different (method) -->

# Examples {#sec-examples}

## A drought index example

-   a basic workflow and congruence with results in the `SPEI` pkg
-   allow multiple distribution fit
-   allow bootstrap uncertainty

easier to make comparison
pick a publication, propose a new index that has this benefit but also correlate highly with the big name indexes

\newpage

## An example on sustainable development indicators

<!-- Indexes are commonly used in social science is to rank entities (i.e. countries) on a topic through a range of indicators. This includes university ranking from QS, Times Higher Education; the Sustainable Development Goals (SDG) Index [@SDSN_Bertelsmann_2021] by the Sustainable Development Solutions Network (SDSN), the Human Development Index (HDI) [@UNDP_2021] by United Nations Development Programme (UNDP), etc. These indexes, or rankings, typically have a set of predefined dimensions, or pillars, and indicators are first reduced to these pillars and then reduced to an index.  -->

In the following example, the Human Development Index (HDI) will first be constructed using the pipeline syntax. The section will then demonstrate how expressions or parameters in the pipeline can be swapped to alternatives to study the index property.

Human Development Index (HDI) measures the development of countries through more than just economic growth, but also people's life expectancy and opportunity to receive education. These three identified dimensions are measured using four indicators: life expectancy at birth (health), expected years of schooling (education), mean years of schooling (education), and Gross National Income (GNI) *per capita* (standard of living). The technical notes [@UNDP_2021_tech_notes] have documented the procedures to calculate HDI and they are summarised below: 

  1. take log on GNI *per capita*,
  2. rescale the four indicators into [0, 1] using mini-max,
  3. aggregate the two education indicators using arithmetic mean, and
  4. aggregate the three dimensions into the index using geometric mean.
  
The values used in mini-max rescaling are summarised in @tbl-rescale-params and the justification of these numbers can be found in the technical notes mentioned above.

```{r}
#| tbl-cap-location: bottom
#| tbl-cap: Maximum and minimum values used to rescale the four HDI indicators into [0, 1] range. The maximum of GNI per capita is taken as the common log of 75,000, approximatly 4.875.
#| label: tbl-rescale-params
scaling_params <- tibble::tribble(
   ~Dimension, ~Indicator, ~Variable,  ~Minimum, ~Maximum,
  "Health",              "Life expectancy at birth (years)",   "life_exp",    "20",          "85",
  "Education",           "Expected years of schooling (years)",  "exp_sch",    "0",          "18",
  "Education",           "Mean years of schooling (years)",   "avg_sch",      "0",          "15",
  "Standard of living",  "GNI per capita (2017 PPP$)",     "gni_pc",       "100",      "75000"
  ) %>%
  mutate(across(contains("mum"), as.numeric),
         across(contains("mum"), ~ifelse(Variable == "gni_pc", log10(.x), .x)))
knitr::kable(scaling_params, digits = 3)
```

Among the four steps listed in the calculation, the first two are variable transformation. The next two can be considered as the dimension reduction step in the data pipeline given its intention to combine multivariate data into univariate. With the index pipeline proposed in the paper, HDI can be calculated as: 

```{r}
dt <- readxl::read_xlsx(path = here::here("data/hdi.xlsx") , skip = 4)
raw <- dt %>%
  janitor::clean_names() %>%
  dplyr::select(-paste0("x", seq(4, 14, 2))) %>%
  dplyr::rename(id = x1, countries = x2) %>%
  dplyr::mutate(across(c(id, human_development_index_hdi:hdi_rank), as.numeric)) %>%
  dplyr::filter(!is.na(id)) %>%
  dplyr::mutate(across(4:7, ~round(.x, digits = 3))) %>% 
  dplyr::select(-c(human_development_index_hdi,
                   gni_per_capita_rank_minus_hdi_rank, 
                   hdi_rank)) 
colnames(raw) <- c("id", "country", "life_exp", "exp_sch", "avg_sch", "gni_pc")
```

```{r echo = TRUE, message=TRUE}
dt <- raw %>% init(id = country, indicators = life_exp:gni_pc)
(res <- dt %>%
  var_trans(gni_pc = log(gni_pc, base = 10)) %>% 
  var_trans(.method = rescale_minmax, .vars = life_exp:gni_pc,
            min = c(20, 0, 0, 2), max = c(85, 18, 15, log10(75000))) %>% 
  dim_red(sch = (exp_sch + avg_sch) / 2) %>%
  dim_red(index = (life_exp * sch * gni_pc)^(1/3)))
```

The output is an `indri` object with a summary of operations followed by the data output. The data output contains the ID, country name, rescaled variables (`life_exp`, `exp_sch`, `avg_sch`, and `gni_pc`), the aggregated education dimension (`sch`), and the final index calculated (`index`).

Index analysts can be interested in studying the property of the index and the countries through the "what-if" questions. In the context of HDI, this can be: 

  - what if arithmetic mean, or different weights, is used to reduce the three dimensions into HDI? 
  - what if an alternative maximum or minimum value is used in the rescaling step? 

These questions can be answered using the function `swap_exprs()` and `swap_values()`. Both functions first use a set of arguments (`.var`, `.module`, `.step`, and `.res`) to locate the operation to be tested. Then, `.expr` and `.values` will accept a list of expressions or a vector of values for testing. Lastly, the initial `indri` (raw data) needs to be provided. 

*What if an alternative dimension reduction expression is used to reduce the three dimensions into the HDI?*

A common method to reduce the data dimension is through linear combination. With different weighting schemes, the combination can be compared to study the variable importance of each dimension or to reveal the underlying structures of countries. The following code tests five expressions (arithmetic mean, principal component analysis weight, and three weighting schemes emphasizing life expectancy, education, and GNI per capita respectively):

```{r echo = TRUE, message=TRUE}
(res2 <- res %>%
  swap_exprs(
    .var = index,
    .expr = list(
      index1 = (life_exp + sch + gni_pc)/3,
      # PCA recommended weight
      index2 = 0.569 * life_exp + 0.576 * sch + 0.586 * gni_pc, 
      index3 = 0.8 * life_exp + 0.1 * sch + 0.1 * gni_pc,
      index4 = 0.1 * life_exp + 0.8 * sch + 0.1 * gni_pc,
      index5 = 0.1 * life_exp + 0.1 * sch + 0.8 * gni_pc),
    .raw_data = dt))
```

The resulting data includes five more columns corresponding to the five alternative indexes. The country ranking can be further computed on the data and plotted in a scatterplot matrix as in @fig-hdi-expr. The plot shows almost similar ranks suggested by the indexes calculated using geometric mean (`rank0`), the arithmetic mean (`rank1`), and the PCA recommended weights (`rank2`). The agreement of the three methods supports the use of geometric mean as the dimension reduction method in the original index. The scatterplot matrix also reveals the difference in ranking when the three dimensions are given different weights in `rank3`, `rank4`, and `rank5`. The panel `rank4` vs `rank5` shows a cluster of highlighted points with relatively high ranks according to `rank5` but low in `rank4`.  These countries are printed in @tbl-hdi-expr prints along with their three dimensions, calculated indexes, and rankings. While having a high life expectancy and GNI *per capita*, these countries (Andorra, Qatar, San Marino, Kuwait, and Brunei Darussalam) score relatively low in education, which is weighted heavily in `index4` (`0.1 * life_exp + 0.1 * sch + 0.8 * gni_pc`).


```{r fig-hdi-expr}
#| fig-height: 8
#| fig-width: 12
#| fig-cap: "Scatterplot matrix of country rankings computed from different dimension reduction expressions in HDI: geometric mean (`rank0`), arithmetic mean (`rank1`), principal component analysis recommended weights (`rank2`), and heavier weight on life expectancy, education, and GNI per capita (`rank3` - `rank5`). Geometric mean, arithmetic mean, and PCA produce similar rankings where countries are aligned in a diagnoal line, while country rankings varies when compare among `rank3` to `rank5`. Panel `rank5` vs. `rank4` highlights a cluster of countries that rank high by `rank5` but low by `rank4`. The plot demonstrates the necessity to test on various alternative expressions to verify the robustness of the method and to capture distinctive characteristics of different countries. "
with_rank <- res2$data %>% 
  mutate(across(contains("index"), ~rank(-.x), .names = "rank.{.col}")) %>% 
  rename_with(~gsub(".index", "", .x), contains("rank.index"))

lower_cluster <- with_rank %>% 
  filter(rank5 < 30, rank4 > 50) %>% 
  select(country, life_exp, sch, gni_pc, index3, index4, rank3, rank4)

with_highlight <- with_rank %>% 
  mutate(highlight = ifelse(country %in% lower_cluster$country, TRUE, FALSE))

p1 <- ggpairs(
  with_highlight, columns = 14:19,
  lower = list(continuous = "points", combo = "facethist", 
               discrete = "facetbar", na = "na"),
  upper = NULL, diag = NULL, switch = "both") +
  theme(aspect.ratio = 1)

# https://stackoverflow.com/questions/65091593/how-to-mark-certain-point-in-ggpairs
p1$plots[[35]]$mapping <- 
    `class<-`(c(p1$plots[[35]]$mapping, aes(color = highlight)), "uneval")
p1 <- p1 + scale_color_manual(values = c("black", "red"))
p1
```

```{r echo = FALSE}
#| tbl-cap: A selected number of countries with low rankings when education is given a heavy weight (`index4`) while ranks high when a heavier weight is given on GNI per capital (`index5`).
#| label: tbl-hdi-expr
lower_cluster %>% 
  rename( `life expectancy` = life_exp, education = sch, `GNI per capita` = gni_pc) %>% 
  knitr::kable(digits = 3)
```

\newpage

*what if the minimum value for GNI per capita is changed?*

The lower limit for GNI per capita is set to be the common log of $100, with justification provided in the technical notes as:

  > The low minimum value for gross national income (GNI) per capita, $100, is justified by the considerable amount of unmeasured subsistence and nonmarket production in economies close to the minimum, which is not captured in the official data
  
The open source data provided by United Nations Development Programme suggest the minimum GNI *per capita* is `$732` and this opens the questions on how the index/ranking will change if the lower minimum (`$100`) is raised or decreased. We can answer this question by testing on a set of different values using `.values` in the function `swap_values()`:

```{r echo = TRUE, message = TRUE}
(res3 <- res %>%
  swap_values(
    .module = var_trans, .step = rescale_minmax, .res = gni_pc,
    .var = min, .values = log10(700),
    .raw_data = dt)) 
```

\newpage

The function computes the GNI *per capita* in both the original value (`gni_pc0`) and the new value provided (`gni_pc1`). All the subsequent operations are also computed for both cases and result in `index0` and `index1`.

```{r}
#| fig-height: 6
#| fig-width: 6
#| fig-align: center
new_rank <- res3$data %>%
  mutate(across(contains("index"), ~rank(-.x), .names = "rank.{.col}")) %>%
  rename_with(~gsub(".index", "", .x), contains("rank.index")) %>% 
  mutate(difference = rank1 - rank0)

try <- new_rank %>%
  dplyr::select(id, country, contains("life_exp"), contains("index"), contains("rank"), difference) %>% 
  arrange(-abs(difference))

new_rank %>% 
  ggplot(aes(x = rank0, y = rank1)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, color = "blue") +
  # geom_text(data = try %>% filter(difference > 15), color = "red", aes(label = country)) +
  theme(aspect.ratio = 1) +
  xlab("log10(100) as the rescale minimum for GNI per capita") +
  ylab("log10(700) as the rescale minimum for GNI per capita") 
```




\newpage

## Calculating indexes on raster data

<!-- Raster data is common data format for delivering large satellite spatio-temporal data.  -->

<!-- Issue 1: variables may be collected at different spatial and temporal extent/ scale, different resolutions and granularities.  -->

<!-- cleaning these variables involves spatial and temporal processing.  -->

<!-- Issue 2: what the data the step is operated on?? -->

<!-- When calculating on raster data, it is confusing on what data the operation is on. Is the operation operated per pixel or across regions, is the operation operated across all the time point, across year, across month etc.  -->

<!-- While it may be clear to researchers themselves, but can be confusing for others who read the analysis for the first time.  -->

<!-- The data structure should be made explicit at each operation on the spatio-temporal raster data, in terms of inputs and outputs. -->

<!-- The raster examples  -->




# Reference
