---
title: "Demo arXiv template"
format:
  arxiv-pdf:
    keep-tex: true  
  arxiv-html: default
author:
  - name: H. Sherry Zhang
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, VIC
        country: Australia
    orcid: 0000-0002-7122-1463
    email: huize.zhang@monash.edu
    url: https://huizezhangsh.netlify.app/
  - name: Collaborators
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, VIC
        country: Australia
abstract: |
  - Indices, useful, quantify severity, early monitoring, 
  - A huge number of indices have been proposed by domain experts, however, a large majority of them are not being adopted, reused, and compared in research or in practice. 
  - One of the reasons for this is the plenty of indices are quite complex and there is no obvious easy-to-use implementation to apply them to user's data. 
  - The paper describes a general pipeline framework to construct indices from spatio-temporal data, 
  - This allows all the indices to be constructed through a uniform data pipeline and different indices to vary on the details of each step in the data pipeline and their orders. 
  - The pipeline proposed aim to smooth the workflow of index construction through breaking down the complicated steps proposed by various indices into small building blocks shared by most of the indices.
  - The framework will be demonstrated with drought indices as examples, but appliable in general to environmental indices constructed from multivariate spatio-temporal data
keywords:
  - indices
  - data pipeline
  - software design
bibliography: bibliography.bib  
editor_options: 
  chunk_output_type: inline
---

# Introduction

<!-- Each individual index follows its own data pipeline and it can be difficult to evaluate how an index can be affected by tweaking parameters in a certain step, rearranging the order of steps, using to different method.  -->

<!-- However, even if most of the indices have available existing implementation available to users, there would be hundreds of them and it will be burdensome to remember, classify, and compare between. -->


**Why index is useful, why people care about indices**

*incorporate the following in why using index: multiple pieces of information (variables) that need to be taken into account*

Many concepts relevant to decision making cannot be directly measured, however, they are crucial for resource allocation, early prevention, and other operational purpose. 
For example, fire authorities would be interested to quantify fire risk since bushfires can have a huge impact on monetary loss, health, and the local ecosystem.
Climatologists would be interested in monitoring the change in global climate since variability in atmospheric and oceanic conditions has a direct impact on global weather and climate.
Usually this concept of interest is associated with more than one variables and these variables need to be integrated to make decisions on the subject matter.
<!-- From a modelling perspective, these unobserved quantities are called latent variables and existing methods can incorporate these latent variables in the model structure.  -->
A common approach to quantify concepts like these is to construct an index using these relevant variables. 
This allows researchers to compare the quantity of interest across entities (i.e. countries, regions) and also cross time.

**Define what is an index, what is not**

In this article, an index is defined as a tool to quantify a concept of interest that does not have a direct measure.
The concept of interest doesn't have a direct measure can because it is impractical to measure at the population level. 
For example, it would be nearly impossible to include all the available stocks in the market to characterise stock market behavior, so indices like Dow Jones Industrial Average, S&P 500, and Nasdaq Composite select a representative set of stocks to measure the overall market behavior. 
Also belonging to this category are the economic indices like the Consumer Price Index, where price changes of a basket of items are weighted to measure inflation. 
The lack of direct measure could also because the concept itself is an unobservable human construction, rather than a physical quantity that can be measured.
Many natural hazard and social concepts falls into this category. 
This includes drought indices constructed from meteorological, agricultural, hydrological, and social-economic variables, e.g. Standardised Precipitation Index (SPI) [@mckee1993relationship] and Aggregated Drought Index (ADI) [@keyantash_aggregate_2004] among others.
Social development indices like Human Development Index [@hdi] and Global Liveability Index [@gli] measure various aspects of the quality of human capital and urban life.

*still need to tweak the tone a bit: "they are called index, they are not the index we will talk about"*

Despite many quantity having the term *index* in their name, they cannot be technically classified as indices according to the definition given above. 
The reason for these quantities to lose their index memberships is that they are variables can be accurately measured given the instrument precision. 
This includes quantities like precipitation of the driest month or percentage of days when maximum temperature is below 10th percentile. 
They are measures of precipitation and percentage of days under specific conditions (dries month, maximum temperature below 10th percentile).
They are variables, or indicators, that can be used to construct indices but are not indices themselves. 
Similarly, a set of remote sensing indices are not indices, since they are measures of electromagnetic wave reflectance.
This includes Normalized Difference Vegetation Index (NDVI) [@tucker_red_1979], derived from the ratio of difference over sum on two segments in the spectrum, also called band: near-infrared (NIR) and red.
So are the "indices" derived from NDVI, e.g. Vegetation Condition Index [@kogan_application_1995]. 
Notice that this does not exclude all the construction derived from remotes sensor variables to be valid indices.
For example, Vegetation Drought Response Index [@brown_vegetation_2008] is a valid index since it integrates climate, satellite, and biophysical variables to quantify vegetation stress.

<!-- FYI:  -->

<!--   - A measure is a variable, but is not an index.  -->
<!--   - Not all multivariate quantities are indices (i.e. those remote sensor measures). It is possible that an index is constructed from a single variable (i.e. SPI). -->
<!--   - Indices is not a single/ set of measures -->


**What is the challenges with current index construction**

*see if there is any paper describing this type of pains*

*useful to reference tidy data and tidy model that makes the workflow on modelling tidy somewhere in introduction*

Currently, index construction lacks a standardised workflow. 
It is often up to researchers or research institutions to decide whether to provide open source code on the new indices, what would be the best user interface for other researchers to use the new indices, and how easily the new indices can be compared with other existing indices. 
This makes the computation lack transparency and indices cumbersome to experiment with: 
  
  - Researchers who wish to validate the indices calculated from large institutes need to reinvent the wheels themselves since the source code used for computing is often not available for public consumption;
  - Open-source code provided by research groups has a narrow margin for exploring other options outside the provided;
  - Similar steps used by different indices are difficult to spot since the design of the user interface for indices often includes all the steps under a single function call; and
  - It is generally hard to inspect intermediate results during the index construction if users wish to check the output of a certain step. 

<!-- Institutes like National Oceanic and Atmospheric Administration (NOAA) and United States Drought Monitor (USDM) moderate indices in their domains. -->
<!-- They provide public information to describe the methodology used in the calculation, publish the index values for public consumption while the source code used for computing is often not available for public consumption.  -->


<!-- For example, many indices involve a step to fit variables into a particular distribution. -->
<!-- Index researchers may provide options for different fits, but it is generally hard to experiment with options outside the provided distributions. -->

<!-- For example, many indices needs standardisation in its workflow, it can be used to rescale the input variables or the index series. -->
<!-- But the current construction does not make it clear through its user interface but sweep it, along with all the other steps in the index construction, under a single function call.  -->


**what can be done if people adopt this pipeline/ why it is beneficial?**

This paper proposes a data pipeline for index construction. 
By recognising the common steps shared by many indices, we develop a pipeline that breaks down index construction into multiple modules and allow operations in various modules to be combined like building blocks to construct indices.
The pipeline approach is general while adaptable to most index construction. 
It allows indices to be created, studied, and compared in a structured tidy form and enables statistical analysis of indices to be performed easily: 
More specifically, it enables researchers to 1) validate the indices calculated from external organisations, 2) unify various indices under the same framework for computing, 3) swap or adjust individual steps in the index construction to study their contribution, 4) calculate uncertainty on indices through bootstrap or others, 5) enhance existing indices through comparing and studying their statistical properties, and finally, 6) propose new indices from combining different steps in existing indices. 


**who would benefit from this paper**

This work is of interest to researchers actively developing new indices since it encourages new indices to be delivered in an easy-to-reproduce design. 
It would also provide analysts who wish to compute a range of indices in their analysis a uniform interface to build relevant indices from raw data. 
For statisticians and software developing engineers, this work frames the process of index construction in a more user-oriented workflow and could motivate similar research for other process in scientific computing.


The rest of the paper is structured as follows: @sec-data-pipeline-in-r reviews the concept of data pipeline in R. 
The pipeline framework for index construction is presented in @sec-a-pipeline-for-building-statistical-indices.  @sec-incorporating-new-buliding-blocks-into-the-pipeline explains how to include a new building block in each pipeline module. 
Examples are given in @sec-examples to demonstrate the index construction with the pipeline built. 

# Data pipeline {#sec-data-pipeline-in-r}

*Think about if there is another word for data pipeline*

**Why you should care about pipeline**

Data pipeline is not a new concept to computing. It refers to a set of data processing elements connected in series, where the output of one element is the input of the next one.
@wickham_plumbing_2009 argues that whether made explicit or not, the pipeline has to be presented in every graphics program.
The paper also argues that breaking down graphic rendering into steps is beneficial for understanding the implementation and comparing between different graphic systems. 
The discussion on pipeline construction is well documented in early interactive graphics software: @buja_elements_1988, @sutherland_orca_2000, and @xie_reactive_2014 and their pipeline steps include non-linear transformation, variable standardization, randomization and dimension reduction.

**What is pipeline, its underlying software design philosophy, and how these are reflected in R**

One of the most commonly known pipeline examples is perhaps the Unix pipeline where programs can be concatenated with `|` to flow the output from the last program into the next program, i.e. 
    
````
command 1 | command 2 | command 3 | ...
````

To solve a complex problem, the Unix system builds simple programs that do one thing well and work well together.
This design is also reflected in the tidyverse ecosystem in R.
To solve a complicated data problem using tidyverse, analysts typically build the solution using a collection of tools from the tidyverse toolbox. 
The data object can flow smoothly from one command to the next, safeguarded by the tidy data format [@wickham_tidy_2014], which prescribes three rules on how to lay out tabular data.
The tidyverse tools also embrace a strong human-centered design where function names are intuitive and easy to reference through autocomplete.
With the tidyverse design principle in mind, the tidymodel suite enables analysts to build machine learning models through the data pipeline. 
It includes typical tasks required in machine learning like data resampling, feature engineering, model fitting, model tuning, and model evaluation.
An advantage of tidymodel pipeline over separate software for individual models is that analysts no longer need to write model-specific syntax to work with each model, but pipeline-specific syntax that is applicable to all the models implemented in tidymodel.
This allows users to easily experiment with a collection of machine learning models. 

**Constructing indices would also benefit from pipeline and embracing the aforementioned design philosophy.**

In index construction, data pipeline is often presented in a workflow diagram in the research paper to illustrate how the raw data is transformed into the final indices. 
This agrees with Wickham's argument on the presence of the data pipeline, however, more often than not, the pipeline is not made explicit in the software.
Often the time, all the steps are lumped into a single wrapper function, rather than being split into smaller, modulated functions. 
This increases the cost of maintaining and understanding the code base, gives analysts little freedom to customise the indices for specific needs, and hinders reusing existing code for building new indices.
A pipeline approach unites a range of indices under a single data pipeline and analysts can compose indices from pipeline steps like building Legos from individual bricks.
In this workflow, analysts are not limited by indices that have been already proposed and can easily combine pipeline steps to compose novel indices.  Analysis of the indices (i.e. calculation of uncertainty) is also feasible by adding external code into the pipeline.

\newpage

# A pipeline for building statistical indices {#sec-a-pipeline-for-building-statistical-indices}

## How does the pipeline constructin of an index look like? 

Consider a commonly used drought index: Standardized Precipitation-Evapotranspiration Index (SPEI)  [@spei]. Its construction involves: 1) calculating potential evapotranspiration (PET) from average temperature, `tavg`, and its difference, `d`, with precipitation, `prcp`, 2) aggregating difference series with a time scale, 3) fitting the aggregated series with a chosen distribution to get density values, and 4) converting the density values to normal quantiles. Under the pipeline approach, SPEI will be constructed as: 

```{r eval = FALSE, echo = TRUE}
DATA %>% 
  trans_pet(method = thornthwaite, var = tavg) %>% 
  aggregate(scale = 12, var = d) %>% 
  normalise(dist = gamma, fit_method = lmom) %>% 
  augment(var = .agg_prcp)
```

<!-- *FWI:*  -->

<!--   - *verbs as name for each step: `aggregate`, `normalise`* -->
<!--   - *use NSE for variable name: `prcp`, `c(tavg, prcp)`* -->
<!--   - *created variables are prefixed with dot: `.agg_prcp`* -->
<!--   - *avoid string and use function to avoid "gamma" vs. "gam": `gamma()`, `lmom()`* -->

*Change the examples to two scales values, compare how indices look like*

Here each command corresponds to one step described above with additional arguments specific to the step. 
The pipeline construction produces the same index value as command like `spei(...)`, as shown in @fig-toy-example, but with additional benefit:

  - Multiple scales and multiple distributions can be fitted using the `c(...)` syntax to compare index values constructed from different parameterisations; 
  - Intermediate results can be checked after each step; and
  - Additional steps and analysis can be wired into the workflow for index diagnostics and customised user need.
  
  
  *everyone is providing single command function, this is also the same for spi*
  
  *may not worthwhile if only one index, as long as start changing*
  
  *just want to calculate index, single function is fine*

```{r fig-toy-example, eval = TRUE, echo = FALSE}
#| fig-align: center
#| fig-cap: "Standardised Precipitation Evapotranspiration Index (SPEI) calculated with two distribution fits in Step 3 described above: generalised logistic (`glogist`) and generalised extreme value (`gev`) distribution, using the `wichita` data from the package `SPEI`."
#| out-height: 30%
#| out-width: 100%
knitr::include_graphics(here::here("figures/toy-example-spei.png"))
```


\newpage

## Pipeline steps for constructing indies

The construction of natural hazard indices also follows a set of steps, which is usually illustrated using a flowchart in the paper. 
However, every researcher follows a certain design philosophy and steps taken in the index constructed by different researchers are not aligned. 
This discourages experiment with multiple indices. Initiate a new workflow when computing a new index.

The most popular indices (i.e. SPI, SPEI, etc) have existing software implementation (`SPEI`) to be applied to a different set of data.

constructing time series index should also be encapsulated in my framework

Here we assume a concept of interest is determined, relevant variables/ indicators are identified and available to construct indices.

```{r}
#| fig-align: center
#| fig-cap: "Diagram of pipeline steps for index construction."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-steps.png"))
```


**Raw data**

*Another section on original data directly downloaded, can have different spatial resolution, temporal granularity, data quality problem. After processing them and align them together they become the "raw data"*

The data used to construct the natural hazard index usually have three dimensions, one for location, one for time, and one for multivariate. 
Mathematically, it can be written as $X_{j, s, t}$, where $j = 1, 2, \cdots, J$ for variable, $s = 1, 2, \cdots, S$ for location, and $t = 1, 2, \cdots, T$ for time.

The location $s$ can refer to vector points or areas characterised by longitude-latitude coordinates, or raster cells obtained from satellite images.

The time dimension $t$ can be daily, weekly, biweekly (14-16 days), monthly, or even quarterly

Variables

This multidimensional array structure is commonly used in geospatial analysis

Given the variety of data sources at different spatial resolution and temporal granularity, the raw data may first come in multiple pieces. 
Sometimes, even a considerable amount of work is needed to align the spatial and temporal extent of multivariate data.

A notation for different variables have different spatial and temporal granularity $X_{j_1, s_1, t_1}$???

**Spatial aggregation**

mostly happen with raster data

**Scaling**

A specific transformation on the scale of the data

z-score standardising, min-max standardisation into [0, 1] or [0, 100], percentage change on the baseline
close to variable transformation step

**Normalising**

The purpose of normalising is for cross-comparison.
This step can get criticism from analysts for ...

specifically for converting from a fitted distribution to normal score via reverse CDF function, non-parametric formula, or empirical approximation, a common step in many index: SPI, SSI, Z score. The purpose of normalising is to convert the index into a standardised series after all the steps for the ease of comparison.

Normalising is usually the last step

**Variable transformation**

<!-- Take one or multiple variables to create a new variable,  -->
Restrict it to single variable, square root, log etc could be linearly, also non-linear

change the shape of the variable

<!-- A conversion of non-normal variables into (approximately) normal. wrong! -->

GAM, can you do additive model pairwise/ three-way

**Temporal processing**

**Dimension reduction**

sometimes called feature extraction in the machine learning community 
With drought indices, the extraction of meaningful variables from the original data is usually supported by the water balance model, for example, in SPEI, the step that create $d$ out of precipitation and potential evapotranspiration (PET) has theoretical backup from [see paper.]

Also include weighting



**Benchmarking**



**Simplification**

Discretise the continuous index into a few labelled categories. For communicating the severity of natural hazard to general public.

uniform workflow to work with index construction.

-   illustration
-   math notation
-   benefit of the pipeline approach
    -   index diagnostic
    -   uncertainty

# Incorporating new buliding blocks into the pipeline {#sec-incorporating-new-buliding-blocks-into-the-pipeline}

<!-- # [Extending the pipeline] a different name here: alternate a module with a different (method) -->

# Examples {#sec-examples}

## Constructing Standardised Precipitation Index (SPI)

-   a basic workflow and congruence with results in the `SPEI` pkg
-   allow multiple distribution fit
-   allow bootstrap uncertainty

## Calculating SPEI with raster data

# Reference
