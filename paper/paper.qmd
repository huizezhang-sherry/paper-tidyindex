---
title: "Demo arXiv template"
format:
  arxiv-pdf:
    keep-tex: true  
pdf-engine: pdflatex
author:
  - name: H. Sherry Zhang
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, VIC
        country: Australia
    orcid: 0000-0002-7122-1463
    email: huize.zhang@monash.edu
    url: https://huizezhangsh.netlify.app/
  - name: Collaborators
    affiliations:
      - name: Monash University
        department: Department of Econometrics and Business Statistics
        address: 29 Ancora Imparo Way, Clayton
        city: Melbourne, VIC
        country: Australia
abstract: |
  - indexes, useful, quantify severity, early monitoring, 
  - A huge number of indexes have been proposed by domain experts, however, a large majority of them are not being adopted, reused, and compared in research or in practice. 
  - One of the reasons for this is the plenty of indexes are quite complex and there is no obvious easy-to-use implementation to apply them to user's data. 
  - The paper describes a general pipeline framework to construct indexes from spatio-temporal data, 
  - This allows all the indexes to be constructed through a uniform data pipeline and different indexes to vary on the details of each step in the data pipeline and their orders. 
  - The pipeline proposed aim to smooth the workflow of index construction through breaking down the complicated steps proposed by various indexes into small building blocks shared by most of the indexes.
  - The framework will be demonstrated with drought indexes as examples, but appliable in general to environmental indexes constructed from multivariate spatio-temporal data
keywords:
  - indexes
  - data pipeline
  - software design
bibliography: bibliography.bib  
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, echo = FALSE)
library(tidyverse)
library(latex2exp)
library(patchwork)
```

# Introduction

<!-- Each individual index follows its own data pipeline and it can be difficult to evaluate how an index can be affected by tweaking parameters in a certain step, rearranging the order of steps, using to different method.  -->

<!-- However, even if most of the indexes have available existing implementation available to users, there would be hundreds of them and it will be burdensome to remember, classify, and compare between. -->


**Why index is useful, why people care about indexes**

*incorporate the following in why using index: multiple pieces of information (variables) that need to be taken into account*

Many concepts relevant to decision making cannot be directly measured, however, they are crucial for resource allocation, early prevention, and other operational purpose. 
For example, fire authorities would be interested to quantify fire risk since bushfires can have a huge impact on monetary loss, health, and the local ecosystem.
Climatologists would be interested in monitoring the change in global climate since variability in atmospheric and oceanic conditions has a direct impact on global weather and climate.
Usually this concept of interest is associated with more than one variables and these variables need to be integrated to make decisions on the subject matter.
<!-- From a modelling perspective, these unobserved quantities are called latent variables and existing methods can incorporate these latent variables in the model structure.  -->
A common approach to quantify concepts like these is to construct an index using these relevant variables. 
This allows researchers to compare the quantity of interest across entities (i.e. countries, regions) and also cross time.

**Define what is an index, what is not**

In this article, an index is defined as a tool to quantify a concept of interest that does not have a direct measure.
The concept of interest doesn't have a direct measure can because it is impractical to measure at the population level. 
For example, it would be nearly impossible to include all the available stocks in the market to characterise stock market behavior, so indexes like Dow Jones Industrial Average, S&P 500, and Nasdaq Composite select a representative set of stocks to measure the overall market behavior. 
Also belonging to this category are the economic indexes like the Consumer Price Index, where price changes of a basket of items are weighted to measure inflation. 
The lack of direct measure could also because the concept itself is an unobservable human construction, rather than a physical quantity that can be measured.
Many natural hazard and social concepts falls into this category. 
This includes drought indexes constructed from meteorological, agricultural, hydrological, and social-economic variables, e.g. Standardised Precipitation Index (SPI) [@mckee1993relationship] and Aggregated Drought Index (ADI) [@keyantash_aggregate_2004] among others.
Social development indexes like Human Development Index [@hdi] and Global Liveability Index [@gli] measure various aspects of the quality of human capital and urban life.

*still need to tweak the tone a bit: "they are called index, they are not the index we will talk about"*

Despite many quantity having the term *index* in their name, they cannot be technically classified as indexes according to the definition given above. 
The reason for these quantities to lose their index memberships is that they are variables can be accurately measured given the instrument precision. 
This includes quantities like precipitation of the driest month or percentage of days when maximum temperature is below 10th percentile. 
They are measures of precipitation and percentage of days under specific conditions (dries month, maximum temperature below 10th percentile).
They are variables, or indicators, that can be used to construct indexes but are not indexes themselves. 
Similarly, a set of remote sensing indexes are not indexes, since they are measures of electromagnetic wave reflectance.
This includes Normalized Difference Vegetation Index (NDVI) [@tucker_red_1979], derived from the ratio of difference over sum on two segments in the spectrum, also called band: near-infrared (NIR) and red.
So are the "indexes" derived from NDVI, e.g. Vegetation Condition Index [@kogan_application_1995]. 
Notice that this does not exclude all the construction derived from remotes sensor variables to be valid indexes.
For example, Vegetation Drought Response Index [@brown_vegetation_2008] is a valid index since it integrates climate, satellite, and biophysical variables to quantify vegetation stress.

<!-- FYI:  -->

<!--   - A measure is a variable, but is not an index.  -->
<!--   - Not all multivariate quantities are indexes (i.e. those remote sensor measures). It is possible that an index is constructed from a single variable (i.e. SPI). -->
<!--   - indexes is not a single/ set of measures -->


**What is the challenges with current index construction**

*see if there is any paper describing this type of pains*

*useful to reference tidy data and tidy model that makes the workflow on modelling tidy somewhere in introduction*

Currently, index construction lacks a standardised workflow. 
It is often up to researchers or research institutions to decide whether to provide open source code on the new indexes, what would be the best user interface for other researchers to use the new indexes, and how easily the new indexes can be compared with other existing indexes. 
This makes the computation lack transparency and indexes cumbersome to experiment with: 
  
  - Researchers who wish to validate the indexes calculated from large institutes need to reinvent the wheels themselves since the source code used for computing is often not available for public consumption;
  - Open-source code provided by research groups has a narrow margin for exploring other options outside the provided;
  - Similar steps used by different indexes are difficult to spot since the design of the user interface for indexes often includes all the steps under a single function call; and
  - It is generally hard to inspect intermediate results during the index construction if users wish to check the output of a certain step. 

<!-- Institutes like National Oceanic and Atmospheric Administration (NOAA) and United States Drought Monitor (USDM) moderate indexes in their domains. -->
<!-- They provide public information to describe the methodology used in the calculation, publish the index values for public consumption while the source code used for computing is often not available for public consumption.  -->


<!-- For example, many indexes involve a step to fit variables into a particular distribution. -->
<!-- Index researchers may provide options for different fits, but it is generally hard to experiment with options outside the provided distributions. -->

<!-- For example, many indexes needs standardisation in its workflow, it can be used to rescale the input variables or the index series. -->
<!-- But the current construction does not make it clear through its user interface but sweep it, along with all the other steps in the index construction, under a single function call.  -->


**what can be done if people adopt this pipeline/ why it is beneficial?**

This paper proposes a data pipeline for index construction. 
By recognising the common steps shared by many indexes, we develop a pipeline that breaks down index construction into multiple modules and allow operations in various modules to be combined like building blocks to construct indexes.
The pipeline approach is general while adaptable to most index construction. 
It allows indexes to be created, studied, and compared in a structured tidy form and enables statistical analysis of indexes to be performed easily: 
More specifically, it enables researchers to 1) validate the indexes calculated from external organisations, 2) unify various indexes under the same framework for computing, 3) swap or adjust individual steps in the index construction to study their contribution, 4) calculate uncertainty on indexes through bootstrap or others, 5) enhance existing indexes through comparing and studying their statistical properties, and finally, 6) propose new indexes from combining different steps in existing indexes. 


**who would benefit from this paper**

This work is of interest to researchers actively developing new indexes since it encourages new indexes to be delivered in an easy-to-reproduce design. 
It would also provide analysts who wish to compute a range of indexes in their analysis a uniform interface to build relevant indexes from raw data. 
For statisticians and software developing engineers, this work frames the process of index construction in a more user-oriented workflow and could motivate similar research for other process in scientific computing.


The rest of the paper is structured as follows: @sec-data-pipeline-in-r reviews the concept of data pipeline in R. 
The pipeline framework for index construction is presented in @sec-a-pipeline-for-building-statistical-indexes.  @sec-incorporating-new-buliding-blocks-into-the-pipeline explains how to include a new building block in each pipeline module. 
Examples are given in @sec-examples to demonstrate the index construction with the pipeline built. 

# Data pipeline {#sec-data-pipeline-in-r}

*Think about if there is another word for data pipeline*

**Why you should care about pipeline**

Data pipeline is not a new concept to computing. It refers to a set of data processing elements connected in series, where the output of one element is the input of the next one.
@wickham_plumbing_2009 argues that whether made explicit or not, the pipeline has to be presented in every graphics program.
The paper also argues that breaking down graphic rendering into steps is beneficial for understanding the implementation and comparing between different graphic systems. 
The discussion on pipeline construction is well documented in early interactive graphics software: @buja_elements_1988, @sutherland_orca_2000, and @xie_reactive_2014 and their pipeline steps include non-linear transformation, variable standardization, randomization and dimension reduction.

**What is pipeline, its underlying software design philosophy, and how these are reflected in R**

One of the most commonly known pipeline examples is perhaps the Unix pipeline where programs can be concatenated with `|` to flow the output from the last program into the next program, i.e. 
    
````
command 1 | command 2 | command 3 | ...
````

To solve a complex problem, the Unix system builds simple programs that do one thing well and work well together.
This design is also reflected in the tidyverse ecosystem in R.
To solve a complicated data problem using tidyverse, analysts typically build the solution using a collection of tools from the tidyverse toolbox. 
The data object can flow smoothly from one command to the next, safeguarded by the tidy data format [@wickham_tidy_2014], which prescribes three rules on how to lay out tabular data.
The tidyverse tools also embrace a strong human-centered design where function names are intuitive and easy to reference through autocomplete.
With the tidyverse design principle in mind, the tidymodel suite enables analysts to build machine learning models through the data pipeline. 
It includes typical tasks required in machine learning like data resampling, feature engineering, model fitting, model tuning, and model evaluation.
An advantage of tidymodel pipeline over separate software for individual models is that analysts no longer need to write model-specific syntax to work with each model, but pipeline-specific syntax that is applicable to all the models implemented in tidymodel.
This allows users to easily experiment with a collection of machine learning models. 

**Constructing indexes would also benefit from pipeline and embracing the aforementioned design philosophy.**

In index construction, data pipeline is often presented in a workflow diagram in the research paper to illustrate how the raw data is transformed into the final indexes. 
This agrees with Wickham's argument on the presence of the data pipeline, however, more often than not, the pipeline is not made explicit in the software.
Often the time, all the steps are lumped into a single wrapper function, rather than being split into smaller, modulated functions. 
This increases the cost of maintaining and understanding the code base, gives analysts little freedom to customise the indexes for specific needs, and hinders reusing existing code for building new indexes.
A pipeline approach unites a range of indexes under a single data pipeline and analysts can compose indexes from pipeline steps like building Legos from individual bricks.
In this workflow, analysts are not limited by indexes that have been already proposed and can easily combine pipeline steps to compose novel indexes.  Analysis of the indexes (i.e. calculation of uncertainty) is also feasible by adding external code into the pipeline.

# A pipeline for building statistical indexes {#sec-a-pipeline-for-building-statistical-indexes}

## How does the pipeline constructin of an index look like? 

Consider a commonly used drought index: Standardized Precipitation-Evapotranspiration Index (SPEI)  [@spei]. Its construction involves: 1) calculating potential evapotranspiration (PET) from average temperature, `tavg`, and its difference, `d`, with precipitation, `prcp`, 2) aggregating difference series with a time scale, 3) fitting the aggregated series with a chosen distribution to get density values, and 4) converting the density values to normal quantiles. Under the pipeline approach, SPEI will be constructed as: 

```{r eval = FALSE, echo = TRUE}
DATA %>% 
  trans_pet(method = thornthwaite, var = tavg) %>% 
  aggregate(scale = 12, var = d) %>% 
  normalise(dist = gamma, fit_method = lmom) %>% 
  augment(var = .agg_prcp)
```

<!-- *FWI:*  -->

<!--   - *verbs as name for each step: `aggregate`, `normalise`* -->
<!--   - *use NSE for variable name: `prcp`, `c(tavg, prcp)`* -->
<!--   - *created variables are prefixed with dot: `.agg_prcp`* -->
<!--   - *avoid string and use function to avoid "gamma" vs. "gam": `gamma()`, `lmom()`* -->

*Change the examples to two scales values, compare how indexes look like*

Here each command corresponds to one step described above with additional arguments specific to the step. 
The pipeline construction produces the same index value as command like `spei(...)`, as shown in @fig-toy-example, but with additional benefit:

  - Multiple scales and multiple distributions can be fitted using the `c(...)` syntax to compare index values constructed from different parameterisations; 
  - Intermediate results can be checked after each step; and
  - Additional steps and analysis can be wired into the workflow for index diagnostics and customised user need.
  
  
  *everyone is providing single command function, this is also the same for spi*
  
  *may not worthwhile if only one index, as long as start changing*
  
  *just want to calculate index, single function is fine*

```{r fig-toy-example, eval = TRUE, echo = FALSE}
#| fig-align: center
#| fig-cap: "Standardised Precipitation Evapotranspiration Index (SPEI) calculated with two distribution fits in Step 3 described above: generalised logistic (`glogist`) and generalised extreme value (`gev`) distribution, using the `wichita` data from the package `SPEI`."
#| out-height: 30%
#| out-width: 100%
knitr::include_graphics(here::here("figures/toy-example-spei.png"))
```


\newpage

## Pipeline steps for constructing indies

<!-- *constructing time series index should also be encapsulated in the framework* -->

An overview of the pipeline is given in @fig-pipeline-steps to illustrate the construction from raw data to the final indexes.
The pipeline includes modules for operations in the spatial, temporal, and multivariate aspects of the data as well as modules for comparing and communicating indexes.
Analysts are free to select the modules they need and arrange them in the order they see fit to construct indexes.
While the starting point of the pipeline is raw data, there are steps prior to this that are crucial to the success of an index. 
For example, the defined index needs to be useful for measuring the concept of interest and variables need to be collected from reliable sources with proper quality control. 

The notation used in this section will be first introduced before elaborating on each pipeline module. Let $\mathbf{x}(\mathbf{s};\mathbf{t})$ denote the raw data with spatial, temporal, and multivariate aspects: the spatial dimension $\mathbf{s} =  (s_1, s_2, \cdots, s_n)^\prime$ is defined in the 2D space: $\mathbf{s} \in \mathcal{D}_s \subseteq \mathbb{R}^2$, the temporal dimension $\mathbf{t} = (t_1, t_2, \cdots, t_J)^\prime$ is defined in the 1D space: $\mathbf{t} \in \mathcal{D}_t \subseteq \mathbb{R}$. When more than one variable is involved, the multivariate data can also be written as: $\mathbf{x}(\mathbf{s}; \mathbf{t}) = (x_1(\mathbf{s}; \mathbf{t}), x_2(\mathbf{s}; \mathbf{t}), \cdots, x_P(\mathbf{s}; \mathbf{t}))^\prime$.

```{r fig-pipeline-steps}
#| fig-align: center
#| fig-cap: "Diagram of pipeline steps for index construction. will need to be updated with better design and the distribution fitting step."
#| out-height: 90%
#| out-width: 100%
knitr::include_graphics(here::here("figures/pipeline-steps.png"))
```

### Temporal processing

The construction of an index sometimes needs to consider information from neighbouring time periods.
The temporal processing is a general operator on the time dimension of the data in the form of  

\begin{equation}
f_{\mathcal{\psi}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where $\psi \in \Psi \subseteq \mathbb{R}^{d_{\psi}}$ is the parameters associated with the temporal operation and $d_{\psi}$ is the number of parameter of $\psi$. 
A typical example of temporal processing is aggregation in the drought index SPI to measure the lack of precipitation for meteorological drought.
In SPI, monthly precipitation is aggregated by a time scale parameter $k$: $x(s_i;t_{j^\prime}) = \sum_{j = j^\prime-k+1}^{j^\prime}x(s_i; t_j),$ where $j^\prime$ is the new time index after the aggregation. In this notation, each spatial location is separately aggregated and precipitation is ummed from $k$ month back, $j^\prime - k + 1$, to the current period, $j^\prime$, to create the aggregated series, indexed by $j^\prime$. 

*more explicit on k will influence 1) long term vs. short term, 2) uncertainty*
The choice of time scales parameter $k$ can result in variation in the calculated index values: a small $k$ of 3 or 6 months produces the index more sensitive to individual months, while a large $k$ of 24 or 36, an equivalent to a 2- or 3-year aggregation, gives dryness information relative to the long term condition. As will be shown in section [SECTION EXAMPLE], this variation may even lead to conflicting conclusions on the dry/wet condition of the area, highlighting the importance to account for index uncertainty when interpreting index values for decision-making.

### Spatial processing

While many indexes are calculated independently on spatial locations, borrowing spatial information can happen at unobservable locations. Also, variables can be collected in different formats: some from satellite imageries as raster data, potentially at different resolutions, and some from ground stations as vector data. These spatial data need to be merged into a uniform unit to construct indexes. 
In the pipeline, spatial aggregation includes operations on the spatial dimension of the data and can be written as 

\begin{equation}
g_{\mathcal{\theta}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where $\theta \in \Theta \subseteq \mathbb{R}^{d_{\theta}}$ is the spatial operation parameters and $d_{\theta}$ is the number of parameter of $\theta$. 
When one of the variables collected is only available in a coarser resolution, $i^\prime$, analysts may choose to reduce other variables with finer resolution, $i$, to match the coarser resolution. This is a spatial aggregation step and aggregating using the mean can be written as: $x(s_{i^\prime};t_{j}) = \bar{x}(s_{i \in i^\prime}; t_j),$, where $i \in i^\prime$ includes all the cells from the finer resolution in the coarser grid. 


### Variable transformation 

Variable transformation, and scaling in the next section, are both pre-processing steps to create variables that fits assumptions for further computing. These include a stable variance, normal distribution, or a certain scale required by some algorithm further down the pipeline.
Variable transformation is a general notion of a functional transformation on the variable: 

\begin{equation}
h_{\tau}(x(\mathbf{s};\mathbf{t})),
\end{equation} 

where $\tau \in T \subseteq \mathbb{R}^{d_{\tau}}$ is the parameter in the transformation if any, and $d_{\tau}$ is the number of parameter of $\tau$. Transformation is needed for data that are highly skewed and some common transformations include log, quadratic, and square root transformation.

### Scaling

While scaling can be seen as a specific type of variable transformation, it is separated into its own step to make the step explicit in the pipeline.
The key difference between the two steps is that variable transformation typically changes the shape of the data while scaling only changes the data scale and can usually be written in the form of 

\begin{equation}
[x(s_i;t_j) - \alpha]/\gamma.
\end{equation}

For example, a z-score standardisation can be written in the above form with $\alpha = \bar{x}(s; t)$ and $\gamma = \sigma(s; t)$, a min-max standardisation uses $\alpha = \min[x(s_i, t_j)]$ and $\gamma = \max[x(s_i, t_j)] - \min[x(s_i, t_j)]$.
@fig-scale-var-trans-compare shows a collection of variable pre-processing operations and uses color to differentiate whether the operation is a variable transformation or a scaling step. All the scaling operations in green result in the same distribution as the original data.

<!-- Take one or multiple variables to create a new variable,  -->
<!-- Restrict it to single variable, square root, log etc could be linearly, also non-linear -->

<!-- change the shape of the variable -->

<!-- <!-- A conversion of non-normal variables into (approximately) normal. wrong! -->

<!-- GAM, can you do additive model pairwise/ three-way -->

<!-- While being a common technique in data pre-processing, scaling has an impact on the outliers of the data and analysts need to be cautious on checking its effect. -->


```{r fig-scale-var-trans-compare}
#| fig-cap: "Comparison of operations in scaling (green) and variable transformation (orange) steps in free scale. Variables after the scaling operations have the same distribution as the origin, while the distribution changes after variable transformation. "
#| fig-height: 8
#| fig-width: 12
set.seed(123)
x <- rgamma(n = 1000, shape = 1, scale = 2)


trans_df <- tibble(origin = x, 
       `z-score` = (x - mean(x)/ sd(x)), 
       qudratic = x^2,
       log = log(x),
       `square root` = sqrt(x),
       `cubic root` = x^(1/3),
       minmax = (x - min(x))/ (max(x) - min(x)),
       boxcox = (x^0.5 - 1)/0.5,
       centering = x - mean(x)
       ) %>% 
  mutate(id = row_number(),
         ) %>% 
  pivot_longer(-id, names_to = "var", values_to = "value") %>% 
  mutate(category = ifelse(
    var %in% c("origin", "z-score", "minmax", "centering"),
    "Scaling", "Var. Trans.")
    ) 

latex_df <- tibble(
  var = c("origin", "z-score", "qudratic", "log", "square root", "cubic root",
          "minmax", "boxcox", "centering"),
  latex = c(
           r"($x$)",
           r"($\frac{x - \bar{x}}{sd(x)}$)",
           r"($x^2$)",
           r"($log(x)$)",
           r"($\sqrt{x}$)",
           r"($x^{1/3}$)",
           r"($\frac{x - min(x)}{max(x) - min(x)}$)",
           r"($\frac{x^{0.5} - 1}{0.5}$)",
           r"($x - \bar{x}$)"
         )
) %>% 
  left_join(trans_df %>% group_by(var) %>% summarise(max = max(value)))

trans_df %>% 
  ggplot() +
  geom_density(aes(x = value, color = category, y = after_stat(scaled))) + 
  geom_label(data = latex_df, 
             aes(x = 0.8 * max, y = 0.8, label = TeX(latex, output = "character")), 
             parse = TRUE) + 
  facet_wrap(vars(var), scales = "free") + 
  scale_color_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(legend.position = "bottom")
```

### Dimension reduction

When constructing indexes from multivariate information, dimension reduction methods combine that information into a univariate series. In the pipeline, dimension reduction includes methods that take multivariate inputs and output the data in a lower dimension (often univariate): 

\begin{equation}
x_{p^*}(\mathbf{s}; \mathbf{t}) \rightarrow x_p(\mathbf{s}; \mathbf{t}),
\end{equation}

where $p^* = 1, 2, \cdots, P^*$ and $p = 1, 2, \cdots, P$ reduce the variable dimension from $P$ to $P^*$. 
The most commonly used dimension reduction technique is Principal Component Analysis (PCA), also called Empirical Orthogonal Function (EOF) in earth science. It can be written as $x_{p^*}(\mathbf{s}; \mathbf{t}) = \sum_{p = 1}^{P}\lambda_{p}x_p(\mathbf{s};\mathbf{t})$ where $\lambda_p$ is the loading of the PC1, derived from maximising the data variance given the constraint $\sum_{p=1}^P\lambda_p^2 = 1$. 

*add another perspective that there are statistical methods, i.e. PCA, and also domain knowledge driven methods, i.e. d = P - PET in SPEI.*

The PCA is a special case of weighting, where individual variables are summed up in a linear combination with potential restrictions imposed on the weight coefficients. These coefficients may be simple mathematical constructions that give equal weight to each variable or can be derived with specialised knowledge by experts in the field. 
While suggested weights can indicate norms adored by practitioners, analysts should be given the flexibility to experiment with different combinations when constructing indexes. 
This could help understand index behavior from its sensitivity to the variables and suggest alternative weights that better suit specific tasks.

<!-- sometimes called feature extraction in the machine learning community  -->
<!-- With drought indexes, the extraction of meaningful variables from the original data is usually supported by the water balance model, for example, in SPEI, the step that create $d$ out of precipitation and potential evapotranspiration (PET) has theoretical backup from [see paper.] -->

### Distribution fit

*model fit? *

<!-- With a probability model imposed,  -->
Distribution fit can be seen as the model fitting in its simplest term. It can be represented by 

\begin{equation}
F_{\eta}(x(\mathbf{s}; \mathbf{t})), 
\end{equation}

where $\eta \in H \subseteq \mathbb{R}^{d_{\eta}}$ is the distribution parameter and $d_{\eta}$ is the number of parameter of $\eta$. A distribution fit typically aims at finding the distribution that best fits the data. 
Analysts may start from a pool of candidate distributions with a chosen fitting method and goodness of fit measure. 
While it is useful to find the ultimate best distribution to fits the data, from a probabilistic perspective, the fitting procedure itself has an uncertainty associated with the data fed and the parameter chosen. A reasonable alternative is to understand how much the index values can vary given different distributions, fitting methods, and goodness of fit tests, and whether these variations are negligible in a given application.

### Normalising

This step maps the univariate series into a different scale, typically for ease of comparison across regions. 
For example, a normal scale, [0, 1], or [0, 100] may be favored for reporting certain indexes. 
In drought indexes, i.e. SPI or SPEI, the quantiles from the fitted distribution are converted into the normal scale via the normal reverse CDF function: $\Phi^{-1}(.)$. 
Normalising is usually used at the end of the pipeline and its main difference from the scaling step is that here the change of scale also changes the distribution of the variable. 
While being commonly used, this step can get criticism from analysts for forcing the data into the decided scale, which can be either unnecessary or inaccurately exaggerate or downplay the outliers. 
Also, the use of a normal scale needs to be interpreted with caution. 
@fig-normalising illustrates the normal density not being directly proportional to its probability of occurrence.
This is concerning, especially at the extreme values, since a small difference in the tail density can have magnitudes of difference in its probability of occurrence.

```{r fig-normalising}
#| fig-cap: "Scatterplot of normal quantiles against their density values. THree tail density values are highlighted with its probability of occurence labelled. Probability is calculated assuming monthly data: with a density of -2, the probability of occurrence is 1/pnorm(-2)/12 = 4 years. The non-linear relationship between the two quantities suggests normalised indexes need to be interpreted with caution since a slight change in the tail distribution can result in magnitudes of difference in its probability of occurrence."
#| fig-width: 9
library(tidyverse)
all <- tibble(x = seq(-3, 3, 0.01), y = pnorm(x)) 
dt <- tibble(x = c(-3, -2.5, -2), y = pnorm(x)) 
label <- tibble(x = c(-3, -2.5, -2), y = pnorm(x), 
                label = c("once in 62 years", "once in 13 years", "once in 4 years"))

dt %>% 
  ggplot() + 
  geom_point(aes(x= x, y = y)) + 
  geom_line(data = all, aes(x = x, y = y)) + 
  ggrepel::geom_label_repel(
    data = label, aes(x = x, y = y, label =label),
    nudge_y = 0.2, nudge_x = 0.1,
    arrow = arrow(length = unit(0.08, "inches")), min.segment.length = 0) +
  scale_x_continuous(breaks = seq(-3, 3, 1)) + 
  xlab("Density") + 
  ylab("Quantile") + 
  theme_bw() + 
  theme(panel.grid.minor = element_blank())
```

### Benchmarking

Benchmarking sets a constant value to allow the constructed index to be compared across time.
Here we denote it with $u[x(s_i, t_j)]$ where $u$ is a scalar of interest in the index constructed. A benchmark value could be a constant or a function of the data, i.e. mean.

### Simplification

In public communication, the index values are usually accompanied by a categorical grade. The categorised grades are an ordered set of descriptive words or colors to communicate the severity or guide the comprehension of the indexes. 
The mapping from continuous index values to the discrete grades is called simplification in the pipeline and it can be written as a piece-wise function: 

\begin{equation}
\begin{cases}
C_0 & c_1 \leq (s_i; t_j) < c_0 \\
C_1 & c_2 \leq x(s_i; t_j) < c_1 \\
C_2 & c_3 \leq x(s_i; t_j) < c_2 \\
\cdots \\
C_z & c_z \leq x(s_i; t_j)
\end{cases}
\end{equation}

where $C_0, C_1,\cdots ,C_z$ are the categories and $c_0, c_1, \cdots, c_z$ are the thresholds for each category. In SPI, droughts are sorted into four categories: mild drought: $[-0.99, 0]$; moderate drought: $[-1.49, -1]$; severe drought: $[-1.99, -1.5]$, and extreme drought: $[-\infty, -2]$. In this case, $C_0, C_1, C_2, C_3$ are the drought categories: mild, moderate, severe, and extreme drought ($z = 3$) and $c_0 =0, c_1 = -1, c_2 = -1.5, c_3 = -2$ are the cutoff value for each class.

# Incorporating new buliding blocks into the pipeline {#sec-incorporating-new-buliding-blocks-into-the-pipeline}

<!-- # [Extending the pipeline] a different name here: alternate a module with a different (method) -->

# Examples {#sec-examples}

## Constructing Standardised Precipitation Index (SPI)

-   a basic workflow and congruence with results in the `SPEI` pkg
-   allow multiple distribution fit
-   allow bootstrap uncertainty

## Calculating SPEI with raster data

# Reference
