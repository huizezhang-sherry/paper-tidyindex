% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmathfont[]{Latin Modern Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{arxiv}
\usepackage{orcidlink}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Demo arXiv template},
  pdfauthor={H. Sherry Zhang; Collaborators},
  pdfkeywords={indexes, data pipeline, software design},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Demo arXiv template}
\author{
\textbf{H. Sherry Zhang}~\orcidlink{0000-0002-7122-1463}\\Department of
Econometrics and Business Statistics\\Monash University\\Melbourne,
VIC\\\href{mailto:huize.zhang@monash.edu}{huize.zhang@monash.edu}\\\\\\
\textbf{Collaborators}\\Department of Econometrics and Business
Statistics\\Monash University\\Melbourne, VIC\\}
\date{}
\begin{document}
\maketitle
\begin{abstract}
\begin{itemize}
\tightlist
\item
  indexes, useful, quantify severity, early monitoring,
\item
  A huge number of indexes have been proposed by domain experts,
  however, a large majority of them are not being adopted, reused, and
  compared in research or in practice.
\item
  One of the reasons for this is the plenty of indexes are quite complex
  and there is no obvious easy-to-use implementation to apply them to
  user's data.
\item
  The paper describes a general pipeline framework to construct indexes
  from spatio-temporal data,
\item
  This allows all the indexes to be constructed through a uniform data
  pipeline and different indexes to vary on the details of each step in
  the data pipeline and their orders.
\item
  The pipeline proposed aim to smooth the workflow of index construction
  through breaking down the complicated steps proposed by various
  indexes into small building blocks shared by most of the indexes.
\item
  The framework will be demonstrated with drought indexes as examples,
  but appliable in general to environmental indexes constructed from
  multivariate spatio-temporal data
\end{itemize}
\end{abstract}
{\bfseries \emph Keywords}
\def\sep{\textbullet\ }
indexes \sep data pipeline \sep 
software design

\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, enhanced, frame hidden, breakable, sharp corners, interior hidden]}{\end{tcolorbox}}\fi

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\textbf{Why index is useful, why people care about indexes}

\emph{incorporate the following in why using index: multiple pieces of
information (variables) that need to be taken into account}

Many concepts relevant to decision making cannot be directly measured,
however, they are crucial for resource allocation, early prevention, and
other operational purpose. For example, fire authorities would be
interested to quantify fire risk since bushfires can have a huge impact
on monetary loss, health, and the local ecosystem. Climatologists would
be interested in monitoring the change in global climate since
variability in atmospheric and oceanic conditions has a direct impact on
global weather and climate. Usually this concept of interest is
associated with more than one variables and these variables need to be
integrated to make decisions on the subject matter. A common approach to
quantify concepts like these is to construct an index using these
relevant variables. This allows researchers to compare the quantity of
interest across entities (i.e.~countries, regions) and also cross time.

\textbf{Define what is an index, what is not}

In this article, an index is defined as a tool to quantify a concept of
interest that does not have a direct measure. The concept of interest
doesn't have a direct measure can because it is impractical to measure
at the population level. For example, it would be nearly impossible to
include all the available stocks in the market to characterise stock
market behavior, so indexes like Dow Jones Industrial Average, S\&P 500,
and Nasdaq Composite select a representative set of stocks to measure
the overall market behavior. Also belonging to this category are the
economic indexes like the Consumer Price Index, where price changes of a
basket of items are weighted to measure inflation. The lack of direct
measure could also because the concept itself is an unobservable human
construction, rather than a physical quantity that can be measured. Many
natural hazard and social concepts falls into this category. This
includes drought indexes constructed from meteorological, agricultural,
hydrological, and social-economic variables, e.g.~Standardised
Precipitation Index (SPI) (McKee et al. 1993) and Aggregated Drought
Index (ADI) (Keyantash and Dracup 2004) among others. Social development
indexes like Human Development Index (United Nations Development
Programme 2022) and Global Liveability Index (Economist Intelligence
Unit 2019) measure various aspects of the quality of human capital and
urban life.

\emph{still need to tweak the tone a bit: ``they are called index, they
are not the index we will talk about''}

Despite many quantity having the term \emph{index} in their name, they
cannot be technically classified as indexes according to the definition
given above. The reason for these quantities to lose their index
memberships is that they are variables can be accurately measured given
the instrument precision. This includes quantities like precipitation of
the driest month or percentage of days when maximum temperature is below
10th percentile. They are measures of precipitation and percentage of
days under specific conditions (dries month, maximum temperature below
10th percentile). They are variables, or indicators, that can be used to
construct indexes but are not indexes themselves. Similarly, a set of
remote sensing indexes are not indexes, since they are measures of
electromagnetic wave reflectance. This includes Normalized Difference
Vegetation Index (NDVI) (Tucker 1979), derived from the ratio of
difference over sum on two segments in the spectrum, also called band:
near-infrared (NIR) and red. So are the ``indexes'' derived from NDVI,
e.g.~Vegetation Condition Index (Kogan 1995). Notice that this does not
exclude all the construction derived from remotes sensor variables to be
valid indexes. For example, Vegetation Drought Response Index (Brown et
al. 2008) is a valid index since it integrates climate, satellite, and
biophysical variables to quantify vegetation stress.

\textbf{What is the challenges with current index construction}

\emph{see if there is any paper describing this type of pains}

\emph{useful to reference tidy data and tidy model that makes the
workflow on modelling tidy somewhere in introduction}

Currently, index construction lacks a standardised workflow. It is often
up to researchers or research institutions to decide whether to provide
open source code on the new indexes, what would be the best user
interface for other researchers to use the new indexes, and how easily
the new indexes can be compared with other existing indexes. This makes
the computation lack transparency and indexes cumbersome to experiment
with:

\begin{itemize}
\tightlist
\item
  Researchers who wish to validate the indexes calculated from large
  institutes need to reinvent the wheels themselves since the source
  code used for computing is often not available for public consumption;
\item
  Open-source code provided by research groups has a narrow margin for
  exploring other options outside the provided;
\item
  Similar steps used by different indexes are difficult to spot since
  the design of the user interface for indexes often includes all the
  steps under a single function call; and
\item
  It is generally hard to inspect intermediate results during the index
  construction if users wish to check the output of a certain step.
\end{itemize}

\textbf{what can be done if people adopt this pipeline/ why it is
beneficial?}

This paper proposes a data pipeline for index construction. By
recognising the common steps shared by many indexes, we develop a
pipeline that breaks down index construction into multiple modules and
allow operations in various modules to be combined like building blocks
to construct indexes. The pipeline approach is general while adaptable
to most index construction. It allows indexes to be created, studied,
and compared in a structured tidy form and enables statistical analysis
of indexes to be performed easily: More specifically, it enables
researchers to 1) validate the indexes calculated from external
organisations, 2) unify various indexes under the same framework for
computing, 3) swap or adjust individual steps in the index construction
to study their contribution, 4) calculate uncertainty on indexes through
bootstrap or others, 5) enhance existing indexes through comparing and
studying their statistical properties, and finally, 6) propose new
indexes from combining different steps in existing indexes.

\textbf{who would benefit from this paper}

This work is of interest to researchers actively developing new indexes
since it encourages new indexes to be delivered in an easy-to-reproduce
design. It would also provide analysts who wish to compute a range of
indexes in their analysis a uniform interface to build relevant indexes
from raw data. For statisticians and software developing engineers, this
work frames the process of index construction in a more user-oriented
workflow and could motivate similar research for other process in
scientific computing.

The rest of the paper is structured as follows:
Section~\ref{sec-data-pipeline-in-r} reviews the concept of data
pipeline in R. The pipeline framework for index construction is
presented in
Section~\ref{sec-a-pipeline-for-building-statistical-indexes}.
Section~\ref{sec-incorporating-new-buliding-blocks-into-the-pipeline}
explains how to include a new building block in each pipeline module.
Examples are given in Section~\ref{sec-examples} to demonstrate the
index construction with the pipeline built.

\hypertarget{sec-data-pipeline-in-r}{%
\section{Data pipeline}\label{sec-data-pipeline-in-r}}

\emph{Think about if there is another word for data pipeline}

\textbf{Why you should care about pipeline}

Data pipeline is not a new concept to computing. It refers to a set of
data processing elements connected in series, where the output of one
element is the input of the next one. Wickham et al. (2009) argues that
whether made explicit or not, the pipeline has to be presented in every
graphics program. The paper also argues that breaking down graphic
rendering into steps is beneficial for understanding the implementation
and comparing between different graphic systems. The discussion on
pipeline construction is well documented in early interactive graphics
software: Buja et al. (1988), Sutherland et al. (2000), and Xie,
Hofmann, and Cheng (2014) and their pipeline steps include non-linear
transformation, variable standardization, randomization and dimension
reduction.

\textbf{What is pipeline, its underlying software design philosophy, and
how these are reflected in R}

One of the most commonly known pipeline examples is perhaps the Unix
pipeline where programs can be concatenated with \texttt{\textbar{}} to
flow the output from the last program into the next program, i.e.~

\begin{verbatim}
command 1 | command 2 | command 3 | ...
\end{verbatim}

To solve a complex problem, the Unix system builds simple programs that
do one thing well and work well together. This design is also reflected
in the tidyverse ecosystem in R. To solve a complicated data problem
using tidyverse, analysts typically build the solution using a
collection of tools from the tidyverse toolbox. The data object can flow
smoothly from one command to the next, safeguarded by the tidy data
format (Wickham 2014), which prescribes three rules on how to lay out
tabular data. The tidyverse tools also embrace a strong human-centered
design where function names are intuitive and easy to reference through
autocomplete. With the tidyverse design principle in mind, the tidymodel
suite enables analysts to build machine learning models through the data
pipeline. It includes typical tasks required in machine learning like
data resampling, feature engineering, model fitting, model tuning, and
model evaluation. An advantage of tidymodel pipeline over separate
software for individual models is that analysts no longer need to write
model-specific syntax to work with each model, but pipeline-specific
syntax that is applicable to all the models implemented in tidymodel.
This allows users to easily experiment with a collection of machine
learning models.

\textbf{Constructing indexes would also benefit from pipeline and
embracing the aforementioned design philosophy.}

In index construction, data pipeline is often presented in a workflow
diagram in the research paper to illustrate how the raw data is
transformed into the final indexes. This agrees with Wickham's argument
on the presence of the data pipeline, however, more often than not, the
pipeline is not made explicit in the software. Often the time, all the
steps are lumped into a single wrapper function, rather than being split
into smaller, modulated functions. This increases the cost of
maintaining and understanding the code base, gives analysts little
freedom to customise the indexes for specific needs, and hinders reusing
existing code for building new indexes. A pipeline approach unites a
range of indexes under a single data pipeline and analysts can compose
indexes from pipeline steps like building Legos from individual bricks.
In this workflow, analysts are not limited by indexes that have been
already proposed and can easily combine pipeline steps to compose novel
indexes. Analysis of the indexes (i.e.~calculation of uncertainty) is
also feasible by adding external code into the pipeline.

\hypertarget{sec-a-pipeline-for-building-statistical-indexes}{%
\section{A pipeline for building statistical
indexes}\label{sec-a-pipeline-for-building-statistical-indexes}}

\hypertarget{how-does-the-pipeline-constructin-of-an-index-look-like}{%
\subsection{How does the pipeline constructin of an index look
like?}\label{how-does-the-pipeline-constructin-of-an-index-look-like}}

Consider a commonly used drought index: Standardized
Precipitation-Evapotranspiration Index (SPEI) (Vicente-Serrano,
Beguería, and López-Moreno 2010). Its construction involves: 1)
calculating potential evapotranspiration (PET) from average temperature,
\texttt{tavg}, and its difference, \texttt{d}, with precipitation,
\texttt{prcp}, 2) aggregating difference series with a time scale, 3)
fitting the aggregated series with a chosen distribution to get density
values, and 4) converting the density values to normal quantiles. Under
the pipeline approach, SPEI will be constructed as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DATA }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{trans\_pet}\NormalTok{(}\AttributeTok{method =}\NormalTok{ thornthwaite, }\AttributeTok{var =}\NormalTok{ tavg) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{aggregate}\NormalTok{(}\AttributeTok{scale =} \DecValTok{12}\NormalTok{, }\AttributeTok{var =}\NormalTok{ d) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{normalise}\NormalTok{(}\AttributeTok{dist =}\NormalTok{ gamma, }\AttributeTok{fit\_method =}\NormalTok{ lmom) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{(}\AttributeTok{var =}\NormalTok{ .agg\_prcp)}
\end{Highlighting}
\end{Shaded}

\emph{Change the examples to two scales values, compare how indexes look
like}

Here each command corresponds to one step described above with
additional arguments specific to the step. The pipeline construction
produces the same index value as command like \texttt{spei(...)}, as
shown in Figure~\ref{fig-toy-example}, but with additional benefit:

\begin{itemize}
\tightlist
\item
  Multiple scales and multiple distributions can be fitted using the
  \texttt{c(...)} syntax to compare index values constructed from
  different parameterisations;
\item
  Intermediate results can be checked after each step; and
\item
  Additional steps and analysis can be wired into the workflow for index
  diagnostics and customised user need.
\end{itemize}

\emph{everyone is providing single command function, this is also the
same for spi}

\emph{may not worthwhile if only one index, as long as start changing}

\emph{just want to calculate index, single function is fine}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=0.3\textheight]{../figures/toy-example-spei.png}

}

\caption{\label{fig-toy-example}Standardised Precipitation
Evapotranspiration Index (SPEI) calculated with two distribution fits in
Step 3 described above: generalised logistic (\texttt{glogist}) and
generalised extreme value (\texttt{gev}) distribution, using the
\texttt{wichita} data from the package \texttt{SPEI}.}

\end{figure}

\newpage

\hypertarget{pipeline-steps-for-constructing-indies}{%
\subsection{Pipeline steps for constructing
indies}\label{pipeline-steps-for-constructing-indies}}

An overview of the pipeline is given in Figure~\ref{fig-pipeline-steps}
to illustrate the construction from raw data to the final indexes. The
pipeline includes modules for operations in the spatial, temporal, and
multivariate aspects of the data as well as modules for comparing and
communicating indexes. Analysts are free to select the modules they need
and arrange them in the order they see fit to construct indexes. While
the starting point of the pipeline is raw data, there are steps prior to
this that are crucial to the success of an index. For example, the
defined index needs to be useful for measuring the concept of interest
and variables need to be collected from reliable sources with proper
quality control.

The notation used in this section will be first introduced before
elaborating on each pipeline module. Let
\(\mathbf{x}(\mathbf{s};\mathbf{t})\) denote the raw data with spatial,
temporal, and multivariate aspects: the spatial dimension
\(\mathbf{s} = (s_1, s_2, \cdots, s_n)^\prime\) is defined in the 2D
space: \(\mathbf{s} \in \mathcal{D}_s \subseteq \mathbb{R}^2\), the
temporal dimension \(\mathbf{t} = (t_1, t_2, \cdots, t_J)^\prime\) is
defined in the 1D space:
\(\mathbf{t} \in \mathcal{D}_t \subseteq \mathbb{R}\). When more than
one variable is involved, the multivariate data can also be written as:
\(\mathbf{x}(\mathbf{s}; \mathbf{t}) = (x_1(\mathbf{s}; \mathbf{t}), x_2(\mathbf{s}; \mathbf{t}), \cdots, x_P(\mathbf{s}; \mathbf{t}))^\prime\).

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=0.9\textheight]{../figures/pipeline-steps.png}

}

\caption{\label{fig-pipeline-steps}Diagram of pipeline steps for index
construction. will need to be updated with better design and the
distribution fitting step.}

\end{figure}

\hypertarget{temporal-processing}{%
\subsubsection{Temporal processing}\label{temporal-processing}}

The construction of an index sometimes needs to consider information
from neighbouring time periods. The temporal processing is a general
operator on the time dimension of the data in the form of

\begin{equation}
f_{\mathcal{\psi}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where \(\psi \in \Psi \subseteq \mathbb{R}^{d_{\psi}}\) is the
parameters associated with the temporal operation and \(d_{\psi}\) is
the number of parameter of \(\psi\). A typical example of temporal
processing is aggregation, which is used in the drought index SPI to
measure the lack of precipitation for meteorological drought. In SPI,
monthly precipitation is aggregated by a time scale parameter \(k\):
\(x(s_i;t_{j^\prime}) = \sum_{j = j^\prime-k+1}^{j^\prime}x(s_i; t_j),\)
where \(j^\prime\) is the new time index after the aggregation. In this
notation, each spatial location is separately aggregated and
precipitation is summed from \(k\) month back, \(j^\prime - k + 1\), to
the current period, \(j^\prime\), to create the aggregated series,
indexed by \(j^\prime\).

\emph{more explicit on k will influence 1) long term vs.~short term, 2)
uncertainty}

The choice of time scales parameter \(k\) can result in variation in the
calculated index values: a small \(k\) of 3 or 6 months produces the
index more sensitive to individual months, while a large \(k\) of 24 or
36, an equivalent to a 2- or 3-year aggregation, gives dryness
information relative to the long term condition. As will be shown in
section {[}SECTION EXAMPLE{]}, this variation may even lead to
conflicting conclusions on the dry/wet condition of the area,
highlighting the importance to account for index uncertainty when
interpreting index values for decision-making.

\hypertarget{spatial-processing}{%
\subsubsection{Spatial processing}\label{spatial-processing}}

Spatial processing may be needed when indexes are not calculated
independently on each collected location or when variables collected
from multiple sources need to be fused before further processing. The
process can be written as a general operation in the form of

\begin{equation}
x(\mathbf{s}^\prime;\mathbf{t}) = g_{\mathcal{\theta}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where \(\theta \in \Theta \subseteq \mathbb{R}^{d_{\theta}}\) is the
associated parameters in the process and \(d_{\theta}\) is the number of
parameter of \(\theta\). An example of spatial processing is to align
variables collected in different resolutions. When variables are
collected at different resolutions, analysts may choose to down-sample
those in a finer resolution, \(i\), to match those in a coarser
resolution, \(i^\prime\). This is a spatial aggregation and if aggregate
using the mean, it can be written as

\begin{equation}
g(x) = \frac{\sum_{i \in i^\prime}x}{n_{i^\prime}},
\end{equation}

where \(i \in i^\prime\) includes all the cells from the finer
resolution in the coarser grid and \(n_{i^\prime}\) is the number of
observations falls into the coarser grid. Other examples of spatial
processing include 1) borrowing information from neighbouring spatial
locations to interpolate unobserved locations and 2) fusing variables
from ground measures with satellite imageries.

\hypertarget{variable-transformation}{%
\subsubsection{Variable transformation}\label{variable-transformation}}

Variable transformation, and scaling in the next section, are both
pre-processing steps to create variables that fits assumptions for
further computing. These include a stable variance, normal distribution,
or a certain scale required by some algorithm further down the pipeline.
Variable transformation is a general notion of a functional
transformation on the variable:

\begin{equation}
h_{\tau}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where \(\tau \in T \subseteq \mathbb{R}^{d_{\tau}}\) is the parameter in
the transformation if any, and \(d_{\tau}\) is the number of parameter
of \(\tau\). Transformation is needed for data that are highly skewed
and some common transformations include log, quadratic, and square root
transformation.

\hypertarget{scaling}{%
\subsubsection{Scaling}\label{scaling}}

While scaling can be seen as a specific type of variable transformation,
it is separated into its own step to make the step explicit in the
pipeline. The key difference between the two steps is that variable
transformation typically changes the shape of the data while scaling
only changes the data scale and can usually be written in the form of

\begin{equation}
[x(s_i;t_j) - \alpha]/\gamma.
\end{equation}

For example, a z-score standardisation can be written in the above form
with \(\alpha = \bar{x}(s; t)\) and \(\gamma = \sigma(s; t)\), a min-max
standardisation uses \(\alpha = \min[x(s_i, t_j)]\) and
\(\gamma = \max[x(s_i, t_j)] - \min[x(s_i, t_j)]\).
Figure~\ref{fig-scale-var-trans-compare} shows a collection of variable
pre-processing operations and uses color to differentiate whether the
operation is a variable transformation or a scaling step. All the
scaling operations in green result in the same distribution as the
original data.

\begin{figure}

{\centering \includegraphics{paper_files/figure-pdf/fig-scale-var-trans-compare-1.pdf}

}

\caption{\label{fig-scale-var-trans-compare}Comparison of operations in
scaling (green) and variable transformation (orange) steps in free
scale. Variables after the scaling operations have the same distribution
as the origin, while the distribution changes after variable
transformation.}

\end{figure}

\hypertarget{dimension-reduction}{%
\subsubsection{Dimension reduction}\label{dimension-reduction}}

When constructing indexes from multivariate information, dimension
reduction methods combine that information into a univariate series. In
the pipeline, dimension reduction includes methods that take
multivariate inputs and output the data in a lower dimension (often
univariate):

\begin{equation}
x_{p^*}(\mathbf{s}; \mathbf{t}) \rightarrow x_p(\mathbf{s}; \mathbf{t}),
\end{equation}

where \(p^* = 1, 2, \cdots, P^*\) and \(p = 1, 2, \cdots, P\) reduce the
variable dimension from \(P\) to \(P^*\). The most commonly used
dimension reduction technique is Principal Component Analysis (PCA),
also called Empirical Orthogonal Function (EOF) in earth science. It can
be written as
\(x_{p^*}(\mathbf{s}; \mathbf{t}) = \sum_{p = 1}^{P}\lambda_{p}x_p(\mathbf{s};\mathbf{t})\)
where \(\lambda_p\) is the loading of the PC1, derived from maximising
the data variance given the constraint \(\sum_{p=1}^P\lambda_p^2 = 1\).

\emph{add another perspective that there are statistical methods,
i.e.~PCA, and also domain knowledge driven methods, i.e.~d = P - PET in
SPEI.}

The PCA is a special case of weighting, where individual variables are
summed up in a linear combination with potential restrictions imposed on
the weight coefficients. These coefficients may be simple mathematical
constructions that give equal weight to each variable or can be derived
with specialised knowledge by experts in the field. While suggested
weights can indicate norms adored by practitioners, analysts should be
given the flexibility to experiment with different combinations when
constructing indexes. This could help understand index behavior from its
sensitivity to the variables and suggest alternative weights that better
suit specific tasks.

\hypertarget{distribution-fit}{%
\subsubsection{Distribution fit}\label{distribution-fit}}

\emph{model fit? }

Distribution fit can be seen as the model fitting in its simplest term.
It can be represented by

\begin{equation}
F_{\eta}(x(\mathbf{s}; \mathbf{t})), 
\end{equation}

where \(\eta \in H \subseteq \mathbb{R}^{d_{\eta}}\) is the distribution
parameter and \(d_{\eta}\) is the number of parameter of \(\eta\). A
distribution fit typically aims at finding the distribution that best
fits the data. Analysts may start from a pool of candidate distributions
with a chosen fitting method and goodness of fit measure. While it is
useful to find the ultimate best distribution to fits the data, from a
probabilistic perspective, the fitting procedure itself has an
uncertainty associated with the data fed and the parameter chosen. A
reasonable alternative is to understand how much the index values can
vary given different distributions, fitting methods, and goodness of fit
tests, and whether these variations are negligible in a given
application.

\hypertarget{normalising}{%
\subsubsection{Normalising}\label{normalising}}

This step maps the univariate series into a different scale, typically
for ease of comparison across regions. For example, a normal scale,
{[}0, 1{]}, or {[}0, 100{]} may be favored for reporting certain
indexes. In drought indexes, i.e.~SPI or SPEI, the quantiles from the
fitted distribution are converted into the normal scale via the normal
reverse CDF function: \(\Phi^{-1}(.)\). Normalising is usually used at
the end of the pipeline and its main difference from the scaling step is
that here the change of scale also changes the distribution of the
variable. While being commonly used, this step can get criticism from
analysts for forcing the data into the decided scale, which can be
either unnecessary or inaccurately exaggerate or downplay the outliers.
Also, the use of a normal scale needs to be interpreted with caution.
Figure~\ref{fig-normalising} illustrates the normal density not being
directly proportional to its probability of occurrence. This is
concerning, especially at the extreme values, since a small difference
in the tail density can have magnitudes of difference in its probability
of occurrence.

\begin{figure}

{\centering \includegraphics{paper_files/figure-pdf/fig-normalising-1.pdf}

}

\caption{\label{fig-normalising}Scatterplot of normal quantiles against
their density values. THree tail density values are highlighted with its
probability of occurence labelled. Probability is calculated assuming
monthly data: with a density of -2, the probability of occurrence is
1/pnorm(-2)/12 = 4 years. The non-linear relationship between the two
quantities suggests normalised indexes need to be interpreted with
caution since a slight change in the tail distribution can result in
magnitudes of difference in its probability of occurrence.}

\end{figure}

\hypertarget{benchmarking}{%
\subsubsection{Benchmarking}\label{benchmarking}}

Benchmarking sets a constant value to allow the constructed index to be
compared across time. Here we denote it with \(u[x(s_i, t_j)]\) where
\(u\) is a scalar of interest in the index constructed. A benchmark
value could be a constant or a function of the data, i.e.~mean.

\hypertarget{simplification}{%
\subsubsection{Simplification}\label{simplification}}

In public communication, the index values are usually accompanied by a
categorical grade. The categorised grades are an ordered set of
descriptive words or colors to communicate the severity or guide the
comprehension of the indexes. The mapping from continuous index values
to the discrete grades is called simplification in the pipeline and it
can be written as a piece-wise function:

\begin{equation}
\begin{cases}
C_0 & c_1 \leq (s_i; t_j) < c_0 \\
C_1 & c_2 \leq x(s_i; t_j) < c_1 \\
C_2 & c_3 \leq x(s_i; t_j) < c_2 \\
\cdots \\
C_z & c_z \leq x(s_i; t_j)
\end{cases}
\end{equation}

where \(C_0, C_1,\cdots ,C_z\) are the categories and
\(c_0, c_1, \cdots, c_z\) are the thresholds for each category. In SPI,
droughts are sorted into four categories: mild drought: \([-0.99, 0]\);
moderate drought: \([-1.49, -1]\); severe drought: \([-1.99, -1.5]\),
and extreme drought: \([-\infty, -2]\). In this case,
\(C_0, C_1, C_2, C_3\) are the drought categories: mild, moderate,
severe, and extreme drought (\(z = 3\)) and
\(c_0 =0, c_1 = -1, c_2 = -1.5, c_3 = -2\) are the cutoff value for each
class.

\hypertarget{sec-incorporating-new-buliding-blocks-into-the-pipeline}{%
\section{Incorporating new buliding blocks into the
pipeline}\label{sec-incorporating-new-buliding-blocks-into-the-pipeline}}

\hypertarget{sec-examples}{%
\section{Examples}\label{sec-examples}}

\hypertarget{constructing-standardised-precipitation-index-spi}{%
\subsection{Constructing Standardised Precipitation Index
(SPI)}\label{constructing-standardised-precipitation-index-spi}}

\begin{itemize}
\tightlist
\item
  a basic workflow and congruence with results in the \texttt{SPEI} pkg
\item
  allow multiple distribution fit
\item
  allow bootstrap uncertainty
\end{itemize}

\hypertarget{calculating-spei-with-raster-data}{%
\subsection{Calculating SPEI with raster
data}\label{calculating-spei-with-raster-data}}

\hypertarget{reference}{%
\section*{Reference}\label{reference}}
\addcontentsline{toc}{section}{Reference}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-brown_vegetation_2008}{}}%
Brown, Jesslyn F., Brian D. Wardlow, Tsegaye Tadesse, Michael J. Hayes,
and Bradley C. Reed. 2008. {``The {Vegetation} {Drought} {Response}
{Index} ({VegDRI}): {A} {New} {Integrated} {Approach} for {Monitoring}
{Drought} {Stress} in {Vegetation}.''} \emph{GIScience \& Remote
Sensing} 45 (1): 16--46.
\url{https://doi.org/10.2747/1548-1603.45.1.16}.

\leavevmode\vadjust pre{\hypertarget{ref-buja_elements_1988}{}}%
Buja, A, D Asimov, C Hurley, and JA McDonald. 1988. {``Elements of a
Viewing Pipeline for Data Analysis.''} In \emph{Dynamic Graphics for
Statistics}, 277--308. Wadsworth, Belmont.

\leavevmode\vadjust pre{\hypertarget{ref-gli}{}}%
Economist Intelligence Unit. 2019. {``The Global Liveability Index
2019.''} The Economist.
\url{https://www.cbeinternational.ca/pdf/Liveability-Free-report-2019.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-keyantash_aggregate_2004}{}}%
Keyantash, John A., and John A. Dracup. 2004. {``An Aggregate Drought
Index: {Assessing} Drought Severity Based on Fluctuations in the
Hydrologic Cycle and Surface Water Storage.''} \emph{Water Resources
Research} 40 (9). \url{https://doi.org/10.1029/2003WR002610}.

\leavevmode\vadjust pre{\hypertarget{ref-kogan_application_1995}{}}%
Kogan, F. N. 1995. {``Application of Vegetation Index and Brightness
Temperature for Drought Detection.''} \emph{Advances in Space Research},
Natural {Hazards}: {Monitoring} and {Assessment} {Using} {Remote}
{Sensing} {Technique}, 15 (11): 91--100.
\url{https://doi.org/10.1016/0273-1177(95)00079-T}.

\leavevmode\vadjust pre{\hypertarget{ref-mckee1993relationship}{}}%
McKee, Thomas B, Nolan J Doesken, John Kleist, et al. 1993. {``The
Relationship of Drought Frequency and Duration to Time Scales.''} In
\emph{Proceedings of the 8th Conference on Applied Climatology},
17:179--83. 22. Boston, MA, USA.

\leavevmode\vadjust pre{\hypertarget{ref-sutherland_orca_2000}{}}%
Sutherland, Peter, Anthony Rossini, Thomas Lumley, Nicholas Lewin-Koh,
Julie Dickerson, Zach Cox, and Dianne Cook. 2000. {``Orca: {A}
{Visualization} {Toolkit} for {High}-{Dimensional} {Data}.''}
\emph{Journal of Computational and Graphical Statistics} 9 (3): 509--29.
\url{https://www.jstor.org/stable/1390943}.

\leavevmode\vadjust pre{\hypertarget{ref-tucker_red_1979}{}}%
Tucker, Compton J. 1979. {``Red and Photographic Infrared Linear
Combinations for Monitoring Vegetation.''} \emph{Remote Sensing of
Environment} 8 (2): 127--50.
\url{https://doi.org/10.1016/0034-4257(79)90013-0}.

\leavevmode\vadjust pre{\hypertarget{ref-hdi}{}}%
United Nations Development Programme. 2022. {``Human Development Report
2021-22.''} New York. \url{http://report.hdr.undp.org}.

\leavevmode\vadjust pre{\hypertarget{ref-spei}{}}%
Vicente-Serrano, Sergio M., Santiago Beguería, and Juan I. López-Moreno.
2010. {``A {Multiscalar} {Drought} {Index} {Sensitive} to {Global}
{Warming}: {The} {Standardized} {Precipitation} {Evapotranspiration}
{Index}.''} \emph{Journal of Climate} 23 (7): 1696--1718.
\url{https://journals.ametsoc.org/view/journals/clim/23/7/2009jcli2909.1.xml}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham_tidy_2014}{}}%
Wickham, Hadley. 2014. {``Tidy {Data}.''} \emph{Journal of Statistical
Software} 59 (September): 1--23.
\url{https://doi.org/10.18637/jss.v059.i10}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham_plumbing_2009}{}}%
Wickham, Hadley, Michael Lawrence, Dianne Cook, Andreas Buja, Heike
Hofmann, and Deborah F. Swayne. 2009. {``The Plumbing of Interactive
Graphics.''} \emph{Computational Statistics} 24 (2): 207--15.
\url{https://doi.org/10.1007/s00180-008-0116-x}.

\leavevmode\vadjust pre{\hypertarget{ref-xie_reactive_2014}{}}%
Xie, Yihui, Heike Hofmann, and Xiaoyue Cheng. 2014. {``Reactive
{Programming} for {Interactive} {Graphics}.''} \emph{Statistical
Science} 29 (2): 201--13.
\url{https://www.jstor.org/stable/43288470?seq=1}.

\end{CSLReferences}



\end{document}
