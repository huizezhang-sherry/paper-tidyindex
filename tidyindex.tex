% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{interact}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{orcidlink}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={A Tidy Framework and Infrastructure to Systematically Assemble Spatio-temporal Indexes from Multivariate Data},
  pdfauthor={H. Sherry Zhang; Dianne Cook; Ursula Laa; Nicolas Langrené; Patricia Menéndez},
  pdfkeywords={indexes, data pipeline, software design},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{A Tidy Framework and Infrastructure to Systematically Assemble
Spatio-temporal Indexes from Multivariate Data}
\author{H. Sherry
Zhang$\textsuperscript{1}$~\orcidlink{0000-0002-7122-1463}, Dianne
Cook$\textsuperscript{1}$~\orcidlink{0000-0002-3813-7155}, Ursula
Laa$\textsuperscript{2}$~\orcidlink{0000-0002-0249-6439}, Nicolas
Langrené$\textsuperscript{3}$~\orcidlink{0000-0001-7601-4618}, Patricia
Menéndez$\textsuperscript{1}$~\orcidlink{0000-0003-0701-6315}}

\thanks{CONTACT: H. Sherry
Zhang. Email: \href{mailto:huize.zhang@monash.edu}{\nolinkurl{huize.zhang@monash.edu}}. }
\begin{document}
\captionsetup{labelsep=space}
\maketitle
\textsuperscript{1} Department of Econometrics and Business
Statistics, Monash University, Melbourne,
Victoria, Australia\\ \textsuperscript{2} Institute of
Statistics, University of Natural Resources and Life
Sciences, Vienna, Austria\\ \textsuperscript{3} Department of
Mathematical Sciences, BNU-HKBU United International College, Zhuhai,
Guangdong, China
\begin{abstract}
\begin{itemize}
\tightlist
\item
  indexes, useful, quantify severity, early monitoring,
\item
  A huge number of indexes have been proposed by domain experts,
  however, a large majority of them are not being adopted, reused, and
  compared in research or in practice.
\item
  One of the reasons for this is the plenty of indexes are quite complex
  and there is no obvious easy-to-use implementation to apply them to
  user's data.
\item
  The paper describes a general pipeline framework to construct indexes
  from spatio-temporal data,
\item
  This allows all the indexes to be constructed through a uniform data
  pipeline and different indexes to vary on the details of each step in
  the data pipeline and their orders.
\item
  The pipeline proposed aim to smooth the workflow of index construction
  through breaking down the complicated steps proposed by various
  indexes into small building blocks shared by most of the indexes.
\item
  The framework will be demonstrated with drought indexes as examples,
  but appliable in general to environmental indexes constructed from
  multivariate spatio-temporal data
\end{itemize}
\end{abstract}
\begin{keywords}
\def\sep{;\ }
indexes\sep data pipeline\sep 
software design
\end{keywords}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[enhanced, borderline west={3pt}{0pt}{shadecolor}, interior hidden, boxrule=0pt, frame hidden, breakable, sharp corners]}{\end{tcolorbox}}\fi

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Indexes are commonly used to combine and summarize different sources of
information into a single number for monitoring, communicating, and
decision-making. Examples of those include the Air Quality Index, El
Niño-Southern Oscillation Index, Consumer Price Index, QS University
Rankings or Human Development Index among many others.

To construct an index, experts typically start by defining a concept of
interest that requires measurement. This concept often lacks a direct
measurable attribute or can only be measure as a composite of various
processes, yet it holds social and public significance. To create an
index, once the underlying processes involved are identified, relevant
and available variables are then defined, collected, and combined using
statistical methods into an index that aims at measure the process of
interest. The construction process is often not straightforward, and
decisions need to be made, such as the selection of variables to be
included, which might depend on data availability and the statistical
definition of the index to be used, among others. For instance, the
indexes constructed from linear combination of variables need to decide
on the weight assigned to each variable. Some indexes have a spatial
and/or temporal components, and variables can be aggregated to different
spatial resolutions and temporal scales, leading to various indexes for
different monitoring purposes. Hence, all these decisions can result in
different index values and have different practical implications.

To be able to test different decision choices systematically for an
index, the index needs to be broken down into its fundamental building
blocks to analyse the contribution and effect of each component. We call
this process of breaking the index construction into different steps the
index pipeline. Such decomposition of index components provides the
means to standardise index construction via a pipeline and offers
benefits for comparing among indexes and calculating index uncertainty.
In social indexes for instance, the OECD handbook (OECD, European Union,
and Joint Research Centre - European Commission 2008) has provided a set
of steps and recommendations for constructing composite socio-economic
indexes. However, there is still a need to extend these guidelines and
methods to accommodate the inclusion of more methodological complex
steps required for indexes in general.

In this work, we aim at providing statistical and computational methods
to develop a data pipeline framework for constructing and customize
indexes using data. As a companion the R package, tidyindex, is
developed to construct and explore different. Furthermore, the tidyindex
package can be used to explore the effects of changes on index
definition, index construction steps, and construction methods. This
work provides researchers actively developing new indexes or aiming at
improving indexes with the tools to easily modify and evaluate indexes.
It also helps index analysts diagnose indexes, carry out sensitivity
analysis to assess small perturbations in the index, and identify
potential weaknesses for methodology improvement.

The rest of the paper is structured as follows:
Section~\ref{sec-pipeline} reviews the concept of data pipeline in R.
The pipeline framework for index construction is presented in
Section~\ref{sec-index-pipeline}. Section~\ref{sec-dev} explains how to
add new steps into the index methodology in the form of new building
blocks inside the index construction pipeline. Examples are given in
Section~\ref{sec-examples} to demonstrate the index construction with
the pipeline built.

\hypertarget{sec-pipeline}{%
\section{Tidy framework}\label{sec-pipeline}}

The tidy framework consists of two key components: tidy data and tidy
tools. The concept of tidy data (Wickham 2014) prescribes specific rules
for organising data in an analysis, with observations as rows, variables
as columns, and types of observational units as tables. Tidy tools, on
the other hand, are concatenated in a sequence through which the tidy
data flows, creating a pipeline for data processing and modelling. These
pipelines are data centric, meaning all the tidy tools or functions take
a tidy data object as inputs and return a processed tidy data object,
directly ready for the next operations to be applied. Also, the pipeline
approach corresponds to the modular programming practice, which breaks
down complex problems into smaller and more manageable pieces, as oppose
to a monolithic design, where all the steps are predetermined and
integrated into a single piece. The flexibility provided by the
modularity makes it easier to modify certain steps in the pipeline and
to maintain and extend the code base.

Examples of using a pipeline approach for data analysis can be traced
back to the interactive graphics literature, including Buja et al.
(1988); Sutherland et al. (2000); Xie, Hofmann, and Cheng (2014);
Wickham et al. (2009). Wickham et al. (2009) argues that whether made
explicit or not, a pipeline has to be presented in every graphics
program and making them explicit is beneficial for understanding the
implementation and comparing between different graphic systems. While
this comment is made in the context of interactive graphics program, it
is also applicable generally to any data analysis workflow. More
recently, the tidyverse suite (Wickham et al. 2019) takes the pipeline
approach for general-purpose data wrangling and has gained popularity
within the R community. The pipeline-style code can be directly read as
a series of operations applied successively on tidy data object,
offering a method to document the data wrangling process with all the
computational details for reproducibility.

Since the success of tidyverse, more packages have been developed to
analyze data using the tidy framework for domains specific applications,
a noticeable example of which is \texttt{tidymodels} for building
machine learning models (Kuhn and Wickham 2020). To create a tidy
workflow tailored to a specific domain, developers first need to
identify the fundamental building blocks to create a workflow. These
components are then implemented as modules, which can be combined to
form the pipeline. For example, in supervised machine learning models,
steps such as data splitting, model training and model evaluation are
commonly used in most workflow. In the \texttt{tidymodels}, these steps
are correspondingly implemented as package \texttt{rsample},
\texttt{parsnip}, and \texttt{yardstick}, agnostic to the specific model
chosen. The uniform interface in tidymodels frees analysts from
recalling model-specific syntax for performing the same operation across
different models, increasing the efficiency to work with different
models simultaneously.

For constructing indexes, the pipeline approach adopts explicit and
standalone modules that can be assembled in different ways. Index
developers can choose the appropriate modules and arrange them
accordingly to generate the data pipeline that is needed for their
purpose. The pipeline approach provides many advantages:

\begin{itemize}
\tightlist
\item
  makes the computation more transparent, and thus more easily debugged.
\item
  allows for rapidly processing new data to check how different
  features, like outliers, might affect the index value.
\item
  provides the capacity to measure uncertainty by computing confidence
  intervals from multiple samples as generated by bootstrapping to
  original data.
\item
  enables systematic comparison of surrogate indexes designed to measure
  the same phenomenon.
\item
  it may even be possible to automate diagramatic explanations and
  documentation of the index.
\end{itemize}

Adoption of this pipeline approach would provide uniformity to the field
of index development, research and application.

\hypertarget{sec-index-pipeline}{%
\section{Details of the index pipeline}\label{sec-index-pipeline}}

In constructing various indexes, the primary aim is to transform the
data, often multivariate, into a univariate index. Spatial and temporal
considerations are also factored into the process when observational
units and time periods are not independent. However, despite the
variations in contextual information for indexes in different fields,
the underlying statistical methodology remains consistent across diverse
domains. Each index can be represented as a series of modular
statistical operations on the data. This allows us to decompose the
index construction process into a unified pipeline workflow with a
standardized set of data processing steps to be applied across different
indexes.

An overview of the pipeline is given in Figure~\ref{fig-pipeline-steps}
to illustrate the construction from raw data to the final indexes. The
pipeline includes eight modules for operations in the spatial, temporal,
and multivariate aspects of the data as well as modules for comparing
and communicating indexes. Analysts can choose a subset of modules and
reorder them as needed to construct an index.

Data pre-processing may happen before the start of the index pipeline to
prepare multivariate data from different locations/ countries ready for
constructing an index. For remote sensing data, it includes aligning the
spatial resolution and temporal frequency of data collected from
different satellite products, or merging in-situ stations with the
satellite data.

Now, we introduce the notation used for describing pipeline modules.
Consider multivariate spatio-temporal data,

\begin{equation}
\begin{aligned}
x^l(s;t) = [x^1(s;t), x^2(s;t), & \cdots, x^L(s;t)] \\
& l = 1, 2, \cdots L, s \in D_s \subseteq \mathbb{R}^d, t \in D_t \subseteq \mathbb{R}^m 
\end{aligned}
\end{equation}

where \(x^l\) is the \(l\)th variable, \(s\) is the spatial location,
and \(t\) is the temporal order. The spatial space,
\(D_s \subseteq \mathbb{R}^d\), is generic to include different spatial
representations, including a 1D set of \(n\) locations
\(\{D_s: s_1, s_2, \cdots, s_n\}\), a 2D set of longitude and latitude
pairs, and \(d\)-D coordinate set. The higher dimension in the temporal
space, \(D_t \subseteq \mathbb{R}^m\), can be used to represent for a
\(m\)-D deconstruction of time. For example, a year-month combination,
\((Y, M)^\prime\), can be used to record monthly precipitation series
from January 1990 to December 2020, where \(Y = 1990, \cdots, 2020\) and
\(M = 1, \cdots, 12\). In the case when the spatial dimension is fixed,
multivariate temporal data can be simplified as \(x^l_s(t)\), and
similarly, multivariate spatial data can be simplified as \(x^l_t(s)\).

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=0.9\textheight]{figures/pipeline-overall.png}

}

\caption{\label{fig-pipeline-steps}Diagram of pipeline steps for index
construction. will need to be updated with better design and the
distribution fitting step.}

\end{figure}

\hypertarget{temporal-processing}{%
\subsection{Temporal processing}\label{temporal-processing}}

\texttt{Input}: \(x^l_s(t)\),

\texttt{Output}: \(x^l_s(t^{\prime})\), where
\(t^\prime \in D_{t^{\prime}}\).

\texttt{Computation}: The construction of an index sometimes needs to
consider information from neighbouring time periods. The temporal
processing is a general operator on the time dimension of the data in
the form of

\begin{equation}
x(\mathbf{s};\mathbf{t}^\prime) = f_{\mathcal{\psi}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where \(\psi \in \Psi \subseteq \mathbb{R}^{d_{\psi}}\) is the
parameters associated with the temporal operation and \(d_{\psi}\) is
the number of parameter of \(\psi\). A typical example of temporal
processing is aggregation and in the drought community, these indexes
are often called multi-scalar indexes. For example, Standardized
Precipitation Index (SPI) aggregates monthly precipitation using a time
scale parameter \(k\) in the form of

\[x(s_i;t_{j^\prime}) = \sum_{j = j^\prime}^{j^\prime + k - 1}x(s_i; t_j),\]

where \(j^\prime\) is the new time index after the aggregation.
Multi-scalar drought indexes are widely used to study different types of
drought events. A smaller value of the time scales parameter \(k\) is
used for short term drought events, while a large \(k\) of 18 or 24, an
equivalent to a 1.5- or 2-year aggregation, is often used for long term
assessment.

\hypertarget{spatial-processing}{%
\subsection{Spatial processing}\label{spatial-processing}}

\texttt{Input}: \(x^l_t(s)\)

\texttt{Output}: \(x^l_t(s^\prime)\) where
\(s^\prime \in D_{s^{\prime}}\)

\texttt{Computation}: Spatial processing may be needed when indexes are
not calculated independently on each collected location or when
variables collected from multiple sources need to be fused before
further processing. The process can be written as a general operation in
the form of

\begin{equation}
x(\mathbf{s}^\prime;\mathbf{t}) = g_{\mathcal{\theta}}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where \(\theta \in \Theta \subseteq \mathbb{R}^{d_{\theta}}\) is the
associated parameters in the process and \(d_{\theta}\) is the number of
parameter of \(\theta\). An example of spatial processing is to align
variables collected in different resolutions. When variables are
collected at different resolutions, analysts may choose to down-sample
those in a finer resolution, \(i\), to match those in a coarser
resolution, \(i^\prime\). This is a spatial aggregation and if aggregate
using the mean, it can be written as

\begin{equation}
g(x) = \frac{\sum_{i \in i^\prime}x}{n_{i^\prime}},
\end{equation}

where \(i \in i^\prime\) includes all the cells from the finer
resolution in the coarser grid and \(n_{i^\prime}\) is the number of
observations falls into the coarser grid. Other examples of spatial
processing include 1) borrowing information from neighbouring spatial
locations to interpolate unobserved locations and 2) fusing variables
from ground measures with satellite imageries.

\hypertarget{variable-transformation}{%
\subsection{Variable transformation}\label{variable-transformation}}

\texttt{Input}: \(x^l(s; t)\)

\texttt{Output}: \(y^l(s; t)\)

\texttt{Computation}: The purpose of variable transformation is to
create variables that fits assumptions for further computing. These
assumptions include a stable variance, normal distribution, or a certain
scale required by some algorithms down the pipeline. Variable
transformation is a general notion of a functional transformation on the
variable:

\begin{equation}
h_{\tau}(x(\mathbf{s};\mathbf{t})),
\end{equation}

where \(\tau \in T \subseteq \mathbb{R}^{d_{\tau}}\) is the parameter in
the transformation if any, and \(d_{\tau}\) is the number of parameter
of \(\tau\). Transformation is needed for data that are highly skewed
and some common transformations include log, quadratic, and square root
transformation.

\hypertarget{scaling}{%
\subsection{Scaling}\label{scaling}}

\texttt{Input}: \(x^l(s; t)\)

\texttt{Output}: \(x^l(s; t)\)

\texttt{Computation}: While scaling can be seen as a specific type of
variable transformation, it is separated into its own step to make the
step explicit in the pipeline. The key difference between the two steps
is that variable transformation typically changes the shape of the data
while scaling only changes the data scale and can usually be written in
the form of

\begin{equation}
[x(s_i;t_j) - \alpha]/\gamma.
\end{equation}

For example, a z-score standardisation can be written in the above form
with \(\alpha = \bar{x}(s; t)\) and \(\gamma = \sigma(s; t)\), a min-max
standardisation uses \(\alpha = \min[x(s_i, t_j)]\) and
\(\gamma = \max[x(s_i, t_j)] - \min[x(s_i, t_j)]\).
Figure~\ref{fig-scale-var-trans-compare} shows a collection of variable
pre-processing operations and uses color to differentiate whether the
operation is a variable transformation or a scaling step. While both
variable transformation and scaling are pre-processing steps, the
scaling operations in green show the same distribution as the original
data.

\begin{figure}

{\centering \includegraphics{tidyindex_files/figure-pdf/fig-scale-var-trans-compare-1.pdf}

}

\caption{\label{fig-scale-var-trans-compare}Comparison of operations in
scaling (green) and variable transformation (orange) steps in free
scale. Variables after the scaling operations have the same distribution
as the origin, while the distribution changes after variable
transformation.}

\end{figure}

\hypertarget{dimension-reduction}{%
\subsection{Dimension reduction}\label{dimension-reduction}}

\texttt{Input}: \(x^l(s; t)\)

\texttt{Output}: \(x^{l^\prime}(s; t)\), where
\(l^{\prime} = 1, 2, \cdots, L^{\prime}\)

\texttt{Computation}: Dimension reduction summarises the multivariate
information into univariate, which can be denoted as:

\begin{equation}
x_p(\mathbf{s}; \mathbf{t}) \rightarrow x(\mathbf{s}; \mathbf{t}),
\end{equation}

where \(p = 1, 2, \cdots, P\). The combination can be based on
domain-specific knowledge, originated from theories describing the
underlying physical process. For example, the SPEI uses a water balance
model (\(D = P - \text{PET}\)) to calculate the difference series
(\(D\)) from precipitation (\(P\)) and potential evapotranspiration
(\(\text{PET}\)).

Another widely used approach is linear combination, which aggregates a
collection of variables in a linear additive structure, expressed as:
\[x(\mathbf{s}; \mathbf{t}) = \sum_{p = 1}^{P}\lambda_{p}x_p(\mathbf{s};\mathbf{t}),\]

where \(\lambda_p\) denotes the weight assigned to variable \(x_p\).
Most indexes uses an equal weight that sum to 1, which allows each
variable to contribute equally to the index. A linear combination can
also be viewed as a linear projection of multivariate information by the
weight vector. Projecting data from higher to lower dimension inevitably
leads to information loss. For example, two countries can receive
similar index, but their components could significantly differ -- one
with average scores across all components and another with extreme
scores the opposite ends. The selected set of weights need to be
examined to understand its effect on the index and its implications for
decision-making. To do this, analysts can vary the coefficients in the
linear projection to observe how the index value and countries' ranking
change. These changes can also be visualized using a method called tour
by generating an animation of the projections among different sets of
weights.

\hypertarget{distribution-fit}{%
\subsection{Distribution fit}\label{distribution-fit}}

\texttt{Input}: \(x^l(s; t)\)

\texttt{Output}:

\texttt{Computation}: Distribution fit can be seen as the model fitting
in its simplest term. It can be represented by

\begin{equation}
F_{\eta}(x(\mathbf{s}; \mathbf{t})), 
\end{equation}

where \(\eta \in H \subseteq \mathbb{R}^{d_{\eta}}\) is the distribution
parameter and \(d_{\eta}\) is the number of parameter of \(\eta\). A
distribution fit typically aims at finding the distribution that best
fits the data. Analysts may start from a pool of candidate distributions
with a chosen fitting method and goodness of fit measure. While it is
useful to find the ultimate best distribution to fits the data, from a
probabilistic perspective, the fitting procedure itself has an
uncertainty associated with the data fed and the parameter chosen. A
reasonable alternative is to understand how much the index values can
vary given different distributions, fitting methods, and goodness of fit
tests, and whether these variations are negligible in a given
application.

\hypertarget{normalising}{%
\subsection{Normalising}\label{normalising}}

\texttt{Input}: \(x^l(s; t)\)

\texttt{Output}:

\texttt{Computation}: This step maps the univariate series into a
different scale, typically for ease of comparison across regions. For
example, a normal scale, {[}0, 1{]}, or {[}0, 100{]} may be favored for
reporting certain indexes. In drought indexes, i.e.~SPI or SPEI, the
quantiles from the fitted distribution are converted into the normal
scale via the normal reverse CDF function: \(\Phi^{-1}(.)\). Normalising
is usually used at the end of the pipeline and its main difference from
the scaling step is that here the change of scale also changes the
distribution of the variable. While being commonly used, this step can
get criticism from analysts for forcing the data into the decided scale,
which can be either unnecessary or inaccurately exaggerate or downplay
the outliers. Also, the use of a normal scale needs to be interpreted
with caution. Figure~\ref{fig-normalising} illustrates the normal
density not being directly proportional to its probability of
occurrence. This is concerning, especially at the extreme values, since
a small difference in the tail density can have magnitudes of difference
in its probability of occurrence.

\begin{figure}

{\centering \includegraphics{tidyindex_files/figure-pdf/fig-normalising-1.pdf}

}

\caption{\label{fig-normalising}Scatterplot of normal quantiles against
their density values. THree tail density values are highlighted with its
probability of occurence labelled. Probability is calculated assuming
monthly data: with a density of -2, the probability of occurrence is
1/pnorm(-2)/12 = 4 years. The non-linear relationship between the two
quantities suggests normalised indexes need to be interpreted with
caution since a slight change in the tail distribution can result in
magnitudes of difference in its probability of occurrence.}

\end{figure}

\hypertarget{benchmarking}{%
\subsection{Benchmarking}\label{benchmarking}}

\texttt{Input}: \(x^l(s; t)\)

\texttt{Output}:

\(D(x^l(s,t), a)\) where \(D(., .)\) is a distance function between

\texttt{Computation}: Benchmarking sets a constant value to allow the
constructed index to be compared across time. Here we denote it with
\(u[x(s_i, t_j)]\) where \(u\) is a scalar of interest in the index
constructed. A benchmark value could be a constant or a function of the
data, i.e.~mean.

\hypertarget{simplification}{%
\subsection{Simplification}\label{simplification}}

\texttt{Input}: \(x^l(s; t)\)

\texttt{Output}: \(C^l(s;t)\) where \(\{C: c_0, c_1, \cdots, c_z\}\) is
a discrete set

\texttt{Computation}: In public communication, the index values are
usually accompanied by a categorical grade. The categorised grades are
an ordered set of descriptive words or colors to communicate the
severity or guide the comprehension of the indexes. The mapping from
continuous index values to the discrete grades is called simplification
in the pipeline and it can be written as a piece-wise function:

\begin{equation}
\begin{cases}
c_0 & C_1 \leq x^l(s; t) < C_0 \\
c_1 & C_2 \leq x^l(s; t) < C_1 \\
c_2 & C_3 \leq x^l(s; t) < C_2 \\
\cdots \\
c_z & C_z \leq x^l(s; t)
\end{cases}
\end{equation}

where \(C_0, C_1,\cdots ,C_z\) are the categories and
\(c_0, c_1, \cdots, c_z\) are the thresholds for each category. In SPI,
droughts are sorted into four categories: mild drought: \([-0.99, 0]\);
moderate drought: \([-1.49, -1]\); severe drought: \([-1.99, -1.5]\),
and extreme drought: \([-\infty, -2]\). In this case,
\(C_0, C_1, C_2, C_3\) are the drought categories: mild, moderate,
severe, and extreme drought (\(z = 3\)) and
\(c_0 =0, c_1 = -1, c_2 = -1.5, c_3 = -2\) are the cutoff value for each
class.

\hypertarget{sec-dev}{%
\section{Software design}\label{sec-dev}}

\hypertarget{sec-examples}{%
\section{Examples}\label{sec-examples}}

This section uses the example of drought and social indexes to show the
analysis made possible with the index pipeline. The drought index
example computes two indexes with various time scales and distributions
simultaneously using the pipeline framework to understand the flood and
drought events in Queensland. The social index example focuses on the
dimension reduction in Global Gender Gap Index to explore the impact of
weight changes in linear combination on index value and country ranking.

\hypertarget{every-distribution-every-scale-every-index-all-at-once}{%
\subsection{Every distribution, every scale, every index all at
once}\label{every-distribution-every-scale-every-index-all-at-once}}

A common task for drought researchers is to compute indexes at different
parameter combinations. This can be used to identify the spatial and
temporal extent of drought events, recommend the best parameter choice,
or compare the effectiveness of indexes for monitoring drought. The
example below computes two indexes: SPI and SPEI, at various time scales
and fitted distributions, for stations in the state of Queensland in
Australia. The purpose of the example is to demonstrate the interfaces
the tidyindex package built to allow easy computing at different
parameter combinations.

The state of Queensland in Australia is frequently affected by natural
disaster events such as flood and drought, which can have significant
impacts on its agricultural industry. This study uses daily data from
Global Historical Climatology Network Daily (GHCND), accessed via the
package \texttt{rnoaa} to examine drought/flood condition in Queensland.
Daily data is average into monthly and stations are excluded if monthly
data contains missings, which is required for calculating both SPI and
SPEI. This gives 29 stations with complete records from 1990 January to
2022 April.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=0.9\textheight]{figures/pipeline-spei.png}

}

\caption{\label{fig-pipeline-spei}Diagram of pipeline steps for index
construction. will need to be updated with better design and the
distribution fitting step.}

\end{figure}

The function \texttt{compute\_indexes()} can be used to collectively
compute multiple indexes. The tidyindex offers wrapper functions, with
the prefix idx\_, that simplify the calculation of commonly used indexes
by combining a set of pipeline steps into a single function. For
example, the function \texttt{idx\_spei()} includes the five steps
previously described in \textbf{?@sec-toy-example} (variable
transformation, dimension reduction, temporal aggregation, distribution
fit, and normalise). Each \texttt{idx\_xxx()} function specifies the
relevant parameters relevant to the index: the thornthwaite method is
used to calculate PET in SPEI, with the average temperature
(\texttt{tavg}) and latitude (\texttt{lat}) used as inputs. The SPEI is
computed at four time scales (6, 12, 24, and 36 months) and fitted with
two distributions (Log-logistic and General Extreme Value (GEV)). The
SPI is also computed at the same four time scales and uses the default
gamma distribution to fit the aggregated series.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{.scale }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{36}\NormalTok{)}
\NormalTok{idx }\OtherTok{\textless{}{-}}\NormalTok{ queensland }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{init}\NormalTok{(}\AttributeTok{id =}\NormalTok{ id, }\AttributeTok{time =}\NormalTok{ ym) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compute\_indexes}\NormalTok{(}
    \AttributeTok{spei =} \FunctionTok{idx\_spei}\NormalTok{(}
      \AttributeTok{.pet\_method =} \StringTok{"thornthwaite"}\NormalTok{, }\AttributeTok{.tavg =}\NormalTok{ tavg, }\AttributeTok{.lat =}\NormalTok{ lat,}
      \AttributeTok{.scale =}\NormalTok{ .scale, }\AttributeTok{.dist =} \FunctionTok{c}\NormalTok{(}\FunctionTok{gev}\NormalTok{(), }\FunctionTok{loglogistic}\NormalTok{())),}
    \AttributeTok{spi =} \FunctionTok{idx\_spi}\NormalTok{(}\AttributeTok{.scale =}\NormalTok{ .scale)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Checking for missing values (`NA`): all the data must be complete. Input type is vector. Assuming the data are monthly time series starting in January, all regular (non-leap) years."
\end{verbatim}

The output from \texttt{compute\_indexes()} contains index values and
associated parameter in a long tibble. It includes the original
variables (\texttt{id}, \texttt{ym}, \texttt{prcp}, \texttt{tmax},
\texttt{tmin}, \texttt{tavg}, \texttt{long}, \texttt{lat}, and
\texttt{name}), index parameters (\texttt{.idx}, \texttt{.scale},
\texttt{.method}, and \texttt{.dist}), intermediate variables
(\texttt{pet}, \texttt{.agg}, and \texttt{.fitted}), and the final index
(\texttt{.index}). This data can be visualised across space or time, or
simultaneously, to explore the wet/dry condition in Queensland.
Figure~\ref{fig-compute-spatial} visualises the spatial distribution of
SPI at two periods (2010 October - 2011 March and 2019 October - 2020
March) with significant natural disaster events: 2010/11 Queensland
flood and 2019 Australia drought, which contributes to the notorious
2019/20 bushfire. Figure~\ref{fig-compute-temporal} displays the
sensitivity of the SPEI series for one particular station, Texas post
office, at different time scales and fitted distributions. These two
plots demonstrate some possibilities to explore the indexes after they
are computed from \texttt{compute\_indexes()}.

\begin{figure}

{\centering \includegraphics{tidyindex_files/figure-pdf/fig-compute-spatial-1.pdf}

}

\caption{\label{fig-compute-spatial}Spatial distribution of Standardized
Precipitation Index (SPI-12) in Queensland, Australia during two major
flood and drought events: 2010/11 and 2019/20. The map shows a continous
wet period during the 2010/11 flood period and a mitigated drought
situation, after its worst in 2019 December and 2020 Janurary, likely
due to the increased rainfall in February from the meteorological
record.}

\end{figure}

\begin{figure}

{\centering \includegraphics{tidyindex_files/figure-pdf/fig-compute-temporal-1.pdf}

}

\caption{\label{fig-compute-temporal}Time series plot of Standardized
Precipitation-Evapotranspiration Index (SPEI) at the Texas post office
station (highlighted by a diamond shape in panel a). The SPEI is
calculated at four time scales (6, 12, 24, and 36 months) and fitted
with two distributions (Log Logistic and GEV). The dashed line at -2
represents the class ``extreme drought'' by the SPEI. A larger time
scale gives a smoother index series, while also takes longer to recover
from an extreme situation as seen in the 2019/20 drought period. The
SPEI values from two distribution fits mostly agree, while GEV can
results in more extreme values, i.e.~in 1998 and 2020.}

\end{figure}

\hypertarget{does-a-minor-change-in-variable-weights-cause-a-tornado}{%
\subsection{Does a minor change in variable weights cause a
tornado?}\label{does-a-minor-change-in-variable-weights-cause-a-tornado}}

The Global Gender Gap Index (GGGI), published annually by the World
Economic Forum, measures gender parity by assessing relative gaps
between men and women in four key areas: Economic Participation and
Opportunity, Educational Attainment, Health and Survival, and Political
Empowerment (World Economic Forum 2023). The index is composed of 14
variables, expressed as female-to-male ratios, which are first
aggregated in a linear combination into the four dimensions using the
weight from the \texttt{V-weight} column in
Table~\ref{tbl-gggi-weights}. The weight is calculated as the inverse of
the standard deviation of each variable and scaling to sum to 1 within
each dimension to allow a one percentage point change in the standard
deviation of each variable to contribute equally to the index. The four
dimensions are then aggregated in another linear combination with equal
weight to obtain the index. The 2023 GGGI data is available from the
Global Gender Gap Report 2023 in the country's economy profile and can
be accessed in R via the \texttt{tidyindex} package as \texttt{gggi},
along with the corresponding weights \texttt{gggi\_weights}.

\hypertarget{tbl-gggi-weights}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.5455}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1299}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1169}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1169}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0909}}@{}}
\caption{\label{tbl-gggi-weights}Weights of the fourteen variables in
Global Gender Gap Index}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
V-weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
D-weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Weight
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
V-weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
D-weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Weight
\end{minipage} \\
\midrule()
\endhead
Labour force participation & Economy & 0.199 & 0.25 & 0.050 \\
Wage equality for similar work & Economy & 0.310 & 0.25 & 0.078 \\
Estimated earned income & Economy & 0.221 & 0.25 & 0.055 \\
Legislators senior officials and managers & Economy & 0.149 & 0.25 &
0.037 \\
Professional and technical workers & Economy & 0.121 & 0.25 & 0.030 \\
Literacy rate & Education & 0.191 & 0.25 & 0.048 \\
Enrolment in primary education & Education & 0.459 & 0.25 & 0.115 \\
Enrolment in secondary education & Education & 0.230 & 0.25 & 0.058 \\
Enrolment in tertiary education & Education & 0.121 & 0.25 & 0.030 \\
Sex ratio at birth & Health & 0.693 & 0.25 & 0.173 \\
Healthy life expectancy & Health & 0.307 & 0.25 & 0.077 \\
Women in parliament & Politics & 0.310 & 0.25 & 0.078 \\
Women in ministerial positions & Politics & 0.247 & 0.25 & 0.062 \\
Years with female head of state & Politics & 0.443 & 0.25 & 0.111 \\
\bottomrule()
\end{longtable}

A natural thing to do when provided with the index data is to reproduce
the index. This helps index analysts to verify the index calculation and
become familiar with the methodology. For GGGI, the construction can be
simplified as a single linear aggregation step in the dimension
reduction module, with the \texttt{Weight} column in
Table~\ref{tbl-gggi-weights}, which is the product of the variable
weight (\texttt{V-weight}) and dimension weight (\texttt{D-weight}).

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=0.9\textheight]{figures/pipeline-gggi.png}

}

\caption{\label{fig-pipeline-steps-gggi}Diagram of pipeline steps for
index construction. will need to be updated with better design and the
distribution fitting step.}

\end{figure}

\begin{verbatim}
gggi %>% 
  init(id = country) %>%
  add_meta(gggi_weights, var_col = variable) %>% 
  dimension_reduction(
    index_new = aggregate_linear(
      ~labour_force_participation:years_with_female_head_of_state,
      weight = weight)) 
\end{verbatim}

The result can be compared with the GGGI values available in the report
as shown in Figure~\ref{fig-compare-gggi}, validating the
reproducibility of the index for country with no missing variables.

\begin{figure}

{\centering \includegraphics{tidyindex_files/figure-pdf/fig-compare-gggi-1.pdf}

}

\caption{\label{fig-compare-gggi}Verifying the calculation of the Global
Gender Gap Index (GGGI). The index can be reproduced from the
methodology described in the Global Gender Gap Report 2023 after
removing the countries with missing variables, the treatment of which is
unclear.}

\end{figure}

To understand the uncertainty of this dimension reduction step from
combining four dimensions (Economy, Education, Health, and Politics), we
run a local tour {[}TODO: reference to local tour{]} that varies the
weight of each dimension slightly to observe how index value and country
ranking changes. The local tour produces an animation that gradually
increases the weight of one variable and reduces it back to equal
weight, one at a time. A set of countries are selected for this
exercise, which includes 1) the top four with GGGI \textgreater{} 0.85,
2) a set of countries with GGGI between 0.72 and 0.73 (Brazil, Panama,
Poland, Bangladesh, Kazakhstan, Armenia, and Slovakia), and 3) the
bottom five countries with GGGI \textless{} 0.6. Five frames (equal
weights and one for each dimension with a relatively higher weight) are
selected from the local tour animation to show in
Figure~\ref{fig-idx-tour} and the full animation is available at
https://vimeo.com/847874016?share=copy.

Figure~\ref{fig-idx-tour} helps to understand the how different
dimension reduction alternatives affect the whole index distribution and
individual countries.

\begin{itemize}
\tightlist
\item
  Bangladesh
\item
  when decreasing the weight on politics, difference between countries
  narrower.
\item
  increase politics -\textgreater{} all the country has lower index
  value
\end{itemize}

\begin{figure}

{\centering \includegraphics{tidyindex_files/figure-pdf/fig-idx-tour-1.pdf}

}

\caption{\label{fig-idx-tour}Five frames selected from varying the
linear weights of four dimensions in Global Gender Gap Index. The
weights vary slightly from the official simple average weights (0.25,
0.25, 0.25, 0.25) to observe how the index and ranking reponse. Full
animation is available at https://vimeo.com/847874016?share=copy.}

\end{figure}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

The paper presents a data pipeline with nine modules for constructing
and analysing indexes. The pipeline increases transparency in the
practice for index analysts to experiment with different index design
and parameter choices to better design and apply their indexes. The
significance of this work is its ability to provide a universal
framework for index construction, which can be applied across different
domains.

Examples have been given in the drought indexes and human development
index to demonstrate computing of indexes with different parameters
combinations and how alternative index design can provide insights to
understand distinctive country characteristics that could sometimes be
overlooked. The accompanied package, tidyindex, is not meant to provide
comprehensive implementation for all indexes across all domains.
Instead, it demonstrates implementing individual pipeline steps that are
versatile to multiple indexes and composing new indexes from existing
steps. Domain experts are welcomed to adopt the pipeline approach to
develop specialised packages for specific-domains indexes.

Future work: - integrate more complex dimension reduction methods to
calculate weights - strengthen the spatial processing module

\hypertarget{reference}{%
\section*{Reference}\label{reference}}
\addcontentsline{toc}{section}{Reference}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-buja_elements_1988}{}}%
Buja, A, D Asimov, C Hurley, and JA McDonald. 1988. {``Elements of a
Viewing Pipeline for Data Analysis.''} In \emph{Dynamic Graphics for
Statistics}, 277--308. Wadsworth, Belmont.

\leavevmode\vadjust pre{\hypertarget{ref-tidymodels}{}}%
Kuhn, Max, and Hadley Wickham. 2020. \emph{Tidymodels: A Collection of
Packages for Modeling and Machine Learning Using Tidyverse Principles.}
\url{https://www.tidymodels.org}.

\leavevmode\vadjust pre{\hypertarget{ref-oecd_handbook_2008}{}}%
OECD, European Union, and Joint Research Centre - European Commission.
2008. \emph{Handbook on {Constructing} {Composite} {Indicators}:
{Methodology} and {User} {Guide}}. OECD.
\url{https://doi.org/10.1787/9789264043466-en}.

\leavevmode\vadjust pre{\hypertarget{ref-sutherland_orca_2000}{}}%
Sutherland, Peter, Anthony Rossini, Thomas Lumley, Nicholas Lewin-Koh,
Julie Dickerson, Zach Cox, and Dianne Cook. 2000. {``Orca: {A}
{Visualization} {Toolkit} for {High}-{Dimensional} {Data}.''}
\emph{Journal of Computational and Graphical Statistics} 9 (3): 509--29.
\url{https://www.jstor.org/stable/1390943}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham_tidy_2014}{}}%
Wickham, Hadley. 2014. {``Tidy {Data}.''} \emph{Journal of Statistical
Software} 59 (September): 1--23.
\url{https://doi.org/10.18637/jss.v059.i10}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham_welcome_2019}{}}%
Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy
D'Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.
{``Welcome to the {Tidyverse}.''} \emph{Journal of Open Source Software}
4 (43): 1686. \url{https://doi.org/10.21105/joss.01686}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham_plumbing_2009}{}}%
Wickham, Hadley, Michael Lawrence, Dianne Cook, Andreas Buja, Heike
Hofmann, and Deborah F. Swayne. 2009. {``The Plumbing of Interactive
Graphics.''} \emph{Computational Statistics} 24 (2): 207--15.
\url{https://doi.org/10.1007/s00180-008-0116-x}.

\leavevmode\vadjust pre{\hypertarget{ref-WEF2023}{}}%
World Economic Forum. 2023. {``{The Global Gender Gap Report 2023}.''}
\url{https://www3.weforum.org/docs/WEF_GGGR_2023.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-xie_reactive_2014}{}}%
Xie, Yihui, Heike Hofmann, and Xiaoyue Cheng. 2014. {``Reactive
{Programming} for {Interactive} {Graphics}.''} \emph{Statistical
Science} 29 (2): 201--13.
\url{https://www.jstor.org/stable/43288470?seq=1}.

\end{CSLReferences}



\end{document}
